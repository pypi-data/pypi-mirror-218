def logistic(x, w=1., phase=0, gain=1):   return gain / (1. + np.exp(-w * (x - phase)))def logistic(x, w=1., phase=0, gain=1):   return gain / (1. + np.exp(-w * (x - phase)))def logistic(x, w=1., phase=0, gain=1):   return gain / (1. + np.exp(-w * (x - phase)))import pandas as pdimport pandas as pdimport numpy as npimport numpy as npimport seaborn as snsimport seaborn as snssns.set_style()sns.set_style()xy = pd.DataFrame(np.arange(-50, 50) / 10., columns=['x'])xy = pd.DataFrame(np.arange(-50, 50) / 10., columns=['x'])for w, phase in zip([1, 3, 1, 1, .5], [0, 0, 2, -1, 0]):   kwargs = dict(w=w, phase=phase)   xy[f'{kwargs}'] = logistic(xy['x'], **kwargs)for w, phase in zip([1, 3, 1, 1, .5], [0, 0, 2, -1, 0]):   kwargs = dict(w=w, phase=phase)   xy[f'{kwargs}'] = logistic(xy['x'], **kwargs)for w, phase in zip([1, 3, 1, 1, .5], [0, 0, 2, -1, 0]):   kwargs = dict(w=w, phase=phase)   xy[f'{kwargs}'] = logistic(xy['x'], **kwargs)for w, phase in zip([1, 3, 1, 1, .5], [0, 0, 2, -1, 0]):   kwargs = dict(w=w, phase=phase)   xy[f'{kwargs}'] = logistic(xy['x'], **kwargs)xy.plot(grid="on", ylabel="y")xy.plot(grid="on", ylabel="y")from collections import Counterfrom collections import Counternp.random.seed(451)np.random.seed(451)tokens = "green egg egg ham ham ham spam spam spam spam".split()tokens = "green egg egg ham ham ham spam spam spam spam".split()bow = Counter(tokens)bow = Counter(tokens)x = pd.Series(bow)x = pd.Series(bow)xxx1, x2, x3, x4 = xx1, x2, x3, x4 = xx1, x2, x3, x4x1, x2, x3, x4w0 = np.round(.1 * np.random.randn(), 2)w0 = np.round(.1 * np.random.randn(), 2)w0w0w1, w2, w3, w4 = (.1 * np.random.randn(len(x))).round(2)w1, w2, w3, w4 = (.1 * np.random.randn(len(x))).round(2)w1, w2, w3, w4w1, w2, w3, w4x = np.array([1, x1, x2, x3, x4])    <1>x = np.array([1, x1, x2, x3, x4])    <1>w = np.array([w0, w1, w2, w3, w4])    <2>w = np.array([w0, w1, w2, w3, w4])    <2>y = np.sum(w * x)    <3>y = np.sum(w * x)    <3>yythreshold = 0.0threshold = 0.0y = int(y > threshold)y = int(y > threshold)y = logistic(x)y = logistic(x)def neuron(x, w):   z = sum(wi * xi for xi, wi in zip(x, w)) <1>   return z > 0    <2>def neuron(x, w):   z = sum(wi * xi for xi, wi in zip(x, w)) <1>   return z > 0    <2>def neuron(x, w):   z = sum(wi * xi for xi, wi in zip(x, w)) <1>   return z > 0    <2>def neuron(x, w):   z = sum(wi * xi for xi, wi in zip(x, w)) <1>   return z > 0    <2>def neuron(x, w):   z = np.array(wi).dot(w)   return z > 0def neuron(x, w):   z = np.array(wi).dot(w)   return z > 0def neuron(x, w):   z = np.array(wi).dot(w)   return z > 0def neuron(x, w):   z = np.array(wi).dot(w)   return z > 0import pandas as pdimport pandas as pdimport numpy as npimport numpy as nppd.options.display.max_rows = 7pd.options.display.max_rows = 7np.random.seed(451)np.random.seed(451)df = pd.read_csv(    <1>    'https://proai.org/baby-names-us.csv.gz')df = pd.read_csv(    <1>    'https://proai.org/baby-names-us.csv.gz')df = pd.read_csv(    <1>    'https://proai.org/baby-names-us.csv.gz')df.to_csv(    <2>    'baby-names-us.csv.gz', compression='gzip')df.to_csv(    <2>    'baby-names-us.csv.gz', compression='gzip')df.to_csv(    <2>    'baby-names-us.csv.gz', compression='gzip')df = df.sample(10_000)    <3>df = df.sample(10_000)    <3>dfdfdf.groupby(['name', 'sex'])['count'].sum()[('Timothy',)]df.groupby(['name', 'sex'])['count'].sum()[('Timothy',)]df = df.set_index(['name', 'sex'])df = df.set_index(['name', 'sex'])groups = df.groupby(['name', 'sex'])groups = df.groupby(['name', 'sex'])counts = groups['count'].sum()counts = groups['count'].sum()countscountsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(use_idf=False,    <1>    analyzer='char', ngram_range=(1, 3))vectorizer = TfidfVectorizer(use_idf=False,    <1>    analyzer='char', ngram_range=(1, 3))vectorizer = TfidfVectorizer(use_idf=False,    <1>    analyzer='char', ngram_range=(1, 3))vectorizervectorizerdf = pd.DataFrame([list(tup) for tup in counts.index.values],                  columns=['name', 'sex'])df = pd.DataFrame([list(tup) for tup in counts.index.values],                  columns=['name', 'sex'])df = pd.DataFrame([list(tup) for tup in counts.index.values],                  columns=['name', 'sex'])df['count'] = counts.valuesdf['count'] = counts.valuesdfdfdf['istrain'] = np.random.rand(len(df)) < .9df['istrain'] = np.random.rand(len(df)) < .9dfdfdf.index = pd.MultiIndex.from_tuples(    zip(df['name'], df['sex']), names=['name_', 'sex_'])df.index = pd.MultiIndex.from_tuples(    zip(df['name'], df['sex']), names=['name_', 'sex_'])df.index = pd.MultiIndex.from_tuples(    zip(df['name'], df['sex']), names=['name_', 'sex_'])dfdfdf_most_common = {}    <1>df_most_common = {}    <1>for name, group in df.groupby('name'):    row_dict = group.iloc[group['count'].argmax()].to_dict()   <2>    df_most_common[(name, row_dict['sex'])] = row_dictfor name, group in df.groupby('name'):    row_dict = group.iloc[group['count'].argmax()].to_dict()   <2>    df_most_common[(name, row_dict['sex'])] = row_dictfor name, group in df.groupby('name'):    row_dict = group.iloc[group['count'].argmax()].to_dict()   <2>    df_most_common[(name, row_dict['sex'])] = row_dictfor name, group in df.groupby('name'):    row_dict = group.iloc[group['count'].argmax()].to_dict()   <2>    df_most_common[(name, row_dict['sex'])] = row_dictdf_most_common = pd.DataFrame(df_most_common).T    <3>df_most_common = pd.DataFrame(df_most_common).T    <3>df_most_common['istest'] = ~df_most_common['istrain'].astype(bool)df_most_common['istest'] = ~df_most_common['istrain'].astype(bool)df_most_commondf_most_commondf['istest'] = df_most_common['istest']df['istest'] = df_most_common['istest']df['istest'] = df['istest'].fillna(False)df['istest'] = df['istest'].fillna(False)df['istrain'] = ~df['istest']df['istrain'] = ~df['istest']istrain = df['istrain']istrain = df['istrain']df['istrain'].sum() / len(df)df['istrain'].sum() / len(df)df['istest'].sum() / len(df)df['istest'].sum() / len(df)(df['istrain'].sum() + df['istest'].sum()) / len(df)(df['istrain'].sum() + df['istest'].sum()) / len(df)unique_names = df['name'][istrain].unique()unique_names = df['name'][istrain].unique()unique_names = df['name'][istrain].unique()unique_names = df['name'][istrain].unique()vectorizer.fit(unique_names)vectorizer.fit(unique_names)vecs = vectorizer.transform(df['name'])vecs = vectorizer.transform(df['name'])vecsvecsvecs = pd.DataFrame(vecs.toarray())vecs = pd.DataFrame(vecs.toarray())vecs.columns = vectorizer.get_feature_names_out()vecs.columns = vectorizer.get_feature_names_out()vecs.index = df.indexvecs.index = df.indexvecs.iloc[:,:7]vecs.iloc[:,:7]vectorizer = TfidfVectorizer(analyzer='char',   ngram_range=(1, 3), use_idf=False, lowercase=False)vectorizer = TfidfVectorizer(analyzer='char',   ngram_range=(1, 3), use_idf=False, lowercase=False)vectorizer = TfidfVectorizer(analyzer='char',   ngram_range=(1, 3), use_idf=False, lowercase=False)vectorizer = vectorizer.fit(unique_names)vectorizer = vectorizer.fit(unique_names)vecs = vectorizer.transform(df['name'])vecs = vectorizer.transform(df['name'])vecs = pd.DataFrame(vecs.toarray())vecs = pd.DataFrame(vecs.toarray())vecs.columns = vectorizer.get_feature_names_out()vecs.columns = vectorizer.get_feature_names_out()vecs.index = df.indexvecs.index = df.indexvecs.iloc[:,:5]vecs.iloc[:,:5]import pandas as pdimport pandas as pdimport reimport redfs = pd.read_html('https://en.wikipedia.org/wiki/'    + 'Comparison_of_deep-learning_software')dfs = pd.read_html('https://en.wikipedia.org/wiki/'    + 'Comparison_of_deep-learning_software')dfs = pd.read_html('https://en.wikipedia.org/wiki/'    + 'Comparison_of_deep-learning_software')tabl = dfs[0]tabl = dfs[0]bincols = list(tabl.loc[:, 'OpenMP support':].columns)bincols = list(tabl.loc[:, 'OpenMP support':].columns)bincols += ['Open source', 'Platform', 'Interface']bincols += ['Open source', 'Platform', 'Interface']dfd = {}dfd = {}for i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdfor i, row in tabl.iterrows():   rowd = row.fillna('No').to_dict()   for c in bincols:       text = str(rowd[c]).strip().lower()       tokens = re.split(r'\W+', text)       tokens += '\*'       rowd[c] = 0       for kw, score in zip(               'yes via roadmap no linux android python \*'.split(),               [1, .9, .2, 0, 2, 2, 2, .1]):           if kw in tokens:               rowd[c] = score               break   dfd[i] = rowdtabl = pd.DataFrame(dfd).Ttabl = pd.DataFrame(dfd).Tscores = tabl[bincols].T.sum()    <1>scores = tabl[bincols].T.sum()    <1>tabl['Portability'] = scorestabl['Portability'] = scorestabl = tabl.sort_values('Portability', ascending=False)tabl = tabl.sort_values('Portability', ascending=False)tabl = tabl.reset_index()tabl = tabl.reset_index()tabl[['Software', 'Portability']][:10]tabl[['Software', 'Portability']][:10]import torchimport torchclass LogisticRegressionNN(torch.nn.Module):class LogisticRegressionNN(torch.nn.Module):model = LogisticRegressionNN(num_features=vecs.shape[1], num_outputs=1)model = LogisticRegressionNN(num_features=vecs.shape[1], num_outputs=1)modelmodelloss_func_train = torch.nn.BCELoss(    weight=torch.Tensor(df[['count']][istrain].values))loss_func_train = torch.nn.BCELoss(    weight=torch.Tensor(df[['count']][istrain].values))loss_func_train = torch.nn.BCELoss(    weight=torch.Tensor(df[['count']][istrain].values))loss_func_test = torch.nn.BCELoss(   <1>    weight=torch.Tensor(df[['count']][~istrain].values))loss_func_test = torch.nn.BCELoss(   <1>    weight=torch.Tensor(df[['count']][~istrain].values))loss_func_test = torch.nn.BCELoss(   <1>    weight=torch.Tensor(df[['count']][~istrain].values))loss_func_trainloss_func_trainfrom torch.optim import SGDfrom torch.optim import SGDhyperparams = {'momentum': 0.001, 'lr': 0.02}    <1>hyperparams = {'momentum': 0.001, 'lr': 0.02}    <1>optimizer = SGD(    model.parameters(), **hyperparams)    <2>optimizer = SGD(    model.parameters(), **hyperparams)    <2>optimizer = SGD(    model.parameters(), **hyperparams)    <2>optimizeroptimizerX = vecs.valuesX = vecs.valuesy = (df[['sex']] == 'F').valuesy = (df[['sex']] == 'F').valuesX_train = torch.Tensor(X[istrain])X_train = torch.Tensor(X[istrain])X_test = torch.Tensor(X[~istrain])X_test = torch.Tensor(X[~istrain])y_train = torch.Tensor(y[istrain])y_train = torch.Tensor(y[istrain])y_test = torch.Tensor(y[~istrain])y_test = torch.Tensor(y[~istrain])from tqdm import tqdmfrom tqdm import tqdmnum_epochs = 200num_epochs = 200pbar_epochs = tqdm(range(num_epochs), desc='Epoch:', total=num_epochs)pbar_epochs = tqdm(range(num_epochs), desc='Epoch:', total=num_epochs)for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>for epoch in pbar_epochs:     optimizer.zero_grad()   <1>     outputs = model(X_train)     loss_train = loss_func_train(outputs, y_train)   <2>     loss_train.backward()   <3>     optimizer.step()   <4>def make_array(x):    if hasattr(x, 'detach'):        return torch.squeeze(x).detach().numpy()    return xdef make_array(x):    if hasattr(x, 'detach'):        return torch.squeeze(x).detach().numpy()    return xdef make_array(x):    if hasattr(x, 'detach'):        return torch.squeeze(x).detach().numpy()    return xdef make_array(x):    if hasattr(x, 'detach'):        return torch.squeeze(x).detach().numpy()    return xdef make_array(x):    if hasattr(x, 'detach'):        return torch.squeeze(x).detach().numpy()    return xdef measure_binary_accuracy(y_pred, y):    y_pred = make_array(y_pred).round()    y = make_array(y).round()    num_correct = (y_pred == y).sum()    return num_correct / len(y)def measure_binary_accuracy(y_pred, y):    y_pred = make_array(y_pred).round()    y = make_array(y).round()    num_correct = (y_pred == y).sum()    return num_correct / len(y)def measure_binary_accuracy(y_pred, y):    y_pred = make_array(y_pred).round()    y = make_array(y).round()    num_correct = (y_pred == y).sum()    return num_correct / len(y)def measure_binary_accuracy(y_pred, y):    y_pred = make_array(y_pred).round()    y = make_array(y).round()    num_correct = (y_pred == y).sum()    return num_correct / len(y)def measure_binary_accuracy(y_pred, y):    y_pred = make_array(y_pred).round()    y = make_array(y).round()    num_correct = (y_pred == y).sum()    return num_correct / len(y)def measure_binary_accuracy(y_pred, y):    y_pred = make_array(y_pred).round()    y = make_array(y).round()    num_correct = (y_pred == y).sum()    return num_correct / len(y)X = vectorizer.transform(    ['John', 'Greg', 'Vishvesh',    <1>     'Ruby', 'Carlana', 'Sarah'])    <2>X = vectorizer.transform(    ['John', 'Greg', 'Vishvesh',    <1>     'Ruby', 'Carlana', 'Sarah'])    <2>X = vectorizer.transform(    ['John', 'Greg', 'Vishvesh',    <1>     'Ruby', 'Carlana', 'Sarah'])    <2>X = vectorizer.transform(    ['John', 'Greg', 'Vishvesh',    <1>     'Ruby', 'Carlana', 'Sarah'])    <2>model(torch.Tensor(X.todense()))model(torch.Tensor(X.todense()))