from nlpia2.chatgpt import send_promptfrom nlpia2.chatgpt import send_promptprompt = "teacher: 9,10,11?\n student: 12\n"prompt = "teacher: 9,10,11?\n student: 12\n"prompt += "teacher: Perfect!\n teacher: 38,39,40?\n"prompt += "teacher: Perfect!\n teacher: 38,39,40?\n"prompt += "student: 42\n teacher: Oops. Not quite. Try again.\n"prompt += "student: 42\n teacher: Oops. Not quite. Try again.\n"prompt += "student: 41\n teacher: Good work! 2,4,6?\n"prompt += "student: 41\n teacher: Good work! 2,4,6?\n"prompt += "student: 8\n teacher: "prompt += "student: 8\n teacher: "print(send_prompt(    model='gpt-3.5-turbo',  # <1>    context_prompt='third_grade', # <2>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',  # <1>    context_prompt='third_grade', # <2>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',  # <1>    context_prompt='third_grade', # <2>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',  # <1>    context_prompt='third_grade', # <2>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',  # <1>    context_prompt='third_grade', # <2>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade', # <1>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade', # <1>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade', # <1>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade', # <1>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade', # <1>    prompt=prompt))print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade',    prompt=prompt))  # <2>print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade',    prompt=prompt))  # <2>print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade',    prompt=prompt))  # <2>print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade',    prompt=prompt))  # <2>print(send_prompt(    model='gpt-3.5-turbo',    context_prompt='third_grade',    prompt=prompt))  # <2>prompt = "\n teacher: 9,10,11? \n student: 12 \n"prompt = "\n teacher: 9,10,11? \n student: 12 \n"prompt +=" teacher: Perfect! \n teacher: 34,36,38? \n"prompt +=" teacher: Perfect! \n teacher: 34,36,38? \n"prompt +=" student: 42 \n"prompt +=" student: 42 \n"prompt +=" teacher: Oops. Not quite right. Try again. \n"prompt +=" teacher: Oops. Not quite right. Try again. \n"prompt +=" student: 42 \n teacher: Good work! 2,4,6? \n student: 8"prompt +=" student: 42 \n teacher: Good work! 2,4,6? \n student: 8"print(send_prompt(prompt, context_prompt='assistant'))print(send_prompt(prompt, context_prompt='assistant'))prompt = "\n teacher: 9, 10, 11? \n student: 12 \n teacher: Perfect! \n teacher: 34, 35, 36? \n student: 38 \n teacher: Oops. Not quite right. Try again. \n student: 37 \n teacher: Good work! 101, 102, 103? \n student: 104"prompt = "\n teacher: 9, 10, 11? \n student: 12 \n teacher: Perfect! \n teacher: 34, 35, 36? \n student: 38 \n teacher: Oops. Not quite right. Try again. \n student: 37 \n teacher: Good work! 101, 102, 103? \n student: 104"send_prompt(prompt)send_prompt(prompt)import numpy as npimport numpy as npnp.random.choice(    'statistical AI stochastic interesting a an in of'.split(),    p=[.18, .17, .15, .1, .1, .1, .1, .1])np.random.choice(    'statistical AI stochastic interesting a an in of'.split(),    p=[.18, .17, .15, .1, .1, .1, .1, .1])np.random.choice(    'statistical AI stochastic interesting a an in of'.split(),    p=[.18, .17, .15, .1, .1, .1, .1, .1])np.random.choice(    'statistical AI stochastic interesting a an in of'.split(),    p=[.18, .17, .15, .1, .1, .1, .1, .1])from transformers import GPT2LMHeadModel, GPT2Tokenizerfrom transformers import GPT2LMHeadModel, GPT2Tokenizerimport torchimport torchimport numpy as np import numpy as np SEED = 42SEED = 42DEVICE = torch.device('cpu')DEVICE = torch.device('cpu')if torch.cuda.is_available():    DEVICE = torch.cuda.device(0)if torch.cuda.is_available():    DEVICE = torch.cuda.device(0)if torch.cuda.is_available():    DEVICE = torch.cuda.device(0)np.random.seed(SEED)np.random.seed(SEED)torch.manual_seed(SEED)torch.manual_seed(SEED)torch.cuda.manual_seed_all(SEED) # <1>torch.cuda.manual_seed_all(SEED) # <1>from transformers import set_seedfrom transformers import set_seedset_seed(SEED)set_seed(SEED)tokenizer = GPT2Tokenizer.from_pretrained('gpt2')tokenizer = GPT2Tokenizer.from_pretrained('gpt2')tokenizer.pad_token = tokenizer.eos_token  # <1>tokenizer.pad_token = tokenizer.eos_token  # <1>vanilla_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')vanilla_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')def generate(prompt,       model=vanilla_gpt2,       tokenizer=tokenizer,       device=DEVICE, **kwargs):def generate(prompt,       model=vanilla_gpt2,       tokenizer=tokenizer,       device=DEVICE, **kwargs):def generate(prompt,       model=vanilla_gpt2,       tokenizer=tokenizer,       device=DEVICE, **kwargs):def generate(prompt,       model=vanilla_gpt2,       tokenizer=tokenizer,       device=DEVICE, **kwargs):def generate(prompt,       model=vanilla_gpt2,       tokenizer=tokenizer,       device=DEVICE, **kwargs):   encoded_prompt = tokenizer.encode(       prompt, return_tensors='pt')   encoded_prompt = tokenizer.encode(       prompt, return_tensors='pt')   encoded_prompt = tokenizer.encode(       prompt, return_tensors='pt')   encoded_prompt = encoded_prompt.to(device)   encoded_prompt = encoded_prompt.to(device)   encoded_output = model.generate (encoded_prompt, **kwargs)   encoded_output = model.generate (encoded_prompt, **kwargs)   encoded_output = encoded_output.squeeze() # <1>   encoded_output = encoded_output.squeeze() # <1>   decoded_output = tokenizer.decode(encoded_output,       clean_up_tokenization_spaces=True,        skip_special_tokens=True)   decoded_output = tokenizer.decode(encoded_output,       clean_up_tokenization_spaces=True,        skip_special_tokens=True)   decoded_output = tokenizer.decode(encoded_output,       clean_up_tokenization_spaces=True,        skip_special_tokens=True)   decoded_output = tokenizer.decode(encoded_output,       clean_up_tokenization_spaces=True,        skip_special_tokens=True)   return decoded_output   return decoded_outputgenerate(    model=vanilla_gpt2,    tokenizer=tokenizer,    prompt='NLP is',    max_length=50)generate(    model=vanilla_gpt2,    tokenizer=tokenizer,    prompt='NLP is',    max_length=50)generate(    model=vanilla_gpt2,    tokenizer=tokenizer,    prompt='NLP is',    max_length=50)generate(    model=vanilla_gpt2,    tokenizer=tokenizer,    prompt='NLP is',    max_length=50)generate(    model=vanilla_gpt2,    tokenizer=tokenizer,    prompt='NLP is',    max_length=50)generate(    model=vanilla_gpt2,    tokenizer=tokenizer,    prompt='NLP is',    max_length=50)input_ids = tokenizer.encode(prompt, return_tensors="pt")input_ids = tokenizer.encode(prompt, return_tensors="pt")input_ids = input_ids.to(DEVICE)input_ids = input_ids.to(DEVICE)vanilla_gpt2(input_ids=input_ids)vanilla_gpt2(input_ids=input_ids)output = vanilla_gpt2(input_ids=input_ids)output = vanilla_gpt2(input_ids=input_ids)output.logits.shapeoutput.logits.shapeencoded_prompt = tokenizer('NLP is a', return_tensors="pt")encoded_prompt = tokenizer('NLP is a', return_tensors="pt")encoded_prompt = encoded_prompt["input_ids"]encoded_prompt = encoded_prompt["input_ids"]encoded_prompt = encoded_prompt.to(DEVICE)encoded_prompt = encoded_prompt.to(DEVICE)output = vanilla_gpt2(input_ids=encoded_prompt)output = vanilla_gpt2(input_ids=encoded_prompt)next_token_logits = output.logits[0, -1, :]next_token_logits = output.logits[0, -1, :]next_token_probs = torch.softmax(next_token_logits, dim=-1)next_token_probs = torch.softmax(next_token_logits, dim=-1)sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)tokenizer.decode(sorted_ids[0])  # <1>tokenizer.decode(sorted_ids[0])  # <1>tokenizer.decode(sorted_ids[1])  # <2>tokenizer.decode(sorted_ids[1])  # <2>kwargs = {   'do_sample': True,    'max_length': 50,    'top_p': 0.92}kwargs = {   'do_sample': True,    'max_length': 50,    'top_p': 0.92}kwargs = {   'do_sample': True,    'max_length': 50,    'top_p': 0.92}kwargs = {   'do_sample': True,    'max_length': 50,    'top_p': 0.92}kwargs = {   'do_sample': True,    'max_length': 50,    'top_p': 0.92}kwargs = {   'do_sample': True,    'max_length': 50,    'top_p': 0.92}print(generate(prompt='NLP is a', **kwargs))print(generate(prompt='NLP is a', **kwargs))import pandas as pdimport pandas as pdDATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')df = pd.read_csv(DATASET_URL)df = pd.read_csv(DATASET_URL)df = df[df['is_text']]df = df[df['is_text']]lines = df.line_text.copy() lines = df.line_text.copy() from torch.utils.data import Datasetfrom torch.utils.data import Datasetfrom torch.utils.data import random_split from torch.utils.data import random_split class NLPiADataset(Dataset):class NLPiADataset(Dataset):    def __init__(self, txt_list, tokenizer, max_length=768):    def __init__(self, txt_list, tokenizer, max_length=768):        self.tokenizer = tokenizer        self.tokenizer = tokenizer        self.input_ids = []        self.input_ids = []        self.attn_masks = []        self.attn_masks = []        for txt in txt_list:        for txt in txt_list:            encodings_dict = tokenizer(txt, truncation=True,                max_length=max_length, padding="max_length")            encodings_dict = tokenizer(txt, truncation=True,                max_length=max_length, padding="max_length")            encodings_dict = tokenizer(txt, truncation=True,                max_length=max_length, padding="max_length")            self.input_ids.append(                torch.tensor(encodings_dict['input_ids']))            self.input_ids.append(                torch.tensor(encodings_dict['input_ids']))            self.input_ids.append(                torch.tensor(encodings_dict['input_ids']))    def __len__(self):    def __len__(self):        return len(self.input_ids)        return len(self.input_ids)    def __getitem__(self, idx):    def __getitem__(self, idx):        return self.input_ids[idx]        return self.input_ids[idx]dataset = NLPiADataset(lines, tokenizer, max_length=768)dataset = NLPiADataset(lines, tokenizer, max_length=768)train_size = int(0.9 * len(dataset))train_size = int(0.9 * len(dataset))eval_size = len(dataset) - train_sizeeval_size = len(dataset) - train_sizetrain_dataset, eval_dataset = random_split(    dataset, [train_size, eval_size])train_dataset, eval_dataset = random_split(    dataset, [train_size, eval_size])train_dataset, eval_dataset = random_split(    dataset, [train_size, eval_size])from nlpia2.constants import DATA_DIR  # <1>from nlpia2.constants import DATA_DIR  # <1>from transformers import TrainingArgumentsfrom transformers import TrainingArgumentsfrom transformers import DataCollatorForLanguageModelingfrom transformers import DataCollatorForLanguageModelingtraining_args = TrainingArguments(   output_dir=DATA_DIR / 'ch10_checkpoints',   per_device_train_batch_size=5,   num_train_epochs=5,   save_strategy='epoch')training_args = TrainingArguments(   output_dir=DATA_DIR / 'ch10_checkpoints',   per_device_train_batch_size=5,   num_train_epochs=5,   save_strategy='epoch')training_args = TrainingArguments(   output_dir=DATA_DIR / 'ch10_checkpoints',   per_device_train_batch_size=5,   num_train_epochs=5,   save_strategy='epoch')training_args = TrainingArguments(   output_dir=DATA_DIR / 'ch10_checkpoints',   per_device_train_batch_size=5,   num_train_epochs=5,   save_strategy='epoch')training_args = TrainingArguments(   output_dir=DATA_DIR / 'ch10_checkpoints',   per_device_train_batch_size=5,   num_train_epochs=5,   save_strategy='epoch')training_args = TrainingArguments(   output_dir=DATA_DIR / 'ch10_checkpoints',   per_device_train_batch_size=5,   num_train_epochs=5,   save_strategy='epoch')collator = DataCollatorForLanguageModeling(    tokenizer=tokenizer, mlm=False)  # <2>collator = DataCollatorForLanguageModeling(    tokenizer=tokenizer, mlm=False)  # <2>collator = DataCollatorForLanguageModeling(    tokenizer=tokenizer, mlm=False)  # <2>from transformers import Trainerfrom transformers import Trainermodel = GPT2LMHeadModel.from_pretrained("gpt2")  # <1>model = GPT2LMHeadModel.from_pretrained("gpt2")  # <1>trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer = Trainer(       model,       training_args,       data_collator=collator,       # <2>       train_dataset=train_dataset,  # <3>       eval_dataset=eval_dataset)trainer.train()        trainer.train()        generate('NLP is')generate('NLP is')print(generate("Neural networks", **nucleus_sampling_args))print(generate("Neural networks", **nucleus_sampling_args))print(generate("Neural networks", **nucleus_sampling_args))print(generate("Neural networks", **nucleus_sampling_args))import numpy as npimport numpy as npv = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])type(v[0])type(v[0])(v * 1_000_000).astype(np.int32)(v * 1_000_000).astype(np.int32)v = (v * 1_000_000).astype(np.int32)  # <1>v = (v * 1_000_000).astype(np.int32)  # <1>v = (v + v) // 2v = (v + v) // 2v / 1_000_000v / 1_000_000v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])v = (v * 10_000).astype(np.int16)  # <1>v = (v * 10_000).astype(np.int16)  # <1>v = (v + v) // 2v = (v + v) // 2v / 10_000v / 10_000v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])v = np.array([1.1, 2.22, 3.333, 4.4444, 5.55555])v = (v * 1_000).astype(np.int16)  # <3>v = (v * 1_000).astype(np.int16)  # <3>v = (v + v) // 2v = (v + v) // 2v / 1_000v / 1_000import pandas as pdimport pandas as pdDATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'    '-/raw/main/src/nlpia2/data/nlpia_lines.csv')df = pd.read_csv(DATASET_URL)df = pd.read_csv(DATASET_URL)df = df[df['is_text']]df = df[df['is_text']]from haystack import Documentfrom haystack import Documenttitles = list(df["line_text"].values)titles = list(df["line_text"].values)texts = list(df["line_text"].values)texts = list(df["line_text"].values)documents = []documents = []for title, text in zip(titles, texts):   documents.append(Document(content=text, meta={"name": title or ""}))for title, text in zip(titles, texts):   documents.append(Document(content=text, meta={"name": title or ""}))for title, text in zip(titles, texts):   documents.append(Document(content=text, meta={"name": title or ""}))documents[0] documents[0] from haystack.document_stores import FAISSDocumentStorefrom haystack.document_stores import FAISSDocumentStoredocument_store = FAISSDocumentStore(faiss_index_factory_str="HNSW",                                     return_embedding=True)document_store = FAISSDocumentStore(faiss_index_factory_str="HNSW",                                     return_embedding=True)document_store = FAISSDocumentStore(faiss_index_factory_str="HNSW",                                     return_embedding=True)document_store.write_documents(documents)document_store.write_documents(documents)from haystack.nodes import TransformersReader, EmbeddingRetrieverfrom haystack.nodes import TransformersReader, EmbeddingRetrieverreader = TransformersReader(model_name_or_path="deepset/roberta-base-squad2")  # <1>reader = TransformersReader(model_name_or_path="deepset/roberta-base-squad2")  # <1>retriever = EmbeddingRetriever(   document_store=document_store,    embedding_model="sentence-transformers/multi-qa-mpnet-base-dot-v1")retriever = EmbeddingRetriever(   document_store=document_store,    embedding_model="sentence-transformers/multi-qa-mpnet-base-dot-v1")retriever = EmbeddingRetriever(   document_store=document_store,    embedding_model="sentence-transformers/multi-qa-mpnet-base-dot-v1")retriever = EmbeddingRetriever(   document_store=document_store,    embedding_model="sentence-transformers/multi-qa-mpnet-base-dot-v1")document_store.update_embeddings(retriever=retriever)document_store.update_embeddings(retriever=retriever)from haystack.pipelines import Pipeline from haystack.pipelines import Pipeline pipe = Pipeline()pipe = Pipeline()pipe.add_node(component=retriever, name="Retriever", inputs=["Query"])pipe.add_node(component=retriever, name="Retriever", inputs=["Query"])pipe.add_node(component=reader, name="Reader", inputs=["Retriever"])pipe.add_node(component=reader, name="Reader", inputs=["Retriever"])from haystack.pipelines import ExtractiveQAPipeline from haystack.pipelines import ExtractiveQAPipeline pipe= ExtractiveQAPipeline(reader, retriever)pipe= ExtractiveQAPipeline(reader, retriever)question = "What is an embedding?"question = "What is an embedding?"result = pipe.run(query=question,   params={"Generator": {"top_k": 1}, "Retriever": {"top_k": 5}})result = pipe.run(query=question,   params={"Generator": {"top_k": 1}, "Retriever": {"top_k": 5}})result = pipe.run(query=question,   params={"Generator": {"top_k": 1}, "Retriever": {"top_k": 5}})print_answers(result, details='minimum')print_answers(result, details='minimum')from haystack.nodes import Seq2SeqGeneratorfrom haystack.nodes import Seq2SeqGeneratorfrom haystack.pipelines import GenerativeQAPipelinefrom haystack.pipelines import GenerativeQAPipelinegenerator = Seq2SeqGenerator(    model_name_or_path="vblagoje/bart_lfqa",    max_length=200)generator = Seq2SeqGenerator(    model_name_or_path="vblagoje/bart_lfqa",    max_length=200)generator = Seq2SeqGenerator(    model_name_or_path="vblagoje/bart_lfqa",    max_length=200)generator = Seq2SeqGenerator(    model_name_or_path="vblagoje/bart_lfqa",    max_length=200)pipe = GenerativeQAPipeline(generator, retriever)pipe = GenerativeQAPipeline(generator, retriever)question = "How CNNs are different from RNNs"question = "How CNNs are different from RNNs"result = pipe.run( query=question,        params={"Retriever": {"top_k": 10}})  # <1>result = pipe.run( query=question,        params={"Retriever": {"top_k": 10}})  # <1>result = pipe.run( query=question,        params={"Retriever": {"top_k": 10}})  # <1>print_answers(result, details='medium')print_answers(result, details='medium')question = "How can artificial intelligence save the world"question = "How can artificial intelligence save the world"result = pipe.run(    query="How can artificial intelligence save the world",    params={"Retriever": {"top_k": 10}})result = pipe.run(    query="How can artificial intelligence save the world",    params={"Retriever": {"top_k": 10}})result = pipe.run(    query="How can artificial intelligence save the world",    params={"Retriever": {"top_k": 10}})result = pipe.run(    query="How can artificial intelligence save the world",    params={"Retriever": {"top_k": 10}})resultresultimport streamlit as stimport streamlit as stst.title("Ask me about NLPiA!")st.title("Ask me about NLPiA!")st.markdown("Welcome to the official Question Answering webapp"    "for _Natural Language Processing in Action, 2nd Ed_")st.markdown("Welcome to the official Question Answering webapp"    "for _Natural Language Processing in Action, 2nd Ed_")st.markdown("Welcome to the official Question Answering webapp"    "for _Natural Language Processing in Action, 2nd Ed_")question = st.text_input("Enter your question here:")question = st.text_input("Enter your question here:")if question:   st.write(f"You asked: '{question}'")if question:   st.write(f"You asked: '{question}'")if question:   st.write(f"You asked: '{question}'")import wikipedia as wikiimport wikipedia as wikiwiki.page("AI")wiki.page("AI")from nlpia2_wikipedia import wikipediafrom nlpia2_wikipedia import wikipediapage = wikipedia.page('AI')page = wikipedia.page('AI')page.titlepage.titleprint(page.content)print(page.content)wikipedia.search('AI')wikipedia.search('AI')wikipedia.set_lang('zh')wikipedia.set_lang('zh')wikipedia.search('AI')wikipedia.search('AI')