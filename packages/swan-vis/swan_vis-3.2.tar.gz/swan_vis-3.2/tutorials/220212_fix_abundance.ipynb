{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vital-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swan_vis as swan\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stable-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in graph from /Users/fairliereese/mortazavi_lab/data/mousewg/adrenal/lr_splitseq/swan/swan.p\n"
     ]
    }
   ],
   "source": [
    "sg = swan.read('/Users/fairliereese/mortazavi_lab/data/mousewg/adrenal/lr_splitseq/swan/swan.p')\n",
    "\n",
    "c_dict = {'Medulla_NE':'#97578a',\n",
    "                    'Medulla_EPI':'#339470',\n",
    "                    'Sox10+':'#753b74',\n",
    "                    'Stromal':'#e2969b',\n",
    "                    'Adipocytes':'#e25e2c',\n",
    "                    'Hepatocyte':'#ad7797',\n",
    "                    'Smooth_muscle':'#86b84d',\n",
    "                    'Macrophage':'#da5774',\n",
    "                    'Endothelial':'#a63b4c',\n",
    "                    'Cortex/Endothelial':'#f99d26',\n",
    "                    'Cortex_ZF':'#f0c130',\n",
    "                    'Cortex_ZG':'#b2373a',\n",
    "                    'Cortex_cycling':'#3f4075',\n",
    "                    'X_zone':'#fade7c',\n",
    "                    'Y_zone':'#e47381',\n",
    "                    'Capsule':'#236d88',\n",
    "                    'Other':'grey'}\n",
    "order = order =  ['Medulla_NE','Medulla_EPI','Sox10+',\n",
    "                  'Stromal','Adipocytes','Hepatocyte',\n",
    "                  'Smooth_muscle','Macrophage','Endothelial',\n",
    "                  'Cortex/Endothelial','Cortex_ZF','Cortex_ZG',\n",
    "                  'Cortex_cycling','X_zone','Y_zone',\n",
    "                  'Capsule','Other']\n",
    "\n",
    "# temp fix for x zone and y zone names \n",
    "# till I can regenerate swan graph\n",
    "m = {'X-Zone': 'X_zone', \n",
    "     'Y-Zone': 'Y_zone'}\n",
    "zones = ['X-Zone', 'Y-Zone']\n",
    "sg.adata.obs.loc[sg.adata.obs.lr_celltype.isin(zones), 'lr_celltype'] = sg.adata.obs.loc[sg.adata.obs.lr_celltype.isin(zones)].lr_celltype.map(m)\n",
    "sg.set_metadata_colors('lr_celltype', c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "headed-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_on_gene_sg(sg, gid=None, datasets=None):\n",
    "    \"\"\"\n",
    "    Subset the swan Graph on a given gene and return the subset graph.\n",
    "\n",
    "    Parameters:\n",
    "        gid (str): Gene ID to subset on\n",
    "        datasets (list of str): List of datasets to keep in the subset\n",
    "\n",
    "    returns:\n",
    "        subset_sg (swan Graph): Swan Graph subset on the input gene.\n",
    "    \"\"\"\n",
    "\n",
    "    # didn't ask for either\n",
    "    if not gid and not datasets:\n",
    "        return sg\n",
    "\n",
    "    # subset on gene\n",
    "    if gid:\n",
    "        # make sure this gid is even in the Graph\n",
    "        sg.check_gene(gid)\n",
    "\n",
    "        # get the strand\n",
    "        strand = sg.get_strand_from_gid(gid)\n",
    "\n",
    "        # subset t_df first, it's the easiest\n",
    "        tids = sg.t_df.loc[sg.t_df.gid == gid].index.tolist()\n",
    "        t_df = sg.t_df.loc[tids].copy(deep=True)\n",
    "        t_df['path'] = sg.t_df.loc[tids].apply(\n",
    "                lambda x: copy.deepcopy(x.path), axis=1)\n",
    "        t_df['loc_path'] = sg.t_df.loc[tids].apply(\n",
    "                lambda x: copy.deepcopy(x.loc_path), axis=1)\n",
    "        \n",
    "        # since we don't keep all transcripts in adata, make\n",
    "        # sure to pare that down\n",
    "        tids = list(set(tids)&set(sg.adata.var.index.tolist()))\n",
    "\n",
    "        # subset loc_df based on all the locs that are in the paths from\n",
    "        # the already-subset t_df\n",
    "        paths = t_df['loc_path'].tolist()\n",
    "        locs = [node for path in paths for node in path]\n",
    "        locs = np.unique(locs)\n",
    "        loc_df = sg.loc_df.loc[locs].copy(deep=True)\n",
    "\n",
    "        # subset edge_df based on all the edges that are in the paths from\n",
    "        # the alread-subset t_df\n",
    "        paths = t_df['path'].tolist()\n",
    "        edges = [node for path in paths for node in path]\n",
    "        edges = np.unique(edges)\n",
    "        edge_df = sg.edge_df.loc[edges].copy(deep=True)\n",
    "    if not gid:\n",
    "        t_df = sg.t_df.copy(deep=True)\n",
    "        edge_df = sg.edge_df.copy(deep=True)\n",
    "        loc_df = sg.loc_df.copy(deep=True)\n",
    "\n",
    "    # also subset anndata\n",
    "    # if obs_col and obs_cats:\n",
    "    # \t# adatas = [sg.adata, sg.edge_adata,\n",
    "    # \t# \t\t  sg.tss_adata, sg.tes_adata]\n",
    "    # \t# for adata in adatas:\n",
    "    # \t# print(obs_col)\n",
    "    # \t# print(obs_cats)\n",
    "    # \tobs_vars = sg.adata.obs.loc[sg.adata.obs[obs_col].isin(obs_cats)]\n",
    "    # \tobs_vars = obs_vars.index.tolist()\n",
    "    # \t# print(obs_vars)\n",
    "    # \t# print(tids)\n",
    "    # \tadata = sg.adata[obs_vars, tids]\n",
    "    new_adatas = dict()\n",
    "    # adatas = {'iso': sg.adata, 'edge': sg.edge_adata,\n",
    "    # \t\t  'tss': sg.tss_adata, 'tes': sg.tes_adata}\n",
    "    adatas = {'iso': sg.adata}\n",
    "    for key, adata in adatas.items():\n",
    "        if datasets and gid:\n",
    "            new_adatas[key] = adata[datasets, tids]\n",
    "        elif gid:\n",
    "            new_adatas[key] = adata[:, tids]\n",
    "        elif datasets:\n",
    "            new_adatas[key] = adata[datasets, :]\n",
    "        else:\n",
    "            new_adatas[key] = adata\n",
    "\n",
    "    # create a new graph that's been subset\n",
    "    subset_sg = swan.SwanGraph()\n",
    "    subset_sg.loc_df = loc_df\n",
    "    subset_sg.edge_df = edge_df\n",
    "    subset_sg.t_df = t_df\n",
    "    subset_sg.adata = new_adatas['iso']\n",
    "    # subset_sg.edge_adata = new_adatas['edge']\n",
    "    # subset_sg.tss_adata = new_adatas['tss']\n",
    "    # subset_sg.tes_adata = new_adatas['tes']\n",
    "    subset_sg.datasets = subset_sg.adata.obs.index.tolist()\n",
    "    subset_sg.abundance = sg.abundance\n",
    "    subset_sg.sc = sg.sc\n",
    "    subset_sg.pg = sg.pg\n",
    "    subset_sg.annotation = sg.annotation\n",
    "\n",
    "    # renumber locs if using a gene\n",
    "    if gid:\n",
    "        if strand == '-':\n",
    "            id_map = subset_sg.get_ordered_id_map(rev_strand=True)\n",
    "            subset_sg.update_ids(id_map)\n",
    "        else:\n",
    "            subset_sg.update_ids()\n",
    "\n",
    "        subset_sg.get_loc_types()\n",
    "\n",
    "    # finally create the graph\n",
    "    subset_sg.create_graph_from_dfs()\n",
    "\n",
    "    return subset_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making gen report work \n",
    "# check if groupby column is present\n",
    "def gen_report(self,\n",
    "               gid,\n",
    "               prefix,\n",
    "               datasets=None,\n",
    "               groupby=None,\n",
    "               metadata_cols=None,\n",
    "               novelty=False,\n",
    "               layer='tpm', # choose from tpm, pi\n",
    "               cmap='Spectral_r',\n",
    "               include_qvals=False,\n",
    "               q=0.05,\n",
    "               log2fc=1,\n",
    "               qval_obs_col=None,\n",
    "               qval_obs_conditions=None,\n",
    "               include_unexpressed=False,\n",
    "               indicate_novel=False,\n",
    "               display_numbers=False,\n",
    "               transcript_name=False,\n",
    "               browser=False,\n",
    "               order='expression'):\n",
    "    multi_groupby = False\n",
    "    indicate_dataset = False\n",
    "    if groupby:\n",
    "        # grouping by more than one column\n",
    "        if type(groupby) == list and len(groupby) > 1:\n",
    "            for g in groupby:\n",
    "                if g not in self.adata.obs.columns.tolist():\n",
    "                    raise Exception('Groupby column {} not found'.format(g))\n",
    "            groupby = self.add_multi_groupby(groupby)\n",
    "            multi_groupby = True\n",
    "        elif groupby not in self.adata.obs.columns.tolist():\n",
    "            raise Exception('Groupby column {} not found'.format(groupby))\n",
    "\n",
    "\n",
    "    # check if metadata columns are present\n",
    "    if metadata_cols:\n",
    "        for c in metadata_cols:\n",
    "            if c not in self.adata.obs.columns.tolist():\n",
    "                raise Exception('Metadata column {} not found'.format(c))\n",
    "\n",
    "            # if we're grouping by a certain variable, make sure\n",
    "            # the other metadata cols we plan on plotting have unique\n",
    "            # mappings to the other columns. if just grouping by dataset,\n",
    "            # since each dataset is unique, that's ok\n",
    "            if groupby and groupby != 'dataset':\n",
    "                if groupby == c:\n",
    "                    continue\n",
    "\n",
    "                temp = self.adata.obs[[groupby, c, 'dataset']].copy(deep=True)\n",
    "                temp = temp.groupby([groupby, c]).count().reset_index()\n",
    "                temp = temp.loc[~temp.dataset.isnull()]\n",
    "\n",
    "                # if there are duplicates from the metadata column, throw exception\n",
    "                if temp[groupby].duplicated().any():\n",
    "                        raise Exception('Metadata column {} '.format(c)+\\\n",
    "                            'not compatible with groupby column {}. '.format(groupby)+\\\n",
    "                            'Groupby column has more than 1 unique possible '+\\\n",
    "                            'value from metadata column.')\n",
    "\n",
    "    # check to see if input gene is in the graph\n",
    "    if gid not in self.t_df.gid.tolist():\n",
    "        gid = self.get_gid_from_gname(gid)\n",
    "    self.check_gene(gid)\n",
    "\n",
    "    # check to see if these plotting settings will play together\n",
    "    self.check_plotting_args(indicate_dataset,\n",
    "        indicate_novel, browser)\n",
    "\n",
    "    # get the list of columns to include from the input datasets dict\n",
    "    if datasets:\n",
    "        # get a df that is subset of metadata\n",
    "        # also sort the datasets based on the order they appear in \"datasets\"\n",
    "        i = 0\n",
    "        sorters = []\n",
    "        for meta_col, meta_cats in datasets.items():\n",
    "            if meta_col not in self.adata.obs.columns.tolist():\n",
    "                raise Exception('Metadata column {} not found'.format(meta_col))\n",
    "            if type(meta_cats) == str:\n",
    "                meta_cats = [meta_cats]\n",
    "            if i == 0:\n",
    "                temp = self.adata.obs.loc[self.adata.obs[meta_col].isin(meta_cats)]\n",
    "            else:\n",
    "                temp = temp.loc[temp[meta_col].isin(meta_cats)]\n",
    "            sort_ind = dict(zip(meta_cats, range(len(meta_cats))))\n",
    "            sort_col = '{}_sort'.format(meta_col)\n",
    "            temp[sort_col] = temp[meta_col].map(sort_ind).astype(int)\n",
    "            sorters.append(sort_col)\n",
    "            i += 1\n",
    "\n",
    "        # sort the df based on the order that different categories appear in \"datasets\"\n",
    "        temp.sort_values(by=sorters, inplace=True, ascending=True)\n",
    "        temp.drop(sorters, axis=1, inplace=True)\n",
    "        columns = temp.dataset.tolist()\n",
    "        del temp\n",
    "    else:\n",
    "        columns = None\n",
    "\n",
    "    # if we've asked for novelty first check to make sure it's there\n",
    "    if novelty:\n",
    "        if not self.has_novelty():\n",
    "            raise Exception('No novelty information present in the graph. '\n",
    "                'Add it or do not use the \"novelty\" report option.')\n",
    "            \n",
    "#     print('finished checking all plotting args')\n",
    "\n",
    "    # abundance info to calculate TPM on - subset on datasets that will\n",
    "    # be included\n",
    "    if columns or datasets:\n",
    "        subset_adata = subset_on_gene_sg(sg, datasets=columns).adata\n",
    "    else:\n",
    "        subset_adata = self.adata\n",
    "\n",
    "    # small SwanGraph with only this gene's data\n",
    "    sg = subset_on_gene_sg(sg, gid=gid, datasets=columns)\n",
    "\n",
    "    # if we're grouping data, calculate those new numbers\n",
    "    # additionally order transcripts\n",
    "    if groupby:\n",
    "        if layer == 'tpm':\n",
    "            # use whole adata to calc tpm\n",
    "            t_df = tpm_df = calc_tpm(subset_adata, obs_col=groupby).transpose()\n",
    "#             print('finished calculating tpm')\n",
    "        elif layer == 'pi':\n",
    "            # calc tpm just so we can order based on exp\n",
    "            tpm_df = calc_tpm(subset_adata, obs_col=groupby).transpose()\n",
    "#             print('finished calculating pi')\n",
    "            t_df, _ = calc_pi(self.adata, self.t_df, obs_col=groupby)\n",
    "#             print('finished calculating tpm and pi')\n",
    "            t_df = t_df.transpose()\n",
    "            \n",
    "    else:\n",
    "        if layer == 'tpm':\n",
    "            # use whole adata to calc tpm\n",
    "            t_df = tpm_df = self.get_tpm().transpose()\n",
    "            t_df = t_df[subset_adata.obs.dataset.tolist()]\n",
    "#             print('finished calculating tpm')\n",
    "        elif layer == 'pi':\n",
    "            # calc tpm just so we can order based on exp\n",
    "            t_df = tpm_df = self.get_tpm().transpose()\n",
    "            t_df = t_df[subset_adata.obs.dataset.tolist()]\n",
    "            t_df, _ = calc_pi(self.adata, self.t_df)\n",
    "#             print('finished calculating and pi')\n",
    "            t_df = t_df.transpose()\n",
    "\n",
    "    # order transcripts by user's preferences\n",
    "    if order == 'expression' and self.abundance == False:\n",
    "        order = 'tid'\n",
    "    elif order == 'expression':\n",
    "        order = 'log2tpm'\n",
    "    tids = self.t_df.loc[self.t_df.gid == gid].index.tolist()\n",
    "    tids = list(set(tids)&set(tpm_df.index.tolist()))\n",
    "    tpm_df = tpm_df.loc[tids]\n",
    "    _, tids = self.order_transcripts_subset(tpm_df, order=order)\n",
    "#     print('finished ordering transcripts')\n",
    "    del tpm_df\n",
    "    t_df = t_df.loc[tids]\n",
    "\n",
    "    # remove unexpressed transcripts if desired\n",
    "    if not include_unexpressed:\n",
    "        t_df = t_df.loc[t_df.any(axis=1)]\n",
    "\n",
    "    # make sure de has been run if needed\n",
    "    if include_qvals:\n",
    "        uns_key = make_uns_key(kind='det',\n",
    "                               obs_col=qval_obs_col,\n",
    "                               obs_conditions=qval_obs_conditions)\n",
    "        qval_df = self.adata.uns[uns_key].copy(deep=True)\n",
    "        qval_df['significant'] = (qval_df.qval <= q)&(qval_df.log2fc >= log2fc)\n",
    "    else:\n",
    "        qval_df = None\n",
    "\n",
    "    # get tids in this report\n",
    "    report_tids = t_df.index.tolist()\n",
    "\n",
    "    # plot each transcript with these settings\n",
    "    print()\n",
    "    print('Plotting transcripts for {}'.format(gid))\n",
    "    self.plot_each_transcript(report_tids, prefix,\n",
    "                              indicate_dataset,\n",
    "                              indicate_novel,\n",
    "                              browser=browser)\n",
    "#     print('finished plotting each transcript')\n",
    "\n",
    "    # get a different prefix for saving colorbars and scales\n",
    "    gid_prefix = prefix+'_{}'.format(gid)\n",
    "\n",
    "    # if we're plotting tracks, we need a scale as well\n",
    "    # also set what type of report this will be, 'swan' or 'browser'\n",
    "    if browser:\n",
    "        self.pg.plot_browser_scale()\n",
    "        save_fig(gid_prefix+'_browser_scale.png')\n",
    "        report_type = 'browser'\n",
    "    else:\n",
    "        report_type = 'swan'\n",
    "\n",
    "    # plot colorbar for either tpm or pi\n",
    "    if layer == 'tpm':\n",
    "\n",
    "        # take log2(tpm) (add pseudocounts)\n",
    "        t_df = np.log2(t_df+1)\n",
    "\n",
    "        # min and max tpm vals\n",
    "        g_max = t_df.max().max()\n",
    "        g_min = t_df.min().min()\n",
    "\n",
    "        # create a colorbar\n",
    "        plt.rcParams.update({'font.size': 30})\n",
    "        fig, ax = plt.subplots(figsize=(14, 1.5))\n",
    "        fig.subplots_adjust(bottom=0.5)\n",
    "        fig.patch.set_visible(False)\n",
    "        ax.patch.set_visible(False)\n",
    "\n",
    "        try:\n",
    "            cmap = plt.get_cmap(cmap)\n",
    "        except:\n",
    "            raise ValueError('Colormap {} not found'.format(cmap))\n",
    "\n",
    "        norm = mpl.colors.Normalize(vmin=g_min, vmax=g_max)\n",
    "\n",
    "        cb = mpl.colorbar.ColorbarBase(ax,\n",
    "                            cmap=cmap,\n",
    "                            norm=norm,\n",
    "                            orientation='horizontal')\n",
    "        cb.set_label('log2(TPM)')\n",
    "        plt.savefig(gid_prefix+'_colorbar_scale.png', format='png',\n",
    "            bbox_inches='tight', dpi=200)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "    elif layer == 'pi':\n",
    "\n",
    "        # min and max pi vals\n",
    "        g_max = 100\n",
    "        g_min = 0\n",
    "\n",
    "        # create a colorbar between 0 and 1\n",
    "        plt.rcParams.update({'font.size': 30})\n",
    "        fig, ax = plt.subplots(figsize=(14, 1.5))\n",
    "        fig.subplots_adjust(bottom=0.5)\n",
    "        fig.patch.set_visible(False)\n",
    "        ax.patch.set_visible(False)\n",
    "\n",
    "        try:\n",
    "            cmap = plt.get_cmap(cmap)\n",
    "        except:\n",
    "            raise ValueError('Colormap {} not found'.format(cmap))\n",
    "\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=100)\n",
    "\n",
    "        cb = mpl.colorbar.ColorbarBase(ax,\n",
    "                            cmap=cmap,\n",
    "                            norm=norm,\n",
    "                            orientation='horizontal')\n",
    "        cb.set_label('Percent of isoform use (' +'$\\pi$'+')')\n",
    "        plt.savefig(gid_prefix+'_colorbar_scale.png', format='png',\n",
    "            bbox_inches='tight', dpi=200)\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "    # merge with self.t_df to get additional columns\n",
    "    print('tdf')\n",
    "    print(t_df.columns)\n",
    "    datasets = t_df.columns\n",
    "    cols = ['novelty', 'tname']\n",
    "    t_df = t_df.merge(self.t_df[cols], how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # create report\n",
    "    print('Generating report for {}'.format(gid))\n",
    "    pdf_name = create_fname(prefix,\n",
    "                 indicate_dataset,\n",
    "                 indicate_novel,\n",
    "                 browser,\n",
    "                 ftype='report',\n",
    "                 gid=gid)\n",
    "    if transcript_name:\n",
    "        t_disp = 'Transcript Name'\n",
    "    else:\n",
    "        t_disp = 'Transcript ID'\n",
    "    report = Report(gid_prefix,\n",
    "                    report_type,\n",
    "                    self.adata.obs,\n",
    "                    self.adata.uns,\n",
    "                    datasets=datasets,\n",
    "                    groupby=groupby,\n",
    "                    metadata_cols=metadata_cols,\n",
    "                    novelty=novelty,\n",
    "                    layer=layer,\n",
    "                    cmap=cmap,\n",
    "                    g_min=g_min,\n",
    "                    g_max=g_max,\n",
    "                    include_qvals=include_qvals,\n",
    "                    qval_df=qval_df,\n",
    "                    display_numbers=display_numbers,\n",
    "                    t_disp=t_disp)\n",
    "    report.add_page()\n",
    "\n",
    "    # loop through each transcript and add it to the report\n",
    "    for ind, entry in t_df.iterrows():\n",
    "        tid = ind\n",
    "\n",
    "        # display name for transcript\n",
    "        if transcript_name:\n",
    "            t_disp = entry['tname']\n",
    "        else:\n",
    "            t_disp = tid\n",
    "        fname = create_fname(prefix,\n",
    "                             indicate_dataset,\n",
    "                             indicate_novel,\n",
    "                             browser,\n",
    "                             ftype='path',\n",
    "                             tid=tid)\n",
    "        report.add_transcript(entry, fname, t_disp)\n",
    "    report.write_pdf(pdf_name)\n",
    "\n",
    "    # remove multi groupby column if necessary\n",
    "    if multi_groupby:\n",
    "        self.rm_multi_groupby(groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stock-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # making gen report work \n",
    "# # check if groupby column is present\n",
    "# def gen_report(sg,\n",
    "#                gid,\n",
    "#                prefix,\n",
    "#                datasets=None,\n",
    "#                groupby=None,\n",
    "#                metadata_cols=None,\n",
    "#                novelty=False,\n",
    "#                layer='tpm', # choose from tpm, pi\n",
    "#                cmap='Spectral_r',\n",
    "#                include_qvals=False,\n",
    "#                q=0.05,\n",
    "#                log2fc=1,\n",
    "#                qval_obs_col=None,\n",
    "#                qval_obs_conditions=None,\n",
    "#                include_unexpressed=False,\n",
    "#                indicate_novel=False,\n",
    "#                display_numbers=False,\n",
    "#                transcript_name=False,\n",
    "#                browser=False,\n",
    "#                order='expression'):\n",
    "#     multi_groupby = False\n",
    "#     indicate_dataset = False\n",
    "#     if groupby:\n",
    "#         # grouping by more than one column\n",
    "#         if type(groupby) == list and len(groupby) > 1:\n",
    "#             for g in groupby:\n",
    "#                 if g not in self.adata.obs.columns.tolist():\n",
    "#                     raise Exception('Groupby column {} not found'.format(g))\n",
    "#             groupby = self.add_multi_groupby(groupby)\n",
    "#             multi_groupby = True\n",
    "#         elif groupby not in self.adata.obs.columns.tolist():\n",
    "#             raise Exception('Groupby column {} not found'.format(groupby))\n",
    "\n",
    "\n",
    "#     # check if metadata columns are present\n",
    "#     if metadata_cols:\n",
    "#         for c in metadata_cols:\n",
    "#             if c not in self.adata.obs.columns.tolist():\n",
    "#                 raise Exception('Metadata column {} not found'.format(c))\n",
    "\n",
    "#             # if we're grouping by a certain variable, make sure\n",
    "#             # the other metadata cols we plan on plotting have unique\n",
    "#             # mappings to the other columns. if just grouping by dataset,\n",
    "#             # since each dataset is unique, that's ok\n",
    "#             if groupby and groupby != 'dataset':\n",
    "#                 if groupby == c:\n",
    "#                     continue\n",
    "\n",
    "#                 temp = self.adata.obs[[groupby, c, 'dataset']].copy(deep=True)\n",
    "#                 temp = temp.groupby([groupby, c]).count().reset_index()\n",
    "#                 temp = temp.loc[~temp.dataset.isnull()]\n",
    "\n",
    "#                 # if there are duplicates from the metadata column, throw exception\n",
    "#                 if temp[groupby].duplicated().any():\n",
    "#                         raise Exception('Metadata column {} '.format(c)+\\\n",
    "#                             'not compatible with groupby column {}. '.format(groupby)+\\\n",
    "#                             'Groupby column has more than 1 unique possible '+\\\n",
    "#                             'value from metadata column.')\n",
    "\n",
    "#     # check to see if input gene is in the graph\n",
    "#     if gid not in self.t_df.gid.tolist():\n",
    "#         gid = self.get_gid_from_gname(gid)\n",
    "#     self.check_gene(gid)\n",
    "\n",
    "#     # check to see if these plotting settings will play together\n",
    "#     self.check_plotting_args(indicate_dataset,\n",
    "#         indicate_novel, browser)\n",
    "\n",
    "#     # get the list of columns to include from the input datasets dict\n",
    "#     if datasets:\n",
    "#         # get a df that is subset of metadata\n",
    "#         # also sort the datasets based on the order they appear in \"datasets\"\n",
    "#         i = 0\n",
    "#         sorters = []\n",
    "#         for meta_col, meta_cats in datasets.items():\n",
    "#             if meta_col not in self.adata.obs.columns.tolist():\n",
    "#                 raise Exception('Metadata column {} not found'.format(meta_col))\n",
    "#             if type(meta_cats) == str:\n",
    "#                 meta_cats = [meta_cats]\n",
    "#             if i == 0:\n",
    "#                 temp = self.adata.obs.loc[self.adata.obs[meta_col].isin(meta_cats)]\n",
    "#             else:\n",
    "#                 temp = temp.loc[temp[meta_col].isin(meta_cats)]\n",
    "#             sort_ind = dict(zip(meta_cats, range(len(meta_cats))))\n",
    "#             sort_col = '{}_sort'.format(meta_col)\n",
    "#             temp[sort_col] = temp[meta_col].map(sort_ind).astype(int)\n",
    "#             sorters.append(sort_col)\n",
    "#             i += 1\n",
    "\n",
    "#         # sort the df based on the order that different categories appear in \"datasets\"\n",
    "#         temp.sort_values(by=sorters, inplace=True, ascending=True)\n",
    "#         temp.drop(sorters, axis=1, inplace=True)\n",
    "#         columns = temp.dataset.tolist()\n",
    "#         del temp\n",
    "#     else:\n",
    "#         columns = None\n",
    "\n",
    "#     # if we've asked for novelty first check to make sure it's there\n",
    "#     if novelty:\n",
    "#         if not self.has_novelty():\n",
    "#             raise Exception('No novelty information present in the graph. '\n",
    "#                 'Add it or do not use the \"novelty\" report option.')\n",
    "            \n",
    "# #     print('finished checking all plotting args')\n",
    "\n",
    "#     # abundance info to calculate TPM on - subset on datasets that will\n",
    "#     # be included\n",
    "#     if columns or datasets:\n",
    "#         subset_adata = subset_on_gene_sg(sg, datasets=columns).adata\n",
    "#     else:\n",
    "#         subset_adata = self.adata\n",
    "\n",
    "#     # small SwanGraph with only this gene's data\n",
    "#     sg = subset_on_gene_sg(sg, gid=gid, datasets=columns)\n",
    "\n",
    "#     # if we're grouping data, calculate those new numbers\n",
    "#     # additionally order transcripts\n",
    "#     if groupby:\n",
    "#         if layer == 'tpm':\n",
    "#             # use whole adata to calc tpm\n",
    "#             t_df = tpm_df = calc_tpm(subset_adata, obs_col=groupby).transpose()\n",
    "# #             print('finished calculating tpm')\n",
    "#         elif layer == 'pi':\n",
    "#             # calc tpm just so we can order based on exp\n",
    "#             tpm_df = calc_tpm(subset_adata, obs_col=groupby).transpose()\n",
    "# #             print('finished calculating pi')\n",
    "#             t_df, _ = calc_pi(self.adata, self.t_df, obs_col=groupby)\n",
    "# #             print('finished calculating tpm and pi')\n",
    "#             t_df = t_df.transpose()\n",
    "            \n",
    "#     else:\n",
    "#         if layer == 'tpm':\n",
    "#             # use whole adata to calc tpm\n",
    "#             t_df = tpm_df = self.get_tpm().transpose()\n",
    "#             t_df = t_df[subset_adata.obs.dataset.tolist()]\n",
    "# #             print('finished calculating tpm')\n",
    "#         elif layer == 'pi':\n",
    "#             # calc tpm just so we can order based on exp\n",
    "#             t_df = tpm_df = self.get_tpm().transpose()\n",
    "#             t_df = t_df[subset_adata.obs.dataset.tolist()]\n",
    "#             t_df, _ = calc_pi(self.adata, self.t_df)\n",
    "# #             print('finished calculating and pi')\n",
    "#             t_df = t_df.transpose()\n",
    "\n",
    "#     # order transcripts by user's preferences\n",
    "#     if order == 'expression' and self.abundance == False:\n",
    "#         order = 'tid'\n",
    "#     elif order == 'expression':\n",
    "#         order = 'log2tpm'\n",
    "#     tids = self.t_df.loc[self.t_df.gid == gid].index.tolist()\n",
    "#     tids = list(set(tids)&set(tpm_df.index.tolist()))\n",
    "#     tpm_df = tpm_df.loc[tids]\n",
    "#     _, tids = self.order_transcripts_subset(tpm_df, order=order)\n",
    "# #     print('finished ordering transcripts')\n",
    "#     del tpm_df\n",
    "#     t_df = t_df.loc[tids]\n",
    "\n",
    "#     # remove unexpressed transcripts if desired\n",
    "#     if not include_unexpressed:\n",
    "#         t_df = t_df.loc[t_df.any(axis=1)]\n",
    "\n",
    "#     # make sure de has been run if needed\n",
    "#     if include_qvals:\n",
    "#         uns_key = make_uns_key(kind='det',\n",
    "#                                obs_col=qval_obs_col,\n",
    "#                                obs_conditions=qval_obs_conditions)\n",
    "#         qval_df = self.adata.uns[uns_key].copy(deep=True)\n",
    "#         qval_df['significant'] = (qval_df.qval <= q)&(qval_df.log2fc >= log2fc)\n",
    "#     else:\n",
    "#         qval_df = None\n",
    "\n",
    "#     # get tids in this report\n",
    "#     report_tids = t_df.index.tolist()\n",
    "\n",
    "#     # plot each transcript with these settings\n",
    "#     print()\n",
    "#     print('Plotting transcripts for {}'.format(gid))\n",
    "#     self.plot_each_transcript(report_tids, prefix,\n",
    "#                               indicate_dataset,\n",
    "#                               indicate_novel,\n",
    "#                               browser=browser)\n",
    "# #     print('finished plotting each transcript')\n",
    "\n",
    "#     # get a different prefix for saving colorbars and scales\n",
    "#     gid_prefix = prefix+'_{}'.format(gid)\n",
    "\n",
    "#     # if we're plotting tracks, we need a scale as well\n",
    "#     # also set what type of report this will be, 'swan' or 'browser'\n",
    "#     if browser:\n",
    "#         self.pg.plot_browser_scale()\n",
    "#         save_fig(gid_prefix+'_browser_scale.png')\n",
    "#         report_type = 'browser'\n",
    "#     else:\n",
    "#         report_type = 'swan'\n",
    "\n",
    "#     # plot colorbar for either tpm or pi\n",
    "#     if layer == 'tpm':\n",
    "\n",
    "#         # take log2(tpm) (add pseudocounts)\n",
    "#         t_df = np.log2(t_df+1)\n",
    "\n",
    "#         # min and max tpm vals\n",
    "#         g_max = t_df.max().max()\n",
    "#         g_min = t_df.min().min()\n",
    "\n",
    "#         # create a colorbar\n",
    "#         plt.rcParams.update({'font.size': 30})\n",
    "#         fig, ax = plt.subplots(figsize=(14, 1.5))\n",
    "#         fig.subplots_adjust(bottom=0.5)\n",
    "#         fig.patch.set_visible(False)\n",
    "#         ax.patch.set_visible(False)\n",
    "\n",
    "#         try:\n",
    "#             cmap = plt.get_cmap(cmap)\n",
    "#         except:\n",
    "#             raise ValueError('Colormap {} not found'.format(cmap))\n",
    "\n",
    "#         norm = mpl.colors.Normalize(vmin=g_min, vmax=g_max)\n",
    "\n",
    "#         cb = mpl.colorbar.ColorbarBase(ax,\n",
    "#                             cmap=cmap,\n",
    "#                             norm=norm,\n",
    "#                             orientation='horizontal')\n",
    "#         cb.set_label('log2(TPM)')\n",
    "#         plt.savefig(gid_prefix+'_colorbar_scale.png', format='png',\n",
    "#             bbox_inches='tight', dpi=200)\n",
    "#         plt.clf()\n",
    "#         plt.close()\n",
    "\n",
    "#     elif layer == 'pi':\n",
    "\n",
    "#         # min and max pi vals\n",
    "#         g_max = 100\n",
    "#         g_min = 0\n",
    "\n",
    "#         # create a colorbar between 0 and 1\n",
    "#         plt.rcParams.update({'font.size': 30})\n",
    "#         fig, ax = plt.subplots(figsize=(14, 1.5))\n",
    "#         fig.subplots_adjust(bottom=0.5)\n",
    "#         fig.patch.set_visible(False)\n",
    "#         ax.patch.set_visible(False)\n",
    "\n",
    "#         try:\n",
    "#             cmap = plt.get_cmap(cmap)\n",
    "#         except:\n",
    "#             raise ValueError('Colormap {} not found'.format(cmap))\n",
    "\n",
    "#         norm = mpl.colors.Normalize(vmin=0, vmax=100)\n",
    "\n",
    "#         cb = mpl.colorbar.ColorbarBase(ax,\n",
    "#                             cmap=cmap,\n",
    "#                             norm=norm,\n",
    "#                             orientation='horizontal')\n",
    "#         cb.set_label('Percent of isoform use (' +'$\\pi$'+')')\n",
    "#         plt.savefig(gid_prefix+'_colorbar_scale.png', format='png',\n",
    "#             bbox_inches='tight', dpi=200)\n",
    "#         plt.clf()\n",
    "#         plt.close()\n",
    "\n",
    "#     # merge with self.t_df to get additional columns\n",
    "#     print('tdf')\n",
    "#     print(t_df.columns)\n",
    "#     datasets = t_df.columns\n",
    "#     cols = ['novelty', 'tname']\n",
    "#     t_df = t_df.merge(self.t_df[cols], how='left', left_index=True, right_index=True)\n",
    "\n",
    "#     # create report\n",
    "#     print('Generating report for {}'.format(gid))\n",
    "#     pdf_name = create_fname(prefix,\n",
    "#                  indicate_dataset,\n",
    "#                  indicate_novel,\n",
    "#                  browser,\n",
    "#                  ftype='report',\n",
    "#                  gid=gid)\n",
    "#     if transcript_name:\n",
    "#         t_disp = 'Transcript Name'\n",
    "#     else:\n",
    "#         t_disp = 'Transcript ID'\n",
    "#     report = Report(gid_prefix,\n",
    "#                     report_type,\n",
    "#                     self.adata.obs,\n",
    "#                     self.adata.uns,\n",
    "#                     datasets=datasets,\n",
    "#                     groupby=groupby,\n",
    "#                     metadata_cols=metadata_cols,\n",
    "#                     novelty=novelty,\n",
    "#                     layer=layer,\n",
    "#                     cmap=cmap,\n",
    "#                     g_min=g_min,\n",
    "#                     g_max=g_max,\n",
    "#                     include_qvals=include_qvals,\n",
    "#                     qval_df=qval_df,\n",
    "#                     display_numbers=display_numbers,\n",
    "#                     t_disp=t_disp)\n",
    "#     report.add_page()\n",
    "\n",
    "#     # loop through each transcript and add it to the report\n",
    "#     for ind, entry in t_df.iterrows():\n",
    "#         tid = ind\n",
    "\n",
    "#         # display name for transcript\n",
    "#         if transcript_name:\n",
    "#             t_disp = entry['tname']\n",
    "#         else:\n",
    "#             t_disp = tid\n",
    "#         fname = create_fname(prefix,\n",
    "#                              indicate_dataset,\n",
    "#                              indicate_novel,\n",
    "#                              browser,\n",
    "#                              ftype='path',\n",
    "#                              tid=tid)\n",
    "#         report.add_transcript(entry, fname, t_disp)\n",
    "#     report.write_pdf(pdf_name)\n",
    "\n",
    "#     # remove multi groupby column if necessary\n",
    "#     if multi_groupby:\n",
    "#         self.rm_multi_groupby(groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "paperback-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset adata\n",
      "['ATTGGCTCACAAGCTATCTATTAC-ont_b', 'AAGAGATCCGCATACACTGTCCCG-ont_b', 'CATACCAACGAACTTATTACCTCG-ont_b', 'AAGAGATCTGAAGAGACGCGACTA-pb_2ka', 'GTACGCAACCTCTATCTAAATATC-ont_b']\n",
      "sg\n",
      "['ATTGGCTCACAAGCTATCTATTAC-ont_b', 'AAGAGATCCGCATACACTGTCCCG-ont_b', 'CATACCAACGAACTTATTACCTCG-ont_b', 'AAGAGATCTGAAGAGACGCGACTA-pb_2ka', 'GTACGCAACCTCTATCTAAATATC-ont_b']\n",
      "in calc_tpm\n",
      "             ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "lr_celltype                                                            \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "lr_celltype                                                            \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "lr_celltype                                                            \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000165567  ...  ENSMUST00000238135.1  \\\n",
      "lr_celltype                     ...                         \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "\n",
      "             ENSMUST00000238138.1  ENSMUST00000238151.1  ENSMUST00000238154.1  \\\n",
      "lr_celltype                                                                     \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "\n",
      "             ENSMUST00000238158.1  ENSMUST00000238165.1  ENSMUST00000238171.1  \\\n",
      "lr_celltype                                                                     \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "\n",
      "             ENSMUST00000238178.1  ENSMUST00000238179.1  ENSMUST00000238193.1  \n",
      "lr_celltype                                                                    \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "\n",
      "[5 rows x 20912 columns]\n",
      "{'Medulla_NE': 0, 'Medulla_EPI': 1, 'Sox10+': 2, 'Stromal': 3, 'Adipocytes': 4, 'Hepatocyte': 5, 'Smooth_muscle': 6, 'Macrophage': 7, 'Endothelial': 8, 'Cortex_ZF': 9, 'Cortex_ZG': 10, 'Cortex_cycling': 11, 'X_zone': 12, 'Y_zone': 13, 'Capsule': 14, 'Other': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:155: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0           28.02298          20.751194   \n",
      "Capsule                       0.0            0.00000           0.000000   \n",
      "Cortex_ZF                     0.0            0.00000           0.000000   \n",
      "Cortex_ZG                     0.0            0.00000           0.000000   \n",
      "Cortex_cycling                0.0            0.00000           0.000000   \n",
      "\n",
      "                ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes               0.000000           0.000000           0.000000   \n",
      "Capsule                  0.000000           0.000000           0.000000   \n",
      "Cortex_ZF               26.217817           0.000000          32.375031   \n",
      "Cortex_ZG                0.000000           0.000000           5.496197   \n",
      "Cortex_cycling           0.000000         105.820107           0.000000   \n",
      "\n",
      "                ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0                0.0                0.0   \n",
      "Capsule                       0.0                0.0                0.0   \n",
      "Cortex_ZF                     0.0                0.0                0.0   \n",
      "Cortex_ZG                     0.0                0.0                0.0   \n",
      "Cortex_cycling                0.0                0.0                0.0   \n",
      "\n",
      "                ENCODEMT000165567  ...  ENSMUST00000238135.1  \\\n",
      "lr_celltype                        ...                         \n",
      "Adipocytes                    0.0  ...              8.560544   \n",
      "Capsule                       0.0  ...              0.000000   \n",
      "Cortex_ZF                     0.0  ...              0.000000   \n",
      "Cortex_ZG                     0.0  ...              4.328255   \n",
      "Cortex_cycling                0.0  ...              0.000000   \n",
      "\n",
      "                ENSMUST00000238138.1  ENSMUST00000238151.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000              0.000000   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                   0.000000              0.000000   \n",
      "Cortex_ZG                   0.000000              0.000000   \n",
      "Cortex_cycling             27.739248             12.642225   \n",
      "\n",
      "                ENSMUST00000238154.1  ENSMUST00000238158.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                 46.808258              0.000000   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                 166.173355             44.515667   \n",
      "Cortex_ZG                  12.013674             54.721916   \n",
      "Cortex_cycling            102.774925              0.000000   \n",
      "\n",
      "                ENSMUST00000238165.1  ENSMUST00000238171.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000                   0.0   \n",
      "Capsule                     0.000000                   0.0   \n",
      "Cortex_ZF                   0.000000                   0.0   \n",
      "Cortex_ZG                  22.160664                   0.0   \n",
      "Cortex_cycling              0.000000                   0.0   \n",
      "\n",
      "                ENSMUST00000238178.1  ENSMUST00000238179.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000              3.411747   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                   7.317644              0.000000   \n",
      "Cortex_ZG                   0.000000              0.000000   \n",
      "Cortex_cycling              0.000000              0.000000   \n",
      "\n",
      "                ENSMUST00000238193.1  \n",
      "lr_celltype                           \n",
      "Adipocytes                 30.931023  \n",
      "Capsule                     0.000000  \n",
      "Cortex_ZF                  26.624668  \n",
      "Cortex_ZG                  47.613117  \n",
      "Cortex_cycling              0.000000  \n",
      "\n",
      "[5 rows x 20912 columns]\n",
      "                ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0           28.02298          20.751194   \n",
      "Capsule                       0.0            0.00000           0.000000   \n",
      "Cortex_ZF                     0.0            0.00000           0.000000   \n",
      "Cortex_ZG                     0.0            0.00000           0.000000   \n",
      "Cortex_cycling                0.0            0.00000           0.000000   \n",
      "\n",
      "                ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes               0.000000           0.000000           0.000000   \n",
      "Capsule                  0.000000           0.000000           0.000000   \n",
      "Cortex_ZF               26.217817           0.000000          32.375031   \n",
      "Cortex_ZG                0.000000           0.000000           5.496197   \n",
      "Cortex_cycling           0.000000         105.820107           0.000000   \n",
      "\n",
      "                ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0                0.0                0.0   \n",
      "Capsule                       0.0                0.0                0.0   \n",
      "Cortex_ZF                     0.0                0.0                0.0   \n",
      "Cortex_ZG                     0.0                0.0                0.0   \n",
      "Cortex_cycling                0.0                0.0                0.0   \n",
      "\n",
      "                ENCODEMT000165567  ...  ENSMUST00000238138.1  \\\n",
      "lr_celltype                        ...                         \n",
      "Adipocytes                    0.0  ...              0.000000   \n",
      "Capsule                       0.0  ...              0.000000   \n",
      "Cortex_ZF                     0.0  ...              0.000000   \n",
      "Cortex_ZG                     0.0  ...              0.000000   \n",
      "Cortex_cycling                0.0  ...             27.739248   \n",
      "\n",
      "                ENSMUST00000238151.1  ENSMUST00000238154.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000             46.808258   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                   0.000000            166.173355   \n",
      "Cortex_ZG                   0.000000             12.013674   \n",
      "Cortex_cycling             12.642225            102.774925   \n",
      "\n",
      "                ENSMUST00000238158.1  ENSMUST00000238165.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000              0.000000   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                  44.515667              0.000000   \n",
      "Cortex_ZG                  54.721916             22.160664   \n",
      "Cortex_cycling              0.000000              0.000000   \n",
      "\n",
      "                ENSMUST00000238171.1  ENSMUST00000238178.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                       0.0              0.000000   \n",
      "Capsule                          0.0              0.000000   \n",
      "Cortex_ZF                        0.0              7.317644   \n",
      "Cortex_ZG                        0.0              0.000000   \n",
      "Cortex_cycling                   0.0              0.000000   \n",
      "\n",
      "                ENSMUST00000238179.1  ENSMUST00000238193.1  row_order  \n",
      "lr_celltype                                                            \n",
      "Adipocytes                  3.411747             30.931023          4  \n",
      "Capsule                     0.000000              0.000000         14  \n",
      "Cortex_ZF                   0.000000             26.624668          9  \n",
      "Cortex_ZG                   0.000000             47.613117         10  \n",
      "Cortex_cycling              0.000000              0.000000         11  \n",
      "\n",
      "[5 rows x 20913 columns]\n",
      "             ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "Medulla_NE           37.515007            0.00000           0.000000   \n",
      "Medulla_EPI           0.000000            0.00000           0.000000   \n",
      "Sox10+                0.000000            0.00000           0.000000   \n",
      "Stromal               0.000000            0.00000           0.000000   \n",
      "Adipocytes            0.000000           28.02298          20.751194   \n",
      "\n",
      "             ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "Medulla_NE           53.524593                0.0                0.0   \n",
      "Medulla_EPI           0.000000                0.0                0.0   \n",
      "Sox10+                0.000000                0.0                0.0   \n",
      "Stromal              33.030556                0.0                0.0   \n",
      "Adipocytes            0.000000                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "Medulla_NE                 0.0                0.0          15.362636   \n",
      "Medulla_EPI                0.0                0.0           0.000000   \n",
      "Sox10+                     0.0                0.0           0.000000   \n",
      "Stromal                    0.0                0.0           0.000000   \n",
      "Adipocytes                 0.0                0.0           0.000000   \n",
      "\n",
      "             ENCODEMT000165567  ...  ENSMUST00000238135.1  \\\n",
      "Medulla_NE            0.000000  ...              0.000000   \n",
      "Medulla_EPI          18.526754  ...              0.000000   \n",
      "Sox10+                0.000000  ...              0.000000   \n",
      "Stromal               0.000000  ...              0.000000   \n",
      "Adipocytes            0.000000  ...              8.560544   \n",
      "\n",
      "             ENSMUST00000238138.1  ENSMUST00000238151.1  ENSMUST00000238154.1  \\\n",
      "Medulla_NE                    0.0              0.000000            256.934601   \n",
      "Medulla_EPI                   0.0              2.421595            210.482422   \n",
      "Sox10+                        0.0              0.000000              0.000000   \n",
      "Stromal                       0.0              0.000000            297.045593   \n",
      "Adipocytes                    0.0              0.000000             46.808258   \n",
      "\n",
      "             ENSMUST00000238158.1  ENSMUST00000238165.1  ENSMUST00000238171.1  \\\n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_EPI                   0.0                   0.0                   0.0   \n",
      "Sox10+                        0.0                   0.0                   0.0   \n",
      "Stromal                       0.0                   0.0                   0.0   \n",
      "Adipocytes                    0.0                   0.0                   0.0   \n",
      "\n",
      "             ENSMUST00000238178.1  ENSMUST00000238179.1  ENSMUST00000238193.1  \n",
      "Medulla_NE             182.681778              0.000000              0.000000  \n",
      "Medulla_EPI              0.000000              0.000000              0.000000  \n",
      "Sox10+                   0.000000              0.000000              0.000000  \n",
      "Stromal                  0.000000              0.000000            251.319427  \n",
      "Adipocytes               0.000000              3.411747             30.931023  \n",
      "\n",
      "[5 rows x 20912 columns]\n",
      "t_df tpm\n",
      "                   Medulla_NE  Medulla_EPI  Sox10+    Stromal  Adipocytes  \\\n",
      "ENCODEMT000142671   37.515007          0.0     0.0   0.000000    0.000000   \n",
      "ENCODEMT000144512    0.000000          0.0     0.0   0.000000   28.022980   \n",
      "ENCODEMT000145637    0.000000          0.0     0.0   0.000000   20.751194   \n",
      "ENCODEMT000147256   53.524593          0.0     0.0  33.030556    0.000000   \n",
      "ENCODEMT000152127    0.000000          0.0     0.0   0.000000    0.000000   \n",
      "\n",
      "                   Hepatocyte  Smooth_muscle  Macrophage  Endothelial  \\\n",
      "ENCODEMT000142671         0.0            0.0         0.0          0.0   \n",
      "ENCODEMT000144512         0.0            0.0         0.0          0.0   \n",
      "ENCODEMT000145637         0.0            0.0         0.0          0.0   \n",
      "ENCODEMT000147256         0.0            0.0         0.0          0.0   \n",
      "ENCODEMT000152127         0.0            0.0         0.0          0.0   \n",
      "\n",
      "                   Cortex_ZF  Cortex_ZG  Cortex_cycling  X_zone  Y_zone  \\\n",
      "ENCODEMT000142671   0.000000        0.0        0.000000     0.0     0.0   \n",
      "ENCODEMT000144512   0.000000        0.0        0.000000     0.0     0.0   \n",
      "ENCODEMT000145637   0.000000        0.0        0.000000     0.0     0.0   \n",
      "ENCODEMT000147256  26.217817        0.0        0.000000     0.0     0.0   \n",
      "ENCODEMT000152127   0.000000        0.0      105.820107     0.0     0.0   \n",
      "\n",
      "                   Capsule  Other  \n",
      "ENCODEMT000142671      0.0    0.0  \n",
      "ENCODEMT000144512      0.0    0.0  \n",
      "ENCODEMT000145637      0.0    0.0  \n",
      "ENCODEMT000147256      0.0    0.0  \n",
      "ENCODEMT000152127      0.0    0.0  \n",
      "\n",
      "Plotting transcripts for ENSMUSG00000039218.17\n",
      "Saving transcript path graph for ENSMUST00000189977.1 as figures/srrm2_ENSMUST00000189977.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000191385.2 as figures/srrm2_ENSMUST00000191385.2_path.png\n",
      "Saving transcript path graph for ENSMUST00000190568.1 as figures/srrm2_ENSMUST00000190568.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000186914.1 as figures/srrm2_ENSMUST00000186914.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000190293.1 as figures/srrm2_ENSMUST00000190293.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000186259.1 as figures/srrm2_ENSMUST00000186259.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000186961.6 as figures/srrm2_ENSMUST00000186961.6_path.png\n",
      "Saving transcript path graph for ENSMUST00000180975.2 as figures/srrm2_ENSMUST00000180975.2_path.png\n",
      "tdf\n",
      "Index(['Medulla_NE', 'Medulla_EPI', 'Sox10+', 'Stromal', 'Adipocytes',\n",
      "       'Hepatocyte', 'Smooth_muscle', 'Macrophage', 'Endothelial', 'Cortex_ZF',\n",
      "       'Cortex_ZG', 'Cortex_cycling', 'X_zone', 'Y_zone', 'Capsule', 'Other'],\n",
      "      dtype='object')\n",
      "Generating report for ENSMUSG00000039218.17\n",
      "subset adata\n",
      "['ATTGGCTCACAAGCTATCTATTAC-ont_b', 'AAGAGATCCGCATACACTGTCCCG-ont_b', 'CATACCAACGAACTTATTACCTCG-ont_b', 'AAGAGATCTGAAGAGACGCGACTA-pb_2ka', 'GTACGCAACCTCTATCTAAATATC-ont_b']\n",
      "sg\n",
      "['ATTGGCTCACAAGCTATCTATTAC-ont_b', 'AAGAGATCCGCATACACTGTCCCG-ont_b', 'CATACCAACGAACTTATTACCTCG-ont_b', 'AAGAGATCTGAAGAGACGCGACTA-pb_2ka', 'GTACGCAACCTCTATCTAAATATC-ont_b']\n",
      "<class 'anndata._core.views.SparseCSRView'>\n",
      "in calc_tpm\n",
      "             ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "lr_celltype                                                            \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "lr_celltype                                                            \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "lr_celltype                                                            \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "Medulla_NE                 0.0                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000165567  ...  ENSMUST00000238135.1  \\\n",
      "lr_celltype                     ...                         \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "Medulla_NE                 0.0  ...                   0.0   \n",
      "\n",
      "             ENSMUST00000238138.1  ENSMUST00000238151.1  ENSMUST00000238154.1  \\\n",
      "lr_celltype                                                                     \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "\n",
      "             ENSMUST00000238158.1  ENSMUST00000238165.1  ENSMUST00000238171.1  \\\n",
      "lr_celltype                                                                     \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "\n",
      "             ENSMUST00000238178.1  ENSMUST00000238179.1  ENSMUST00000238193.1  \n",
      "lr_celltype                                                                    \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "Medulla_NE                    0.0                   0.0                   0.0  \n",
      "\n",
      "[5 rows x 20912 columns]\n",
      "{'Medulla_NE': 0, 'Medulla_EPI': 1, 'Sox10+': 2, 'Stromal': 3, 'Adipocytes': 4, 'Hepatocyte': 5, 'Smooth_muscle': 6, 'Macrophage': 7, 'Endothelial': 8, 'Cortex_ZF': 9, 'Cortex_ZG': 10, 'Cortex_cycling': 11, 'X_zone': 12, 'Y_zone': 13, 'Capsule': 14, 'Other': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:155: UserWarning: Revieved a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0           28.02298          20.751194   \n",
      "Capsule                       0.0            0.00000           0.000000   \n",
      "Cortex_ZF                     0.0            0.00000           0.000000   \n",
      "Cortex_ZG                     0.0            0.00000           0.000000   \n",
      "Cortex_cycling                0.0            0.00000           0.000000   \n",
      "\n",
      "                ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes               0.000000           0.000000           0.000000   \n",
      "Capsule                  0.000000           0.000000           0.000000   \n",
      "Cortex_ZF               26.217817           0.000000          32.375031   \n",
      "Cortex_ZG                0.000000           0.000000           5.496197   \n",
      "Cortex_cycling           0.000000         105.820107           0.000000   \n",
      "\n",
      "                ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0                0.0                0.0   \n",
      "Capsule                       0.0                0.0                0.0   \n",
      "Cortex_ZF                     0.0                0.0                0.0   \n",
      "Cortex_ZG                     0.0                0.0                0.0   \n",
      "Cortex_cycling                0.0                0.0                0.0   \n",
      "\n",
      "                ENCODEMT000165567  ...  ENSMUST00000238135.1  \\\n",
      "lr_celltype                        ...                         \n",
      "Adipocytes                    0.0  ...              8.560544   \n",
      "Capsule                       0.0  ...              0.000000   \n",
      "Cortex_ZF                     0.0  ...              0.000000   \n",
      "Cortex_ZG                     0.0  ...              4.328255   \n",
      "Cortex_cycling                0.0  ...              0.000000   \n",
      "\n",
      "                ENSMUST00000238138.1  ENSMUST00000238151.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000              0.000000   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                   0.000000              0.000000   \n",
      "Cortex_ZG                   0.000000              0.000000   \n",
      "Cortex_cycling             27.739248             12.642225   \n",
      "\n",
      "                ENSMUST00000238154.1  ENSMUST00000238158.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                 46.808258              0.000000   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                 166.173355             44.515667   \n",
      "Cortex_ZG                  12.013674             54.721916   \n",
      "Cortex_cycling            102.774925              0.000000   \n",
      "\n",
      "                ENSMUST00000238165.1  ENSMUST00000238171.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000                   0.0   \n",
      "Capsule                     0.000000                   0.0   \n",
      "Cortex_ZF                   0.000000                   0.0   \n",
      "Cortex_ZG                  22.160664                   0.0   \n",
      "Cortex_cycling              0.000000                   0.0   \n",
      "\n",
      "                ENSMUST00000238178.1  ENSMUST00000238179.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000              3.411747   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                   7.317644              0.000000   \n",
      "Cortex_ZG                   0.000000              0.000000   \n",
      "Cortex_cycling              0.000000              0.000000   \n",
      "\n",
      "                ENSMUST00000238193.1  \n",
      "lr_celltype                           \n",
      "Adipocytes                 30.931023  \n",
      "Capsule                     0.000000  \n",
      "Cortex_ZF                  26.624668  \n",
      "Cortex_ZG                  47.613117  \n",
      "Cortex_cycling              0.000000  \n",
      "\n",
      "[5 rows x 20912 columns]\n",
      "                ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0           28.02298          20.751194   \n",
      "Capsule                       0.0            0.00000           0.000000   \n",
      "Cortex_ZF                     0.0            0.00000           0.000000   \n",
      "Cortex_ZG                     0.0            0.00000           0.000000   \n",
      "Cortex_cycling                0.0            0.00000           0.000000   \n",
      "\n",
      "                ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes               0.000000           0.000000           0.000000   \n",
      "Capsule                  0.000000           0.000000           0.000000   \n",
      "Cortex_ZF               26.217817           0.000000          32.375031   \n",
      "Cortex_ZG                0.000000           0.000000           5.496197   \n",
      "Cortex_cycling           0.000000         105.820107           0.000000   \n",
      "\n",
      "                ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "lr_celltype                                                               \n",
      "Adipocytes                    0.0                0.0                0.0   \n",
      "Capsule                       0.0                0.0                0.0   \n",
      "Cortex_ZF                     0.0                0.0                0.0   \n",
      "Cortex_ZG                     0.0                0.0                0.0   \n",
      "Cortex_cycling                0.0                0.0                0.0   \n",
      "\n",
      "                ENCODEMT000165567  ...  ENSMUST00000238138.1  \\\n",
      "lr_celltype                        ...                         \n",
      "Adipocytes                    0.0  ...              0.000000   \n",
      "Capsule                       0.0  ...              0.000000   \n",
      "Cortex_ZF                     0.0  ...              0.000000   \n",
      "Cortex_ZG                     0.0  ...              0.000000   \n",
      "Cortex_cycling                0.0  ...             27.739248   \n",
      "\n",
      "                ENSMUST00000238151.1  ENSMUST00000238154.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000             46.808258   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                   0.000000            166.173355   \n",
      "Cortex_ZG                   0.000000             12.013674   \n",
      "Cortex_cycling             12.642225            102.774925   \n",
      "\n",
      "                ENSMUST00000238158.1  ENSMUST00000238165.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                  0.000000              0.000000   \n",
      "Capsule                     0.000000              0.000000   \n",
      "Cortex_ZF                  44.515667              0.000000   \n",
      "Cortex_ZG                  54.721916             22.160664   \n",
      "Cortex_cycling              0.000000              0.000000   \n",
      "\n",
      "                ENSMUST00000238171.1  ENSMUST00000238178.1  \\\n",
      "lr_celltype                                                  \n",
      "Adipocytes                       0.0              0.000000   \n",
      "Capsule                          0.0              0.000000   \n",
      "Cortex_ZF                        0.0              7.317644   \n",
      "Cortex_ZG                        0.0              0.000000   \n",
      "Cortex_cycling                   0.0              0.000000   \n",
      "\n",
      "                ENSMUST00000238179.1  ENSMUST00000238193.1  row_order  \n",
      "lr_celltype                                                            \n",
      "Adipocytes                  3.411747             30.931023          4  \n",
      "Capsule                     0.000000              0.000000         14  \n",
      "Cortex_ZF                   0.000000             26.624668          9  \n",
      "Cortex_ZG                   0.000000             47.613117         10  \n",
      "Cortex_cycling              0.000000              0.000000         11  \n",
      "\n",
      "[5 rows x 20913 columns]\n",
      "             ENCODEMT000142671  ENCODEMT000144512  ENCODEMT000145637  \\\n",
      "Medulla_NE           37.515007            0.00000           0.000000   \n",
      "Medulla_EPI           0.000000            0.00000           0.000000   \n",
      "Sox10+                0.000000            0.00000           0.000000   \n",
      "Stromal               0.000000            0.00000           0.000000   \n",
      "Adipocytes            0.000000           28.02298          20.751194   \n",
      "\n",
      "             ENCODEMT000147256  ENCODEMT000152127  ENCODEMT000153402  \\\n",
      "Medulla_NE           53.524593                0.0                0.0   \n",
      "Medulla_EPI           0.000000                0.0                0.0   \n",
      "Sox10+                0.000000                0.0                0.0   \n",
      "Stromal              33.030556                0.0                0.0   \n",
      "Adipocytes            0.000000                0.0                0.0   \n",
      "\n",
      "             ENCODEMT000155017  ENCODEMT000158767  ENCODEMT000160480  \\\n",
      "Medulla_NE                 0.0                0.0          15.362636   \n",
      "Medulla_EPI                0.0                0.0           0.000000   \n",
      "Sox10+                     0.0                0.0           0.000000   \n",
      "Stromal                    0.0                0.0           0.000000   \n",
      "Adipocytes                 0.0                0.0           0.000000   \n",
      "\n",
      "             ENCODEMT000165567  ...  ENSMUST00000238135.1  \\\n",
      "Medulla_NE            0.000000  ...              0.000000   \n",
      "Medulla_EPI          18.526754  ...              0.000000   \n",
      "Sox10+                0.000000  ...              0.000000   \n",
      "Stromal               0.000000  ...              0.000000   \n",
      "Adipocytes            0.000000  ...              8.560544   \n",
      "\n",
      "             ENSMUST00000238138.1  ENSMUST00000238151.1  ENSMUST00000238154.1  \\\n",
      "Medulla_NE                    0.0              0.000000            256.934601   \n",
      "Medulla_EPI                   0.0              2.421595            210.482422   \n",
      "Sox10+                        0.0              0.000000              0.000000   \n",
      "Stromal                       0.0              0.000000            297.045593   \n",
      "Adipocytes                    0.0              0.000000             46.808258   \n",
      "\n",
      "             ENSMUST00000238158.1  ENSMUST00000238165.1  ENSMUST00000238171.1  \\\n",
      "Medulla_NE                    0.0                   0.0                   0.0   \n",
      "Medulla_EPI                   0.0                   0.0                   0.0   \n",
      "Sox10+                        0.0                   0.0                   0.0   \n",
      "Stromal                       0.0                   0.0                   0.0   \n",
      "Adipocytes                    0.0                   0.0                   0.0   \n",
      "\n",
      "             ENSMUST00000238178.1  ENSMUST00000238179.1  ENSMUST00000238193.1  \n",
      "Medulla_NE             182.681778              0.000000              0.000000  \n",
      "Medulla_EPI              0.000000              0.000000              0.000000  \n",
      "Sox10+                   0.000000              0.000000              0.000000  \n",
      "Stromal                  0.000000              0.000000            251.319427  \n",
      "Adipocytes               0.000000              3.411747             30.931023  \n",
      "\n",
      "[5 rows x 20912 columns]\n",
      "tdf pi\n",
      "                      Medulla_NE  Medulla_EPI  Sox10+    Stromal  Adipocytes  \\\n",
      "tid                                                                            \n",
      "ENSMUST00000190568.1         0.0    17.741936     NaN   7.317073    6.349207   \n",
      "ENSMUST00000186914.1         0.0     3.225806     NaN   2.439024    7.936508   \n",
      "ENSMUST00000190293.1         0.0     1.612903     NaN   4.878048    1.587302   \n",
      "ENSMUST00000186259.1         0.0     3.225806     NaN   0.000000    0.000000   \n",
      "ENSMUST00000189977.1       100.0    66.129036     NaN  68.292686   69.841270   \n",
      "\n",
      "                      Hepatocyte  Smooth_muscle  Macrophage  Endothelial  \\\n",
      "tid                                                                        \n",
      "ENSMUST00000190568.1         0.0      33.333336   11.111112     8.771930   \n",
      "ENSMUST00000186914.1         0.0       0.000000   11.111112     1.754386   \n",
      "ENSMUST00000190293.1         0.0       0.000000    0.000000     8.771930   \n",
      "ENSMUST00000186259.1         0.0       0.000000    0.000000     1.754386   \n",
      "ENSMUST00000189977.1         0.0      66.666672   77.777779    71.929825   \n",
      "\n",
      "                      Cortex_ZF  Cortex_ZG  Cortex_cycling     X_zone  \\\n",
      "tid                                                                     \n",
      "ENSMUST00000190568.1   6.481482   9.433963        0.000000   0.000000   \n",
      "ENSMUST00000186914.1   4.629630   3.773585        5.555556  11.764706   \n",
      "ENSMUST00000190293.1   1.851852   0.000000       11.111112  11.764706   \n",
      "ENSMUST00000186259.1   0.925926   0.000000        0.000000   0.000000   \n",
      "ENSMUST00000189977.1  69.444443  79.245285       72.222221  52.941177   \n",
      "\n",
      "                         Y_zone    Capsule  Other  \n",
      "tid                                                \n",
      "ENSMUST00000190568.1   6.818182   0.000000    0.0  \n",
      "ENSMUST00000186914.1  14.772727   0.000000    0.0  \n",
      "ENSMUST00000190293.1   1.136364   0.000000    0.0  \n",
      "ENSMUST00000186259.1   0.000000   0.000000    0.0  \n",
      "ENSMUST00000189977.1  69.318184  66.666672   50.0  \n",
      "\n",
      "Plotting transcripts for ENSMUSG00000039218.17\n",
      "Saving transcript path graph for ENSMUST00000189977.1 as figures/srrm2_browser_ENSMUST00000189977.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000191385.2 as figures/srrm2_browser_ENSMUST00000191385.2_path.png\n",
      "Saving transcript path graph for ENSMUST00000190568.1 as figures/srrm2_browser_ENSMUST00000190568.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000186914.1 as figures/srrm2_browser_ENSMUST00000186914.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000190293.1 as figures/srrm2_browser_ENSMUST00000190293.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000186259.1 as figures/srrm2_browser_ENSMUST00000186259.1_path.png\n",
      "Saving transcript path graph for ENSMUST00000186961.6 as figures/srrm2_browser_ENSMUST00000186961.6_path.png\n",
      "Saving transcript path graph for ENSMUST00000180975.2 as figures/srrm2_browser_ENSMUST00000180975.2_path.png\n",
      "tdf\n",
      "Index(['Medulla_NE', 'Medulla_EPI', 'Sox10+', 'Stromal', 'Adipocytes',\n",
      "       'Hepatocyte', 'Smooth_muscle', 'Macrophage', 'Endothelial', 'Cortex_ZF',\n",
      "       'Cortex_ZG', 'Cortex_cycling', 'X_zone', 'Y_zone', 'Capsule', 'Other'],\n",
      "      dtype='object')\n",
      "Generating report for ENSMUSG00000039218.17\n"
     ]
    }
   ],
   "source": [
    "# make some swan reports\n",
    "gen_report(sg, 'Srrm2',\n",
    "              prefix='figures/srrm2',\n",
    "              layer='tpm',\n",
    "              cmap='viridis',\n",
    "              novelty=True,\n",
    "              groupby='lr_celltype',\n",
    "              transcript_name=True,\n",
    "              metadata_cols=['lr_celltype'],\n",
    "              datasets={'lr_celltype': order})\n",
    "\n",
    "gen_report(sg, 'Srrm2',\n",
    "              prefix='figures/srrm2',\n",
    "              layer='pi',\n",
    "              cmap='magma',\n",
    "              novelty=True,\n",
    "              groupby='lr_celltype',\n",
    "              transcript_name=True,\n",
    "              metadata_cols=['lr_celltype'],\n",
    "              datasets={'lr_celltype': order},\n",
    "              browser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fixing groupbys in calc_pi and calc_tpm (and maybe create_edge_adata and create_end_adata)\n",
    "# # sg = swan.read('/Users/fairliereese/mortazavi_lab/data/mousewg/adrenal/lr_splitseq/swan/swan.p')\n",
    "# keys   = ['dataset1', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6']\n",
    "# vals = [1,2,3,4,5,6]\n",
    "\n",
    "# def sparse_groupby(keys, vals):\n",
    "#     print(keys)\n",
    "#     unique_keys, row = np.unique(keys, return_inverse=True)\n",
    "#     col = np.arange(len(keys))\n",
    "#     mat = sparse.coo_matrix((vals, (row, col)))\n",
    "#     print(mat.toarray())\n",
    "#     return dict(zip(unique_keys, mat.sum(1).flat))\n",
    "\n",
    "# sparse_groupby(keys, vals)\n",
    "\n",
    "# # # vals = [[1,2,3,4,5,6], [1,2,3,4,5,7]]\n",
    "# df = pd.DataFrame(index=keys, data=vals)\n",
    "# df.head()\n",
    "# # df.groupby(keys).sum()\n",
    "\n",
    "# # def pandas_groupby(keys, vals):\n",
    "# #     return pd.Series(vals).groupby(keys).sum().to_dict()\n",
    "# # pandas_groupby(keys, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debugging / speeding up get_edge_ab\n",
    "# sg = swan.SwanGraph()\n",
    "# sg.add_annotation('../testing/files/test_full_annotation.gtf')\n",
    "# sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "# sg.add_abundance('../testing/files/test_ab_talon_1.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stock-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished running pivot_path_list\n",
      "finished making t_exp_df\n",
      "finished merging\n",
      "hwllo\n",
      "sparse\n",
      "finished summing\n",
      "--- 0.014503955841064453 seconds ---\n",
      "dense\n",
      "finished summing\n",
      "--- 0.003910064697265625 seconds ---\n"
     ]
    }
   ],
   "source": [
    "t_df = sg.t_df.copy(deep=True)\n",
    "t_df = t_df.loc[sg.adata.var.index.tolist()]\n",
    "# print('subset t_df')\n",
    "# print(t_df)\n",
    "edge_exp_df = swan.pivot_path_list(t_df, 'path')\n",
    "\n",
    "print('finished running pivot_path_list')\n",
    "\n",
    "# get a mergeable transcript expression df\n",
    "tid = sg.adata.var.index.tolist()\n",
    "obs = sg.adata.obs.index.tolist()\n",
    "data = sg.adata.layers['counts'].transpose()\n",
    "t_exp_df = pd.DataFrame.sparse.from_spmatrix(columns=obs,\n",
    "                                             data=data,\n",
    "                                             index=tid)\n",
    "\n",
    "print('finished making t_exp_df')\n",
    "\n",
    "# merge counts per transcript with edges\n",
    "edge_exp_df = edge_exp_df.merge(t_exp_df, how='left',\n",
    "    left_index=True, right_index=True)\n",
    "\n",
    "print('finished merging')\n",
    "\n",
    "# sum the counts per transcript / edge / dataset\n",
    "\n",
    "# THIS IS THE STEP that takes forever\n",
    "# ensure that this is sparse befre the operation\n",
    "# idea - subset by those that have 1 or multiple edge ids, only sum up\n",
    "# those that have multiple, then concat\n",
    "print('hwllo')\n",
    "# print(edge_exp_df)\n",
    "# print(edge_exp_df.dtypes)\n",
    "edge_exp_df.reset_index(inplace=True, drop=True)\n",
    "edge_exp_df.set_index('edge_id', inplace=True)\n",
    "\n",
    "# sparse\n",
    "print('sparse')\n",
    "start_time = time.time()\n",
    "edge_exp_df = edge_exp_df.groupby(by='edge_id', as_index=True).sum()\n",
    "print('finished summing')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# dense\n",
    "print('dense')\n",
    "start_time = time.time()\n",
    "edge_exp_df[sg.datasets] = edge_exp_df[sg.datasets].sparse.to_dense()\n",
    "edge_exp_df = edge_exp_df.groupby(by='edge_id', as_index=True).sum()\n",
    "print('finished summing')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "knowing-discharge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1    Sparse[float64, 0]\n",
      "dataset2    Sparse[float64, 0]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset1</th>\n",
       "      <th>dataset2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset1  dataset2\n",
       "edge_id                    \n",
       "0             5.0       5.0\n",
       "1             5.0       5.0\n",
       "2             5.0       5.0\n",
       "3             5.0       5.0\n",
       "4             5.0       5.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(edge_exp_df.dtypes)\n",
    "edge_exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "turkish-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "finished getting ends\n",
      "finished merging exp\n",
      "finished merging\n",
      "finished sorting\n",
      "sparse\n",
      "finished summing\n",
      "--- 0.010179758071899414 seconds ---\n",
      "dense\n",
      "finished summing\n",
      "--- 0.006715059280395508 seconds ---\n"
     ]
    }
   ],
   "source": [
    "kind = 'tss'\n",
    "\n",
    "# limit to only expresed transcripts\n",
    "t_df = sg.t_df.copy(deep=True)\n",
    "t_df = t_df.loc[sg.adata.var.index.tolist()]\n",
    "print(len(sg.t_df.index))\n",
    "print(len(t_df.index))\n",
    "\n",
    "df = swan.get_ends(sg.t_df, kind)\n",
    "print('finished getting ends')\n",
    "\n",
    "# get a mergeable transcript expression df\n",
    "tid = sg.adata.var.index.tolist()\n",
    "obs = sg.adata.obs.index.tolist()\n",
    "data = sg.adata.layers['counts'].transpose()\n",
    "t_exp_df = pd.DataFrame.sparse.from_spmatrix(columns=obs, data=data, index=tid)\n",
    "t_exp_df = t_exp_df.merge(t_df, how='left',\n",
    "    left_index=True, right_index=True)\n",
    "\n",
    "print('finished merging exp')\n",
    "\n",
    "# merge counts per transcript with end expression\n",
    "df = df.merge(t_exp_df, how='left',\n",
    "    left_index=True, right_index=True)\n",
    "print('finished merging')\n",
    "\n",
    "# sort based on vertex id\n",
    "df.sort_index(inplace=True, ascending=True)\n",
    "print('finished sorting')\n",
    "\n",
    "# set index to gene ID, gene name, and vertex id\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.set_index(['gid', 'gname', 'vertex_id'], inplace=True)\n",
    "df = df[sg.datasets]\n",
    "\n",
    "# groupby on gene and assign each unique TSS / gene combo an ID\n",
    "id_col = '{}_id'.format(kind)\n",
    "name_col = '{}_name'.format(kind)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# sparse\n",
    "print('sparse')\n",
    "start_time = time.time()\n",
    "df = df.groupby(['gid', 'gname', 'vertex_id']).sum().reset_index()\n",
    "print('finished summing')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# dense\n",
    "print('dense')\n",
    "start_time = time.time()\n",
    "df[sg.datasets] = df[sg.datasets].sparse.to_dense()\n",
    "df = df.groupby(['gid', 'gname', 'vertex_id']).sum().reset_index()\n",
    "print('finished summing')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "earned-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gid                      object\n",
       "gname                    object\n",
       "vertex_id                 int64\n",
       "dataset1     Sparse[float64, 0]\n",
       "dataset2     Sparse[float64, 0]\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "endless-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1    Sparse[float64, 0]\n",
      "dataset2    Sparse[float64, 0]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset1</th>\n",
       "      <th>dataset2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset1  dataset2\n",
       "edge_id                    \n",
       "0             5.0       5.0\n",
       "1             5.0       5.0\n",
       "2             5.0       5.0\n",
       "3             5.0       5.0\n",
       "4             5.0       5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_exp_df.reset_index(inplace=True, drop=True)\n",
    "edge_exp_df.set_index('edge_id', inplace=True)\n",
    "print(edge_exp_df.dtypes)\n",
    "edge_exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "administrative-reading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset1</th>\n",
       "      <th>dataset2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset1  dataset2\n",
       "edge_id                    \n",
       "0             5.0       5.0\n",
       "1             5.0       5.0\n",
       "2             5.0       5.0\n",
       "3             5.0       5.0\n",
       "4             5.0       5.0\n",
       "5            10.0       0.0\n",
       "6            10.0       0.0\n",
       "7            10.0       0.0\n",
       "8            10.0       0.0\n",
       "9            10.0       0.0\n",
       "5             0.0      10.0\n",
       "6             0.0      10.0\n",
       "14            0.0      10.0\n",
       "15            0.0      10.0\n",
       "9             0.0      10.0\n",
       "10           10.0      10.0\n",
       "5             5.0       5.0\n",
       "11            5.0       5.0\n",
       "12            5.0       5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "helpful-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset1</th>\n",
       "      <th>dataset2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset1  dataset2\n",
       "edge_id                    \n",
       "0             5.0       5.0\n",
       "1             5.0       5.0\n",
       "2             5.0       5.0\n",
       "3             5.0       5.0\n",
       "4             5.0       5.0\n",
       "5            15.0      15.0\n",
       "6            10.0      10.0\n",
       "7            10.0       0.0\n",
       "8            10.0       0.0\n",
       "9            10.0      10.0\n",
       "10           10.0      10.0\n",
       "11            5.0       5.0\n",
       "12            5.0       5.0\n",
       "14            0.0      10.0\n",
       "15            0.0      10.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_exp_df.groupby(by='edge_id', as_index=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "irish-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added \n",
    "# utils.py\n",
    "def calc_total_counts(adata, obs_col='dataset', layer='counts'):\n",
    "    \n",
    "    # turn into a sparse dataframe\n",
    "    cols = adata.var.index.tolist()\n",
    "    inds = adata.obs[obs_col].tolist()\n",
    "    data = adata.layers[layer]\n",
    "    data = scipy.sparse.csr_matrix(data)\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(data, index=inds, columns=cols)\n",
    "    df.index.name = obs_col \n",
    "\n",
    "    # add up values on condition (row)\n",
    "    df = df.groupby(level=0).sum()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "commercial-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added\n",
    "# utils.py\n",
    "def calc_tpm(adata, obs_col='dataset'):\n",
    "    \n",
    "    # calculate tpm using scanpy\n",
    "    d = sc.pp.normalize_total(adata,\n",
    "                              layer='counts',\n",
    "                              target_sum=1e6,\n",
    "                              key_added='total_counts',\n",
    "                              inplace=False)\n",
    "    adata.obs['total_counts'] = d['norm_factor']\n",
    "    \n",
    "    # turn into a sparse dataframe\n",
    "    cols = adata.var.index.tolist()\n",
    "    inds = adata.obs[obs_col].tolist()\n",
    "    data = d['X']\n",
    "    data = scipy.sparse.csr_matrix(data)\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(data, index=inds, columns=cols)\n",
    "    df.index.name = obs_col    \n",
    "\n",
    "    # average across tpm\n",
    "    if obs_col != 'dataset':\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.groupby(obs_col).mean()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "dimensional-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added\n",
    "# swangraph.py\n",
    "def add_abundance(sg, counts_file):\n",
    "    \"\"\"\n",
    "    Adds abundance from a counts matrix to the SwanGraph. Transcripts in the\n",
    "    SwanGraph but not in the counts matrix will be assigned 0 counts.\n",
    "    Transcripts in the abundance matrix but not in the SwanGraph will not\n",
    "    have expression added.\n",
    "\n",
    "    Parameters:\n",
    "        counts_file (str): Path to TSV expression file where first column is\n",
    "            the transcript ID and following columns name the added datasets and\n",
    "            their counts in each dataset, OR to a TALON abundance matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # read in abundance file\n",
    "    swan.check_file_loc(counts_file, 'abundance matrix')\n",
    "    try:\n",
    "        df = pd.read_csv(counts_file, sep='\\t')\n",
    "    except:\n",
    "        raise ValueError('Problem reading expression matrix {}'.format(counts_file))\n",
    "\n",
    "    # check if abundance matrix is a talon abundance matrix\n",
    "    cols = ['gene_ID', 'transcript_ID', 'annot_gene_id', 'annot_transcript_id',\n",
    "        'annot_gene_name', 'annot_transcript_name', 'n_exons', 'length',\n",
    "        'gene_novelty', 'transcript_novelty', 'ISM_subtype']\n",
    "    if df.columns.tolist()[:11] == cols:\n",
    "        df = swan.reformat_talon_abundance(counts_file)\n",
    "\n",
    "    # rename transcript ID column\n",
    "    col = df.columns[0]\n",
    "    df.rename({col: 'tid'}, axis=1, inplace=True)\n",
    "\n",
    "    # limit to just the transcripts already in the graph\n",
    "    sg_tids = sg.t_df.tid.tolist()\n",
    "    ab_tids = df.tid.tolist()\n",
    "    tids = list(set(sg_tids)&set(ab_tids))\n",
    "    df = df.loc[df.tid.isin(tids)]\n",
    "    \n",
    "    # transpose to get adata format\n",
    "    df.set_index('tid', inplace=True)\n",
    "    df = df.T\n",
    "    \n",
    "    # get adata components - obs, var, and X\n",
    "    var = df.columns.to_frame()\n",
    "    var.columns = ['tid']\n",
    "    obs = df.index.to_frame()\n",
    "    obs.columns = ['dataset']\n",
    "    X = sparse.csr_matrix(df.to_numpy())\n",
    "    \n",
    "    # create transcript-level adata object and filter out unexpressed transcripts\n",
    "    adata = anndata.AnnData(var=var, obs=obs, X=X)\n",
    "    genes, _  = sc.pp.filter_genes(adata, min_counts=1, inplace=False)\n",
    "    adata = adata[:, genes]\n",
    "    adata.layers['counts'] = adata.X\n",
    "\n",
    "    # add each dataset to list of \"datasets\", check if any are already there!\n",
    "    datasets = adata.obs.dataset.tolist()\n",
    "    for d in datasets:\n",
    "        if d in sg.datasets:\n",
    "            raise ValueError('Dataset {} already present in the SwanGraph.'.format(d))\n",
    "    sg.datasets.extend(datasets)\n",
    "\n",
    "    print()\n",
    "    if len(datasets) <= 5:\n",
    "        print('Adding abundance for datasets {} to SwanGraph.'.format(', '.join(datasets)))\n",
    "    else:\n",
    "        mini_datasets = datasets[:5]\n",
    "        n = len(datasets) - len(mini_datasets)\n",
    "        print('Adding abundance for datasets {}... (and {} more) to SwanGraph'.format(', '.join(mini_datasets), n))\n",
    "\n",
    "    # if there is preexisting abundance data in the SwanGraph, concatenate\n",
    "    # otherwise, adata is the new transcript level adata\n",
    "    if not sg.has_abundance():\n",
    "\n",
    "        # create transcript-level adata object\n",
    "        sg.adata = adata\n",
    "\n",
    "        # add counts as layers\n",
    "        sg.adata.layers['counts'] = sg.adata.X\n",
    "        print('Calculating transcript TPM...')\n",
    "        sg.adata.layers['tpm'] = sparse.csr_matrix(calc_tpm(sg.adata).to_numpy())\n",
    "\n",
    "        if not sg.sc:\n",
    "            print('Calculating PI...') \n",
    "            sg.adata.layers['pi'] = sparse.csr_matrix(calc_pi(sg.adata, sg.t_df)[0].to_numpy())\n",
    "    else:\n",
    "        \n",
    "        # first set current layer to be counts\n",
    "        sg.adata.X = sg.adata.layers['counts']\n",
    "        \n",
    "        # concatenate existing adata with new one\n",
    "        # outer join to add all new transcripts (that are from added\n",
    "        # annotation or transcriptome) to the abundance\n",
    "        uns = sg.adata.uns\n",
    "        sg.adata = sg.adata.concatenate(adata, join='outer', index_unique=None)\n",
    "        sg.adata.uns = uns\n",
    "        \n",
    "        # recalculate pi and tpm\n",
    "        print('Calculating transcript TPM...')\n",
    "        sg.adata.layers['tpm'] = sparse.csr_matrix(calc_tpm(sg.adata).to_numpy())\n",
    "\n",
    "        if not sg.sc:\n",
    "            print('Calculating PI...')\n",
    "            sg.adata.layers['pi'] = sparse.csr_matrix(calc_pi(sg.adata, sg.t_df)[0].to_numpy())\n",
    "\n",
    "    # add abundance for edges, TSS per gene, and TES per gene\n",
    "    sg = create_edge_adata(sg)\n",
    "    print('Calculating TSS usage...')\n",
    "    sg = create_end_adata(sg, kind='tss')\n",
    "    print('Calculating TES usage...')\n",
    "    sg = create_end_adata(sg, kind='tes')\n",
    "\n",
    "    # set abundance flag to true\n",
    "    sg.abundance = True\n",
    "    \n",
    "    return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "characteristic-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added\n",
    "# swangraph.py\n",
    "def create_end_adata(sg, kind):\n",
    "    \"\"\"\n",
    "    Create a tss / tes-level adata object. Enables calculating tss / tes\n",
    "    usage across samples.\n",
    "\n",
    "    Parameters:\n",
    "        kind (str): Choose from 'tss' or 'tes'\n",
    "    \"\"\"\n",
    "\n",
    "    df = swan.get_ends(sg.t_df, kind)\n",
    "\n",
    "    # get a mergeable transcript expression df\n",
    "    tid = sg.adata.var.index.tolist()\n",
    "    obs = sg.adata.obs.index.tolist()\n",
    "    data = sg.adata.layers['counts'].transpose()\n",
    "    t_exp_df = pd.DataFrame.sparse.from_spmatrix(columns=obs, data=data, index=tid)\n",
    "    t_exp_df = t_exp_df.merge(sg.t_df, how='left',\n",
    "        left_index=True, right_index=True)\n",
    "\n",
    "    # merge counts per transcript with end expression\n",
    "    df = df.merge(t_exp_df, how='left',\n",
    "        left_index=True, right_index=True)\n",
    "\n",
    "    # sort based on vertex id\n",
    "    df.sort_index(inplace=True, ascending=True)\n",
    "\n",
    "    # set index to gene ID, gene name, and vertex id \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.set_index(['gid', 'gname', 'vertex_id'], inplace=True)\n",
    "    df = df[sg.datasets]\n",
    "\n",
    "    # groupby on gene and assign each unique TSS / gene combo an ID\n",
    "    id_col = '{}_id'.format(kind)\n",
    "    name_col = '{}_name'.format(kind)\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.groupby(['gid', 'gname', 'vertex_id']).sum().reset_index()\n",
    "    df['end_gene_num'] = df.sort_values(['gid', 'vertex_id'],\n",
    "                    ascending=[True, True])\\\n",
    "                    .groupby(['gid']) \\\n",
    "                    .cumcount() + 1\n",
    "    df[id_col] = df['gid']+'_'+df['end_gene_num'].astype(str)\n",
    "    df[name_col] = df['gname']+'_'+df['end_gene_num'].astype(str)\n",
    "    df.drop('end_gene_num', axis=1, inplace=True)\n",
    "\n",
    "    # obs, var, and X tables for new data\n",
    "    var_cols = ['gid', 'gname', 'vertex_id', id_col, name_col]\n",
    "    var = df[var_cols]\n",
    "    var.set_index('{}_id'.format(kind), inplace=True)\n",
    "    df.drop(var_cols, axis=1, inplace=True)\n",
    "    df = df[sg.adata.obs.index.tolist()]\n",
    "    X = sparse.csr_matrix(df.transpose().values)\n",
    "    obs = sg.adata.obs\n",
    "    \n",
    "    # create anndata\n",
    "    adata = anndata.AnnData(var=var, obs=obs, X=X)\n",
    "    \n",
    "    # add counts and tpm as layers\n",
    "    adata.layers['counts'] = adata.X\n",
    "    adata.layers['tpm'] = sparse.csr_matrix(calc_tpm(adata).to_numpy())\n",
    "    if not sg.sc:\n",
    "        adata.layers['pi'] = sparse.csr_matrix(calc_pi(adata,\n",
    "                adata.var)[0].to_numpy())\n",
    "\n",
    "    # assign adata and clean up unstructured data if needed\n",
    "    if kind == 'tss':\n",
    "        if sg.has_abundance():\n",
    "            adata.uns = sg.tss_adata.uns\n",
    "        sg.tss_adata = adata\n",
    "        \n",
    "    elif kind == 'tes':\n",
    "        if sg.has_abundance():\n",
    "            adata.uns = sg.tss_adata.uns\n",
    "        sg.tes_adata = adata\n",
    "    \n",
    "    return sg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "constitutional-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added\n",
    "# swangraph.py\n",
    "def create_edge_adata(sg):\n",
    "    \"\"\"\n",
    "    Create an edge-level adata object. Enables calculating edge usage across\n",
    "    samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # get table what edges are in each transcript\n",
    "    edge_exp_df = swan.pivot_path_list(sg.t_df, 'path')\n",
    "\n",
    "    # get a mergeable transcript expression df\n",
    "    tid = sg.adata.var.index.tolist()\n",
    "    obs = sg.adata.obs.index.tolist()\n",
    "    data = sg.adata.layers['counts'].transpose()\n",
    "    t_exp_df = pd.DataFrame.sparse.from_spmatrix(columns=obs,\n",
    "                                                 data=data,\n",
    "                                                 index=tid)\n",
    "\n",
    "    # merge counts per transcript with edges\n",
    "    edge_exp_df = edge_exp_df.merge(t_exp_df, how='left',\n",
    "        left_index=True, right_index=True)\n",
    "\n",
    "    # sum the counts per transcript / edge / dataset\n",
    "    edge_exp_df = edge_exp_df.groupby('edge_id').sum()\n",
    "\n",
    "    # order based on order of edges in sg.edge_df\n",
    "    edge_exp_df = edge_exp_df.merge(sg.edge_df[['v1', 'v2']],\n",
    "        how='left', left_index=True, right_index=True)\n",
    "    edge_exp_df.sort_values(by=['v1', 'v2'], inplace=True)\n",
    "    edge_exp_df.drop(['v1', 'v2'], axis=1, inplace=True)\n",
    "    \n",
    "    # drop edges that are unexpressed\n",
    "    edge_exp_df = edge_exp_df.loc[edge_exp_df.sum(1) > 0]\n",
    "\n",
    "    # obs, var, and X tables for new data\n",
    "    var = edge_exp_df.index.to_frame()\n",
    "    X = sparse.csr_matrix(edge_exp_df.transpose().values)\n",
    "    obs = sg.adata.obs\n",
    "\n",
    "    # create edge-level adata object\n",
    "    adata = anndata.AnnData(var=var, obs=obs, X=X)\n",
    "\n",
    "    # add counts and tpm as layers\n",
    "    adata.layers['counts'] = adata.X\n",
    "    adata.layers['tpm'] = sparse.csr_matrix(calc_tpm(adata).to_numpy())\n",
    "    # can't make pi for edges unless I make a new edge for \n",
    "    # each gene that the edge is in\n",
    "    # could just have sg.edge_adata var separate from sg.edge_df for now tho\n",
    "#     sg.edge_adata.layers['pi'] = sparse.csr_matrix(calc_pi(sg.adata, sg.edge_df)[0].to_numpy())\n",
    "\n",
    "    # assign adata and clean up unstructured data if needed\n",
    "    if sg.has_abundance():\n",
    "        adata.uns = sg.edge_adata.uns\n",
    "    sg.edge_adata = adata\n",
    "    \n",
    "    return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "laden-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added\n",
    "# utils.py\n",
    "def calc_pi(adata, t_df, obs_col='dataset'):\n",
    "\n",
    "    # calculate cumulative counts across obs_col\n",
    "    id_col = adata.var.index.name\n",
    "    conditions = adata.obs[obs_col].unique().tolist()\n",
    "    df = calc_total_counts(adata, obs_col=obs_col)\n",
    "    df = df.transpose()\n",
    "    # we use ints to index edges and locs\n",
    "    if id_col == 'vertex_id' or id_col == 'edge_id':\n",
    "        df.index = df.index.astype('int')\n",
    "\n",
    "    sums = df.copy(deep=True)\n",
    "    sums = sums[conditions]\n",
    "    sums = sums.transpose()\n",
    "\n",
    "    # add gid\n",
    "    df = df.merge(t_df['gid'], how='left', left_index=True, right_index=True)\n",
    "    t_counts = df.melt(id_vars=['gid'],\n",
    "                       value_vars=conditions,\n",
    "                       var_name=obs_col,\n",
    "                       value_name='t_counts',\n",
    "                       ignore_index=False)\n",
    "    t_counts.index.name = id_col\n",
    "    t_counts.reset_index(inplace=True)\n",
    "\n",
    "    # calculate total number of reads per gene per condition\n",
    "    temp = df.copy(deep=True)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    totals = temp.groupby('gid').sum().reset_index()\n",
    "\n",
    "    # merge back in\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename({'index':id_col}, axis=1, inplace=True)\n",
    "    df = df.merge(totals, on='gid', suffixes=('_t_counts', None))\n",
    "    del totals\n",
    "\n",
    "    df = df.melt(id_vars=['gid'], \n",
    "                 value_vars=conditions, \n",
    "                 var_name=obs_col,\n",
    "                 value_name='gene_counts')\n",
    "    df = df.drop_duplicates()\n",
    "    df = t_counts.merge(df, how='left', on=['gid', obs_col])\n",
    "\n",
    "\n",
    "    df['pi'] = (df.t_counts/df.gene_counts)*100\n",
    "    df = df.pivot(columns=obs_col, index=id_col, values='pi')\n",
    "\n",
    "    # order based on order in adata\n",
    "    ids = adata.var.index.tolist()\n",
    "    df = df.loc[ids]\n",
    "    cols = adata.obs[obs_col].unique().tolist()\n",
    "    df = df[cols]\n",
    "\n",
    "    # convert to sparse\n",
    "    df = df.transpose()\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(data=sparse.csr_matrix(df.values),\n",
    "                                           index=df.index.tolist(),\n",
    "                                           columns=df.columns)\n",
    "    return df, sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "south-crossing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding annotation to the SwanGraph\n",
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n",
      "df\n",
      "       dataset1  dataset2\n",
      "test1       5.0       5.0\n",
      "test2      10.0       0.0\n",
      "test3       0.0      10.0\n",
      "test4      10.0      10.0\n",
      "test5       5.0       5.0\n",
      "t_df\n",
      "             tname        gid        gname               path    tid  \\\n",
      "tid                                                                    \n",
      "test1  test1_tname  test1_gid  test1_gname    [0, 1, 2, 3, 4]  test1   \n",
      "test2  test2_tname  test2_gid  test2_gname    [5, 6, 7, 8, 9]  test2   \n",
      "test3  test3_tname  test2_gid  test2_gname  [5, 6, 14, 15, 9]  test3   \n",
      "test4  test4_tname  test4_gid  test4_gname               [10]  test4   \n",
      "test5  test5_tname  test2_gid  test2_gname        [5, 11, 12]  test5   \n",
      "\n",
      "                    loc_path  annotation    novelty  \n",
      "tid                                                  \n",
      "test1     [0, 1, 2, 3, 4, 5]        True      Known  \n",
      "test2  [12, 11, 10, 8, 7, 6]        True      Known  \n",
      "test3  [12, 11, 10, 9, 7, 6]       False  Undefined  \n",
      "test4                 [6, 7]        True      Known  \n",
      "test5         [12, 11, 8, 7]        True      Known  \n",
      "Calculating TSS usage...\n",
      "df\n",
      "             dataset1  dataset2\n",
      "test1_gid_1       5.0       5.0\n",
      "test2_gid_1      15.0      15.0\n",
      "test4_gid_1      10.0      10.0\n",
      "t_df\n",
      "                   gid        gname  vertex_id       tss_name\n",
      "tss_id                                                       \n",
      "test1_gid_1  test1_gid  test1_gname          0  test1_gname_1\n",
      "test2_gid_1  test2_gid  test2_gname         12  test2_gname_1\n",
      "test4_gid_1  test4_gid  test4_gname          6  test4_gname_1\n",
      "Calculating TES usage...\n",
      "df\n",
      "             dataset1  dataset2\n",
      "test1_gid_1       5.0       5.0\n",
      "test2_gid_1      10.0      10.0\n",
      "test2_gid_2       5.0       5.0\n",
      "test4_gid_1      10.0      10.0\n",
      "t_df\n",
      "                   gid        gname  vertex_id       tes_name\n",
      "tes_id                                                       \n",
      "test1_gid_1  test1_gid  test1_gname          5  test1_gname_1\n",
      "test2_gid_1  test2_gid  test2_gname          6  test2_gname_1\n",
      "test2_gid_2  test2_gid  test2_gname          7  test2_gname_2\n",
      "test4_gid_1  test4_gid  test4_gname          7  test4_gname_1\n",
      "                   gid        gname  vertex_id       tss_name\n",
      "tss_id                                                       \n",
      "test1_gid_1  test1_gid  test1_gname          0  test1_gname_1\n",
      "test2_gid_1  test2_gid  test2_gname         12  test2_gname_1\n",
      "test4_gid_1  test4_gid  test4_gname          6  test4_gname_1\n",
      "tid\n",
      "test1       [0, 1, 2, 3, 4, 5]\n",
      "test2    [12, 11, 10, 8, 7, 6]\n",
      "test3    [12, 11, 10, 9, 7, 6]\n",
      "test4                   [6, 7]\n",
      "test5           [12, 11, 8, 7]\n",
      "test6    [13, 11, 10, 8, 7, 6]\n",
      "Name: loc_path, dtype: object\n",
      "         tid\n",
      "tid         \n",
      "test1  test1\n",
      "test2  test2\n",
      "test3  test3\n",
      "test4  test4\n",
      "test5  test5\n",
      "[[ 5. 10.  0. 10.  5.]\n",
      " [ 5.  0. 10. 10.  5.]]\n",
      "                   gid        gname  vertex_id       tss_name\n",
      "tss_id                                                       \n",
      "test1_gid_1  test1_gid  test1_gname          0  test1_gname_1\n",
      "test2_gid_1  test2_gid  test2_gname         12  test2_gname_1\n",
      "test4_gid_1  test4_gid  test4_gname          6  test4_gname_1\n",
      "[[ 5. 15. 10.]\n",
      " [ 5. 15. 10.]]\n",
      "[[166666.69 500000.03 333333.38]\n",
      " [166666.69 500000.03 333333.38]]\n",
      "[[100. 100. 100.]\n",
      " [100. 100. 100.]]\n",
      "                   gid        gname  vertex_id       tes_name\n",
      "tes_id                                                       \n",
      "test1_gid_1  test1_gid  test1_gname          5  test1_gname_1\n",
      "test2_gid_1  test2_gid  test2_gname          6  test2_gname_1\n",
      "test2_gid_2  test2_gid  test2_gname          7  test2_gname_2\n",
      "test4_gid_1  test4_gid  test4_gname          7  test4_gname_1\n",
      "             gid    tid               loc_path\n",
      "tid                                           \n",
      "test1  test1_gid  test1     [0, 1, 2, 3, 4, 5]\n",
      "test2  test2_gid  test2  [12, 11, 10, 8, 7, 6]\n",
      "test3  test2_gid  test3  [12, 11, 10, 9, 7, 6]\n",
      "test4  test4_gid  test4                 [6, 7]\n",
      "test5  test2_gid  test5         [12, 11, 8, 7]\n",
      "test6  test2_gid  test6  [13, 11, 10, 8, 7, 6]\n",
      "         tid\n",
      "tid         \n",
      "test1  test1\n",
      "test2  test2\n",
      "test3  test3\n",
      "test4  test4\n",
      "test5  test5\n",
      "[[ 5. 10.  0. 10.  5.]\n",
      " [ 5.  0. 10. 10.  5.]]\n",
      "                   gid        gname  vertex_id       tes_name\n",
      "tes_id                                                       \n",
      "test1_gid_1  test1_gid  test1_gname          5  test1_gname_1\n",
      "test2_gid_1  test2_gid  test2_gname          6  test2_gname_1\n",
      "test2_gid_2  test2_gid  test2_gname          7  test2_gname_2\n",
      "test4_gid_1  test4_gid  test4_gname          7  test4_gname_1\n",
      "[[ 5. 10.  5. 10.]\n",
      " [ 5. 10.  5. 10.]]\n",
      "[[166666.69 333333.38 166666.69 333333.38]\n",
      " [166666.69 333333.38 166666.69 333333.38]]\n",
      "[[100.        66.66667   33.333336 100.      ]\n",
      " [100.        66.66667   33.333336 100.      ]]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/anndata/_core/anndata.py:120: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "sg = swan.SwanGraph()\n",
    "sg.add_annotation('../testing/files/test_full_annotation.gtf')\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_talon_1.tsv')\n",
    "\n",
    "print(sg.tss_adata.var.head())\n",
    "print(sg.t_df.loc_path)\n",
    "print(sg.adata.var.head())\n",
    "print(sg.adata.layers['counts'].toarray())\n",
    "print(sg.tss_adata.var.head())\n",
    "print(sg.tss_adata.layers['counts'].toarray())\n",
    "print(sg.tss_adata.layers['tpm'].toarray())\n",
    "print(sg.tss_adata.layers['pi'].toarray())\n",
    "\n",
    "print(sg.tes_adata.var.head())\n",
    "print(sg.t_df[['gid', 'tid', 'loc_path']])\n",
    "print(sg.adata.var.head())\n",
    "print(sg.adata.layers['counts'].toarray())\n",
    "print(sg.tes_adata.var)\n",
    "print(sg.tes_adata.layers['counts'].toarray())\n",
    "print(sg.tes_adata.layers['tpm'].toarray())\n",
    "print(sg.tes_adata.layers['pi'].toarray())\n",
    "\n",
    "print(type(sg.tes_adata.layers['counts']))\n",
    "print(type(sg.tes_adata.layers['tpm']))\n",
    "print(type(sg.tes_adata.layers['pi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swan_vis as swan\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "international-compact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding annotation to the SwanGraph\n",
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n",
      "   dataset1  dataset2        gid\n",
      "0       5.0       5.0  test1_gid\n",
      "1      10.0       0.0  test2_gid\n",
      "2       0.0      10.0  test2_gid\n",
      "3      10.0      10.0  test4_gid\n",
      "4       5.0       5.0  test2_gid\n",
      "dataset1    float32\n",
      "dataset2    float32\n",
      "gid          object\n",
      "dtype: object\n",
      "Calculating edge usage...\n",
      "Calculating TSS usage...\n",
      "   dataset1  dataset2        gid\n",
      "0       5.0       5.0  test1_gid\n",
      "1      15.0      15.0  test2_gid\n",
      "2      10.0      10.0  test4_gid\n",
      "dataset1    float32\n",
      "dataset2    float32\n",
      "gid          object\n",
      "dtype: object\n",
      "Calculating TES usage...\n",
      "   dataset1  dataset2        gid\n",
      "0       5.0       5.0  test1_gid\n",
      "1      10.0      10.0  test2_gid\n",
      "2       5.0       5.0  test2_gid\n",
      "3      10.0      10.0  test4_gid\n",
      "dataset1    float32\n",
      "dataset2    float32\n",
      "gid          object\n",
      "dtype: object\n",
      "['test1', 'test2', 'test3', 'test4', 'test5', 'test6']\n",
      "['test1', 'test2', 'test3', 'test4', 'test5']\n",
      "[[ 5. 10.  0. 10.  5.]\n",
      " [ 5.  0. 10. 10.  5.]]\n",
      "[[166666.69 333333.38      0.   333333.38 166666.69]\n",
      " [166666.69      0.   333333.38 333333.38 166666.69]]\n",
      "[[100.        66.66667    0.       100.        33.333336]\n",
      " [100.         0.        66.66667  100.        33.333336]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fairliereese/miniconda3/lib/python3.7/site-packages/anndata/_core/anndata.py:120: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "# test_add_abundance_3\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_annotation('../testing/files/test_full_annotation.gtf')\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg.add_abundance('../testing/files/test_ab_talon_1.tsv')\n",
    "print(sg.t_df.index.tolist())\n",
    "print(sg.adata.var.index.tolist())\n",
    "print(sg.adata.layers['counts'].toarray())\n",
    "print(sg.adata.layers['tpm'].toarray())\n",
    "print(sg.adata.layers['pi'].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "interior-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding annotation to the SwanGraph\n",
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n",
      "\n",
      "Adding abundance for datasets dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n",
      "['test1', 'test2', 'test3', 'test4', 'test5', 'test6']\n",
      "['test1', 'test2', 'test3', 'test4', 'test5']\n",
      "[[ 5. 10.  0. 10.  5.]\n",
      " [ 5.  0. 10. 10.  5.]]\n",
      "[[166666.69 333333.38      0.   333333.38 166666.69]\n",
      " [166666.69      0.   333333.38 333333.38 166666.69]]\n",
      "[[100.        66.66667    0.       100.        33.333336]\n",
      " [100.         0.        66.66667  100.        33.333336]]\n"
     ]
    }
   ],
   "source": [
    "# test_add_abundance_2\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_annotation('../testing/files/test_full_annotation.gtf')\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_dataset1.tsv')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_dataset2.tsv')\n",
    "\n",
    "print(sg.t_df.index.tolist())\n",
    "print(sg.adata.var.index.tolist())\n",
    "print(sg.adata.layers['counts'].toarray())\n",
    "print(sg.adata.layers['tpm'].toarray())\n",
    "print(sg.adata.layers['pi'].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "adverse-taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding annotation to the SwanGraph\n",
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n",
      "['test1', 'test2', 'test3', 'test4', 'test5', 'test6']\n",
      "['test1', 'test2', 'test3', 'test4', 'test5']\n",
      "[[ 5. 10.  0. 10.  5.]\n",
      " [ 5.  0. 10. 10.  5.]]\n",
      "[[166666.69 333333.38      0.   333333.38 166666.69]\n",
      " [166666.69      0.   333333.38 333333.38 166666.69]]\n",
      "[[100.        66.66667    0.       100.        33.333336]\n",
      " [100.         0.        66.66667  100.        33.333336]]\n"
     ]
    }
   ],
   "source": [
    "# test_add_abundance_1\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_annotation('../testing/files/test_full_annotation.gtf')\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "\n",
    "print(sg.t_df.index.tolist())\n",
    "print(sg.adata.var.index.tolist())\n",
    "print(sg.adata.layers['counts'].toarray())\n",
    "print(sg.adata.layers['tpm'].toarray())\n",
    "print(sg.adata.layers['pi'].toarray())\n",
    "\n",
    "\n",
    "# looks good but tests still needa be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "romantic-result",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n",
      "Calculating TSS usage...\n",
      "Calculating TES usage...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tid</th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>test4</th>\n",
       "      <th>test5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>33.333336</td>\n",
       "      <td>33.333336</td>\n",
       "      <td>100.0</td>\n",
       "      <td>33.333336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tid  test1      test2      test3  test4      test5\n",
       "c1   100.0  33.333336  33.333336  100.0  33.333336"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_calc_pi_2\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "sg.adata.obs['cluster'] = 'c1'\n",
    "test_df, test_sums = calc_pi(sg.adata, sg.t_df, obs_col='cluster')\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "amateur-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n",
      "Calculating PI...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tid</th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>test4</th>\n",
       "      <th>test5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>33.333336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.666672</td>\n",
       "      <td>100.0</td>\n",
       "      <td>33.333336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tid       test1      test2      test3  test4      test5\n",
       "dataset1  100.0  66.666672   0.000000  100.0  33.333336\n",
       "dataset2  100.0   0.000000  66.666672  100.0  33.333336"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_calc_pi_1\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "test_df, test_sums = calc_pi(sg.adata, sg.t_df, obs_col='dataset')\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "concrete-photograph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in graph from test_mousewg.p\n"
     ]
    }
   ],
   "source": [
    "sg = swan.read('test_mousewg.p')\n",
    "ab = '/Users/fairliereese/mortazavi_lab/data/mousewg/lr_bulk/talon/mouse_talon_abundance_filtered.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "boxed-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding abundance for datasets gastroc_14d_f_2, gastroc_14d_f_1, heart_18-20mo_m_1, heart_18-20mo_m_2, heart_18-20mo_f_1... (and 86 more) to SwanGraph\n",
      "Calculating transcript TPM...\n"
     ]
    }
   ],
   "source": [
    "# test adding de novo\n",
    "sg = add_abundance(sg, ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "casual-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n",
      "Calculating transcript TPM...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>test4</th>\n",
       "      <th>test5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset1</th>\n",
       "      <td>166666.671875</td>\n",
       "      <td>333333.34375</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>333333.34375</td>\n",
       "      <td>166666.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset2</th>\n",
       "      <td>166666.671875</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>333333.34375</td>\n",
       "      <td>333333.34375</td>\n",
       "      <td>166666.671875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  test1         test2         test3         test4  \\\n",
       "dataset                                                             \n",
       "dataset1  166666.671875  333333.34375       0.00000  333333.34375   \n",
       "dataset2  166666.671875       0.00000  333333.34375  333333.34375   \n",
       "\n",
       "                  test5  \n",
       "dataset                  \n",
       "dataset1  166666.671875  \n",
       "dataset2  166666.671875  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_calc_tpm_1\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "sg.adata.obs['cluster'] = ['c1', 'c1']\n",
    "\n",
    "df = calc_tpm(sg.adata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "welsh-lloyd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[166666.67, 333333.34,      0.  , 333333.34, 166666.67],\n",
       "       [166666.67,      0.  , 333333.34, 333333.34, 166666.67]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.adata.layers['tpm'].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "sexual-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>test4</th>\n",
       "      <th>test5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>166666.671875</td>\n",
       "      <td>166666.671875</td>\n",
       "      <td>166666.671875</td>\n",
       "      <td>333333.34375</td>\n",
       "      <td>166666.671875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test1          test2          test3         test4  \\\n",
       "cluster                                                              \n",
       "c1       166666.671875  166666.671875  166666.671875  333333.34375   \n",
       "\n",
       "                 test5  \n",
       "cluster                 \n",
       "c1       166666.671875  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_calc_tpm_2\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "sg.adata.obs['cluster'] = ['c1', 'c1']\n",
    "\n",
    "df = calc_tpm(sg.adata, obs_col='cluster')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "recognized-bishop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>test4</th>\n",
       "      <th>test5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          test1  test2  test3  test4  test5\n",
       "dataset                                    \n",
       "dataset1    5.0   10.0    0.0   10.0    5.0\n",
       "dataset2    5.0    0.0   10.0   10.0    5.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_calc_total_counts_1\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "sg.adata.obs['cluster'] = ['c1', 'c1']\n",
    "\n",
    "df = calc_total_counts(sg.adata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "compatible-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding transcriptome to the SwanGraph\n",
      "\n",
      "Adding abundance for datasets dataset1, dataset2 to SwanGraph.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>test4</th>\n",
       "      <th>test5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         test1  test2  test3  test4  test5\n",
       "cluster                                   \n",
       "c1        10.0   10.0   10.0   20.0   10.0"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_calc_total_counts_2\n",
    "sg = swan.SwanGraph()\n",
    "sg.add_transcriptome('../testing/files/test_full.gtf')\n",
    "sg = add_abundance(sg, '../testing/files/test_ab_1.tsv')\n",
    "sg.adata.obs['cluster'] = ['c1', 'c1']\n",
    "\n",
    "df = calc_total_counts(sg.adata, obs_col='cluster')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test merging when incoming adata has duplicate dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test merging when adding new dataset adds new transcript id to the adata - already tested with test_add_abundance_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
