Metadata-Version: 2.1
Name: promptrix
Version: 0.3.1
Summary: Promptrix. A prompt layout manager for LLMs
Author-email: Steven Ickman <author@example.com>, Bruce DAmbrosio <bruce.dambrosio@gmail.com>
Project-URL: Homepage, https://tuuyi.io/promptrix
Project-URL: Bug Tracker, https://github.com/Stevenic/promptrix-py/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: tiktoken
Requires-Dist: pyyaml
Requires-Dist: importlib-metadata ; python_version < "3.9"

# promptrix-py
Promptrix is a prompt layout engine for Large Language Models.

# Here is a first trivial example:

    from promptrix import promptrixTypes, VolatileMemory, FunctionRegistry, GPT3Tokenizer
    from promptrix.Prompt import Prompt
    from promptrix.SystemMessage import SystemMessage
    from promptrix.UserMessage import UserMessage
    from promptrix.AssistantMessage import AssistantMessage
    from promptrix.ConversationHistory import ConversationHistory

    functions = FunctionRegistry()
    tokenizer = GPT3Tokenizer()
    memory = VolatileMemory({'input':'', 'history':[]})
    max_tokens = 2000

    prompt_text = 'You are helpful, creative, clever, and very friendly. '
    PROMPT = Prompt([
        UserMessage(prompt_text),
        ConversationHistory('history', .5),    # allow history to use up 1/2 the remaining token budget left after the prompt and input 
        UserMessage('{{$input}}')
    ])

    async def render_messages_completion():
        as_msgs = await PROMPT.renderAsMessages(memory, functions, tokenizer, max_tokens)
        msgs = []
        if not as_msgs.tooLong:
            msgs = as_msgs.output
        return msgs

    ### basic chat loop
    while True:
        memory.set('input', query)
        msgs = asyncio.run(render_messages_completion())
        response = ... your favorite llm api (model, msgs, ...)
        print(response)
        history = memory.get('history')
        history.append({'role':USER_PREFIX, 'content': query})
        history.append({'role': ASSISTANT_PREFIX, 'content': response})
        memory.set('history', history)
 
