{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spyglass Spike Sorting Tutorial\n",
    "\n",
    "**Note: make a copy of this notebook and run the copy to avoid git conflicts in the future**\n",
    "\n",
    "This is the second in a multi-part tutorial on the Spyglass pipeline used in Loren Frank's lab, UCSF. It demonstrates how to run spike sorting and curate units within the pipeline.\n",
    "\n",
    "If you have not done [tutorial 0](0_intro.ipynb) yet, make sure to do so before proceeding. It is also recommended that you complete the [datajoint tutorials](https://tutorials.datajoint.io/) before starting this. \n",
    "\n",
    "**Note 2: Make sure you are running this within the spyglass Conda environment)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike Sorting Overview - With associated tables\n",
    "#### [Extracting the recording from the NWB file](#section1)<br>\n",
    "1. Specifying your [NWB](#Specifying-your-NWB-filename) file.<br>\n",
    "2. Specifying which electrodes involved in the recording to sort data from. - [`SortGroup`](#SortGroup)<br>\n",
    "3. Specifying the time segment of the recording we want to sort. - [`IntervalList`](#IntervalList), [`SortInterval`](#SortInterval)<br>\n",
    "4. Specifying the parameters to use for filtering the recording. - [`SpikeSortingPreprocessingParameters`](#SpikeSortingPreprocessingParameters)<br>\n",
    "5. Combining these parameters. - [`SpikeSortingRecordingSelection`](#SpikeSortingRecordingSelection)<br>\n",
    "6. Extracting the recording. - [`SpikeSortingRecording`](#SpikeSortingRecording)<br>\n",
    "7. Specifying the parameters to apply for artifact detection/removal. -[`ArtifactDetectionParameters`](#ArtifactDetectionParameters)<br>\n",
    "    \n",
    "#### [Spike sorting the extracted recording](#section2)<br>\n",
    "1. Specify the spike sorter and parameters to use. - [`SpikeSorterParameters`](#SpikeSorterParameters)<br>\n",
    "2. Combine these parameters. - [`SpikeSortingSelection`](#SpikeSortingSelection)<br>\n",
    "3. Spike sort the extracted recording according to chose parameter set. - [`SpikeSorting`](#SpikeSorting)<br>\n",
    "\n",
    "<a href='#section1'></a>\n",
    "<a href='#section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing tables from spyglass along with some other useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import spyglass as sg\n",
    "import spyglass.common as sgc\n",
    "import spyglass.spikesorting as sgss\n",
    "\n",
    "# ignore datajoint+jupyter async warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ResourceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure that you're a part of the LorenLab `LabTeam`, so you'll have the right permissions for this tutorial.<br>Replace `your_name`, `your_email`, and `datajoint_username`, with your information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = \"FirstName LastName\"\n",
    "your_email = \"gmail@gmail.com\"\n",
    "datajoint_username = \"user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorenlab_team_members = (\n",
    "    (sgc.LabTeam().LabTeamMember() & {\"team_name\": \"LorenLab\"})\n",
    "    .fetch(\"lab_member_name\")\n",
    "    .tolist()\n",
    ")\n",
    "sgc.LabMember.insert_from_name(your_name)\n",
    "sgc.LabMember.LabMemberInfo.insert1(\n",
    "    [your_name, your_email, datajoint_username], skip_duplicates=True\n",
    ")\n",
    "sgc.LabTeam.LabTeamMember.insert1(\n",
    "    {\"team_name\": \"LorenLab\", \"lab_member_name\": your_name},\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try and use the `fetch` command and the example above to complete the code below and query the `LabTeam` table to double-check that you are a part of the `LorenLab` team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if your_name in (sgc.LabTeam._____() & {___: ____}).fetch(______).tolist():\n",
    "    print(\"You made it in!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying your NWB filename\n",
    "NWB filenames take the form of an animal name plus the date of the recording.<br>For this tutorial, we will use the nwb file `'montague20200802_tutorial_.nwb'`. The animal name is `montague` and the date of the recording is `20200802`, the `tutorial` is unique to this setting :-).<br>We'll first re-insert the NWB file that you deleted at the end of `0_intro`. This file may already be in the table, in which case there will be a warning, that's ok!<br>**Note**: If you're not on the Frank Lab server, this file will be accessible as `montague20200802.nwb` on DANDI through this URL: **insert-url**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spyglass.data_import as sdi\n",
    "\n",
    "sdi.insert_sessions(\"montague20200802_tutorial.nwb\")\n",
    "nwb_file_name = \"montague20200802_tutorial_.nwb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE**: This is a common file for this tutorial, table entries may change during use if others are also working through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the recording from the NWB file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SortGroup`\n",
    "For each NWB file there will be multiple electrodes available to sort spikes from.<br>We commonly sort over multiple electrodes at a time, also referred to as a `SortGroup`.<br>This is accomplished by grouping electrodes according to what tetrode or shank of a probe they were on.<br>**NOTE**: answer 'yes' when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer 'yes' when prompted\n",
    "sgss.SortGroup().set_group_by_shank(nwb_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each electrode will have an `electrode_id` and be associated with an `electrode_group_name`, which will correspond with a `sort_group_id`. In this case, the data was recorded from a 32 tetrode (128 channel) drive, and thus results in 128 unique `electrode_id`, 32 unique `electrode_group_name`, and 32 unique `sort_group_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SortGroup.SortGroupElectrode & {\"nwb_file_name\": nwb_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `IntervalList`\n",
    "Next, we make a decision about the time interval for our spike sorting. Let's re-examine `IntervalList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgc.IntervalList & {\"nwb_file_name\": nwb_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, let's choose to start with the first run interval (`02_r1`) as our sort interval. We first fetch `valid_times` for this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_list_name = \"02_r1\"\n",
    "interval_list = (\n",
    "    sgc.IntervalList\n",
    "    & {\"nwb_file_name\": nwb_file_name, \"interval_list_name\": interval_list_name}\n",
    ").fetch1(\"valid_times\")\n",
    "print(\n",
    "    f\"IntervalList begins as a {np.round((interval_list[0][1] - interval_list[0][0]) / 60,0):g} min long epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SortInterval`\n",
    "For brevity's sake, we'll select only the first 180 seconds of that 90 minute epoch as our sort interval. To do so, we define our new sort interval as the start time of `interval_list` from the previous cell, plus 180 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_interval = interval_list[0]\n",
    "sort_interval_name = interval_list_name + \"_first180\"\n",
    "sort_interval = np.copy(interval_list[0])\n",
    "sort_interval[1] = sort_interval[0] + 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add this `sort_interval` with the specified `sort_interval_name` `'02_r1_first180'` to the `SortInterval` table. The `SortInterval.insert()` function requires the arguments input as a dictionary with keys `nwb_file_name`, `sort_interval_name`, and `sort_interval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SortInterval.insert1(\n",
    "    {\n",
    "        \"nwb_file_name\": nwb_file_name,\n",
    "        \"sort_interval_name\": sort_interval_name,\n",
    "        \"sort_interval\": sort_interval,\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted the entry into `SortInterval` you can see that entry by querying `SortInterval` using the `nwb_file_name` and `sort_interval_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SortInterval & {\n",
    "    \"nwb_file_name\": nwb_file_name,\n",
    "    \"sort_interval_name\": sort_interval_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `.fetch()` command, you can retrieve your user-defined sort interval from the `SortInterval` table.<br>A quick double-check will show that it is indeed a 180 second segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_sort_interval = (\n",
    "    sgss.SortInterval\n",
    "    & {\"nwb_file_name\": nwb_file_name, \"sort_interval_name\": sort_interval_name}\n",
    ").fetch1(\"sort_interval\")\n",
    "print(\n",
    "    f\"The sort interval goes from {fetched_sort_interval[0]} to {fetched_sort_interval[1]}, \\\n",
    "which is {(fetched_sort_interval[1] - fetched_sort_interval[0])} seconds. COOL!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SpikeSortingPreprocessingParameters`\n",
    "Let's first take a look at the `SpikeSortingPreprocessingParameters` table, which contains the parameters used to filter the recorded data in the spike band prior to spike sorting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SpikeSortingPreprocessingParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the filtering parameters. Here we insert the default parameters, and then fetch the default parameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SpikeSortingPreprocessingParameters().insert_default()\n",
    "preproc_params = (\n",
    "    sgss.SpikeSortingPreprocessingParameters()\n",
    "    & {\"preproc_params_name\": \"default\"}\n",
    ").fetch1(\"preproc_params\")\n",
    "print(preproc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first adjust the `frequency_min` parameter to 600, which is the preference for hippocampal data. Now we can insert that into `SpikeSortingPreprocessingParameters` as a new set of filtering parameters for hippocampal data, named `'franklab_default_hippocampus'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_params[\"frequency_min\"] = 600\n",
    "sgss.SpikeSortingPreprocessingParameters().insert1(\n",
    "    {\n",
    "        \"preproc_params_name\": \"franklab_default_hippocampus\",\n",
    "        \"preproc_params\": preproc_params,\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting a key\n",
    "Now we set up the parameters of the recording we are interested in, and make a dictionary to hold all these values, which will make querying and inserting into tables all the easier moving forward.<br>We'll assign this to `ssr_key` as these values are relvant to the recording we'll use to spike sort, also referred to as the spike sorting recording **(ssr)** :-)<br>The `sort_group_id` refers back to the `SortGroup` table we populated at the beginning of the tutorial. We'll use `sort_group_id` 10 here. <br>Our `sort_interval_name` is the same as above: `'02_r1_first600'`.<br>Our `preproc_params_name` is the same ones we just inserted into `SpikeSortingPreprocessingParameters`.<br>The `interval_list` was also set above as `'02_r1'`. Unlike `sort_interval_name`, which reflects our subsection of the recording, we keep `interval_list` unchanged from the original epoch name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = dict()\n",
    "key[\"nwb_file_name\"] = nwb_file_name\n",
    "key[\"sort_group_id\"] = 10\n",
    "key[\"sort_interval_name\"] = \"02_r1_first180\"\n",
    "key[\"preproc_params_name\"] = \"franklab_default_hippocampus\"\n",
    "key[\"interval_list_name\"] = \"02_r1\"\n",
    "key[\"team_name\"] = \"LorenLab\"\n",
    "\n",
    "ssr_key = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SpikeSortingRecordingSelection`\n",
    "We now insert all of these parameters into the `SpikeSortingRecordingSelection` table, which we will use to specify what time/tetrode/etc of the recording we want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True)\n",
    "sgss.SpikeSortingRecordingSelection() & ssr_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SpikeSortingRecording`\n",
    "And now we're ready to extract the recording! We use the `.proj()` command to pass along all of the primary keys from the `SpikeSortingRecordingSelection` table to the `SpikeSortingRecording` table, so it knows exactly what to extract.<br>**Note**: This step might take a bit with longer duration sort intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SpikeSortingRecording.populate(\n",
    "    [(sgss.SpikeSortingRecordingSelection & ssr_key).proj()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can see our recording in the table. _E x c i t i n g !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SpikeSortingRecording() & ssr_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ArtifactDetectionParameters`\n",
    "Similarly, we set up the `ArtifactDetectionParameters`, which can allow us to remove artifacts from the data.<br>Specifically, we want to target artifact signal that is within the frequency band of our filter (600Hz-6KHz), and thus will not get removed by filtering.<br>For the moment we just set up a `\"none\"` parameter set, which will do nothing when used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.ArtifactDetectionParameters().insert_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_key = (sgss.SpikeSortingRecording() & ssr_key).fetch1(\"KEY\")\n",
    "artifact_key[\"artifact_params_name\"] = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pair your choice of artifact detection parameters (as entered into `ArtifactParameters`) with the recording you just extracted through population of `SpikeSortingRecording` and insert into `ArtifactDetectionSelection`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.ArtifactDetectionSelection().insert1(artifact_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.ArtifactDetectionSelection() & artifact_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted into `ArtifactDetectionSelection` we're ready to populate the `ArtifactDetection` table, which will find periods where there are artifacts (as specified by your parameters) in the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.ArtifactDetection.populate(artifact_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populating `ArtifactDetection` also inserts an entry into `ArtifactRemovedIntervalList`, which stores the interval without detected artifacts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.ArtifactRemovedIntervalList() & artifact_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike sorting the extracted recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SpikeSorterParameters`\n",
    "For our example, we will be using `mountainsort4`. There are already some default parameters in the `SpikeSorterParameters` table we'll `fetch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgss.SpikeSorterParameters().insert_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the default params\n",
    "sorter_name = \"mountainsort4\"\n",
    "ms4_default_params = (\n",
    "    sgss.SpikeSorterParameters\n",
    "    & {\"sorter\": sorter_name, \"sorter_params_name\": \"default\"}\n",
    ").fetch1()\n",
    "print(ms4_default_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can change these default parameters to line up more closely with our preferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_dict = ms4_default_params[\"sorter_params\"]\n",
    "# Detect downward going spikes (1 is for upward, 0 is for both up and down)\n",
    "param_dict[\"detect_sign\"] = -1\n",
    "# We will sort electrodes together that are within 100 microns of each other\n",
    "param_dict[\"adjacency_radius\"] = 100\n",
    "# Turn filter off since we will filter it prior to starting sort\n",
    "param_dict[\"filter\"] = False\n",
    "param_dict[\"freq_min\"] = 0\n",
    "param_dict[\"freq_max\"] = 0\n",
    "# Turn whiten off since we will whiten it prior to starting sort\n",
    "param_dict[\"whiten\"] = False\n",
    "# set num_workers to be the same number as the number of electrodes\n",
    "param_dict[\"num_workers\"] = 4\n",
    "param_dict[\"verbose\"] = True\n",
    "# set clip size as number of samples for 1.33 millisecond based on the sampling rate\n",
    "param_dict[\"clip_size\"] = np.int(\n",
    "    1.33e-3\n",
    "    * (sgc.Raw & {\"nwb_file_name\": nwb_file_name}).fetch1(\"sampling_rate\")\n",
    ")\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and insert a new `sorter_params_name` and `sorter_params` dict into the `SpikeSorterParameters` table named `franklab_hippocampus_tutorial`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_set_name = \"franklab_hippocampus_tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we insert our parameters for use by the spike sorter into `SpikeSorterParameters` and double-check that it made it in to the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgss.SpikeSorterParameters.insert1(\n",
    "    {\n",
    "        \"sorter\": sorter_name,\n",
    "        \"sorter_params_name\": parameter_set_name,\n",
    "        \"sorter_params\": param_dict,\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "# Check that insert was successful\n",
    "p = (\n",
    "    sgss.SpikeSorterParameters\n",
    "    & {\"sorter\": sorter_name, \"sorter_params_name\": parameter_set_name}\n",
    ").fetch1()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SpikeSortingSelection`\n",
    "##### Gearing up to Spike Sort!\n",
    "We now collect all the decisions we made up to here and put it into the `SpikeSortingSelection` table, which is specific to this recording and eventual sorting segment.<br>We'll add in a few parameters to our key and call it `ss_key`.<br>(**note**: the spike *sorter* parameters defined above are for the sorter, `mountainsort4` in this case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_key = (sgss.ArtifactDetection & ssr_key).fetch1(\"KEY\")\n",
    "ss_key.update((sgss.ArtifactRemovedIntervalList() & key).fetch1(\"KEY\"))\n",
    "ss_key[\"sorter\"] = sorter_name\n",
    "ss_key[\"sorter_params_name\"] = parameter_set_name\n",
    "del ss_key[\"artifact_params_name\"]\n",
    "sgss.SpikeSortingSelection.insert1(ss_key, skip_duplicates=True)\n",
    "(sgss.SpikeSortingSelection & ss_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SpikeSorting`\n",
    "Now we can run spike sorting. It's nothing more than populating a table (`SpikeSorting`) based on the entries of `SpikeSortingSelection`.<br>**Note**: This will take a little bit with longer data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sgss.SpikeSorting.populate([(sgss.SpikeSortingSelection & ss_key).proj()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to make sure the table populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgss.SpikeSorting() & ss_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you've spike sorted! See 2_curation for the next tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
