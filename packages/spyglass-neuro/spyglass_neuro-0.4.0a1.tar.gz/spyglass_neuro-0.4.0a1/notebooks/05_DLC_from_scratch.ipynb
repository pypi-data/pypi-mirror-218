{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93a1550-8a67-4346-a4bf-e5a136f3d903",
   "metadata": {},
   "source": [
    "## Position using DeepLabCut from Scratch\n",
    "\n",
    "**Note: make a copy of this notebook and run the copy to avoid git conflicts in the future**\n",
    "\n",
    "This is a tutorial on how to extract position via DeepLabCut (DLC) using the Spyglass pipeline used in Loren Frank's lab, UCSF. It will walk through creating your DLC project, extracting and labeling frames, training your model, executing pose estimation on a novel behavioral video, processing the pose estimation output to extract a centroid and orientation, and inserting the resulting information into the `IntervalPositionInfo` table.<br>\n",
    "-> This tutorial assumes you've completed [tutorial 0](0_intro.ipynb)<br>\n",
    "**Note 2: Make sure you are running this within the spyglass Conda environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f567531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-20 14:51:05,338][INFO]: Connecting dgramling@lmf-db.cin.ucsf.edu:3306\n",
      "[2023-04-20 14:51:05,410][INFO]: Connected dgramling@lmf-db.cin.ucsf.edu:3306\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, PosixPath, PurePath\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pynwb\n",
    "import datajoint as dj\n",
    "import spyglass.common as sgc\n",
    "import spyglass.position.v1 as sgp\n",
    "from spyglass.position import FinalPosition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8b531f7",
   "metadata": {},
   "source": [
    "#### Here is a schematic showing the tables used in this notebook.<br>\n",
    "![dlc_scratch.png|2000x900](./../notebook-images/dlc_scratch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d88c-c90e-467b-ae2e-672c49a12f95",
   "metadata": {},
   "source": [
    "### Table of Contents<a id='TableOfContents'></a>\n",
    "[`DLCProject`](#DLCProject1)<br>\n",
    "[`DLCModelTraining`](#DLCModelTraining1)<br>\n",
    "[`DLCModel`](#DLCModel1)<br>\n",
    "[`DLCPoseEstimation`](#DLCPoseEstimation1)<br>\n",
    "[`DLCSmoothInterp`](#DLCSmoothInterp1)<br>\n",
    "[`DLCCentroid`](#DLCCentroid1)<br>\n",
    "[`DLCOrientation`](#DLCOrientation1)<br>\n",
    "[`DLCPos`](#DLCPos1)<br>\n",
    "[`DLCPosVideo`](#DLCPosVideo1)<br>\n",
    "[`PosSource`](#PosSource1)<br>\n",
    "[`IntervalPositionInfo`](#IntervalPositionInfo1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6221a3-17e5-45c0-aa40-2fd664b02219",
   "metadata": {},
   "source": [
    "#### [DLCProject](#TableOfContents) <a id=\"DLCProject1\"></a>\n",
    "__You can click on any header to return to the Table of Contents__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aed0e1-3af7-4499-bae8-96a64e81041e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The cells within the <code>DLCProject</code> step need to be performed in a local Jupyter notebook to allow for use of the frame labeling GUI.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96637cb9-519d-41e1-8bfd-69f68dc66b36",
   "metadata": {},
   "source": [
    "Let us begin with visualizing the contents of the BodyPart table. This table will store standard names of body parts used within DLC models throughout the lab with a concise description.<br>\n",
    "><div class=\"alert alert-block alert-warning\">Please do not add to this table unless necessary.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69f829f-9877-48ae-89d1-f876af2b8835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    <style type=\"text/css\">\n",
       "        .Relation{\n",
       "            border-collapse:collapse;\n",
       "        }\n",
       "        .Relation th{\n",
       "            background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "            font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "        }\n",
       "        .Relation td{\n",
       "            padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "        }\n",
       "        .Relation tr:nth-child(odd){\n",
       "            background: #ffffff;\n",
       "        }\n",
       "        .Relation tr:nth-child(even){\n",
       "            background: #f3f1ff;\n",
       "        }\n",
       "        /* Tooltip container */\n",
       "        .djtooltip {\n",
       "        }\n",
       "        /* Tooltip text */\n",
       "        .djtooltip .djtooltiptext {\n",
       "            visibility: hidden;\n",
       "            width: 120px;\n",
       "            background-color: black;\n",
       "            color: #fff;\n",
       "            text-align: center;\n",
       "            padding: 5px 0;\n",
       "            border-radius: 6px;\n",
       "            /* Position the tooltip text - see examples below! */\n",
       "            position: absolute;\n",
       "            z-index: 1;\n",
       "        }\n",
       "        #primary {\n",
       "            font-weight: bold;\n",
       "            color: black;\n",
       "        }\n",
       "        #nonprimary {\n",
       "            font-weight: normal;\n",
       "            color: white;\n",
       "        }\n",
       "\n",
       "        /* Show the tooltip text when you mouse over the tooltip container */\n",
       "        .djtooltip:hover .djtooltiptext {\n",
       "            visibility: visible;\n",
       "        }\n",
       "    </style>\n",
       "    \n",
       "    <b></b>\n",
       "        <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "        <table border=\"1\" class=\"Relation\">\n",
       "            <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">bodypart</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">bodypart_description</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div> </th> </tr> </thead>\n",
       "            <tbody> <tr> <td>greenLED</td>\n",
       "<td>green LED on implant LED ring</td></tr><tr><td>redLED</td>\n",
       "<td>redLED</td></tr><tr><td>redLED_C</td>\n",
       "<td>center red LED on implant LED ring</td></tr><tr><td>redLED_L</td>\n",
       "<td>left red LED on implant LED ring</td></tr><tr><td>redLED_R</td>\n",
       "<td>right red LED on implant LED ring</td></tr><tr><td>tailBase</td>\n",
       "<td>base of the tail on subject</td></tr><tr><td>whiteLED</td>\n",
       "<td>white LED on headstage</td> </tr> </tbody>\n",
       "        </table>\n",
       "        \n",
       "        <p>Total: 7</p></div>\n",
       "        "
      ],
      "text/plain": [
       "*bodypart    bodypart_descr\n",
       "+----------+ +------------+\n",
       "greenLED     green LED on i\n",
       "redLED       redLED        \n",
       "redLED_C     center red LED\n",
       "redLED_L     left red LED o\n",
       "redLED_R     right red LED \n",
       "tailBase     base of the ta\n",
       "whiteLED     white LED on h\n",
       " (Total: 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgp.BodyPart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a15e2-f087-4bd2-9d4a-ea2ac4becd80",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If the bodyparts you plan to use in your model are not yet in the table, here is code to add bodyparts:\n",
    "</div>\n",
    "\n",
    ">```python\n",
    "sgp.BodyPart.insert([{'bodypart': 'bodypart_1', 'bodypart_description': 'concise description of bodypart'},\n",
    "                     {'bodypart': 'bodypart_2', 'bodypart_description': 'concise description of bodypart'},],\n",
    "                    skip_duplicates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe7c06-30c9-43e1-9e9a-029a70b0d4dd",
   "metadata": {},
   "source": [
    "Next we want to construct a list of videos from which we will extract frames to train the model.<br>The list can either contain dictionaries identifying behavioral videos for NWB files that have already been added to Spyglass, or absolute file paths to the videos you want to use.<br>For this tutorial, we'll use two videos for which we already have frames labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3aa1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [\n",
    "    {\"nwb_file_name\": \"J1620210529_.nwb\", \"epoch\": 2},\n",
    "    {\"nwb_file_name\": \"peanut20201103_.nwb\", \"epoch\": 4},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c023b0-d00d-40b0-9a37-d0d3e4a4ae2a",
   "metadata": {},
   "source": [
    "Before creating our project, we need to define a few variables.\n",
    ">First, we want to set a team name to one that exists in the `LabTeam` table to ensure proper permission are set.<br>In this case we'll use \"LorenLab\", as all Frank Lab members are a part of this team.<br>\n",
    "We also need to define a `project_name`, which should be a unique identifier for this project. For the tutorial we'll set it as __\"tutorial_scratch_yourinitials\"__<br>Next, we need to define a list of `bodyparts` for which we want to extract position. The pre-labeled frames we're using include the bodyparts listed below, but please modify as needed for your own project.<br>We also want to define how many frames we want to extract and eventually label from each video we're using. I will typically use 200 `frames_per_video`, but we'll keep it to 100 for efficiency's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347e98f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project name: tutorial_scratch_DG is already in use.\n"
     ]
    }
   ],
   "source": [
    "team_name = \"LorenLab\"\n",
    "project_name = \"tutorial_scratch_DG\"\n",
    "frames_per_video = 100\n",
    "bodyparts = [\"redLED_C\", \"greenLED\", \"redLED_L\", \"redLED_R\", \"tailBase\"]\n",
    "project_key = sgp.DLCProject.insert_new_project(\n",
    "    project_name=project_name,\n",
    "    bodyparts=bodyparts,\n",
    "    lab_team=team_name,\n",
    "    frames_per_video=frames_per_video,\n",
    "    video_list=video_list,\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d83452-48eb-4669-89eb-a6beb1f2d051",
   "metadata": {},
   "source": [
    "Now that we've intialized our project we'll need to extract and label frames.<br>While this has already been done for this tutorial, here are the commands in order to pull up the DLC GUI to perform these actions:\n",
    ">```python\n",
    "sgp.DLCProject().run_extract_frames(project_key)\n",
    "sgp.DLCProject().run_label_frames(project_key)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df257015",
   "metadata": {},
   "source": [
    "Typically, in order to use pre-labeled frames to your project you'll need to change the values in the labeled-data files. You can do that using the `import_labeled_frames` method. \n",
    "<blockquote>This function expects the `project_key` from your new project<br>The absolute path to the project you want to import the labeled frames from<br>The filename (without file extension) of the videos from which you want the frames.\n",
    "</blockquote>Here we'll use the path to a pre-existing project from tutorial 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520a9526-fcd1-417b-b368-00d17e0284e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_project.py:451: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n",
      "/home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_project.py:451: FutureWarning: inplace is deprecated and will be removed in a future version.\n",
      "  dlc_df.columns.set_levels([team_name], level=0, inplace=True)\n",
      "2023-04-20 14:51:08.706450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 14:51:08.897194: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.2.3...\n",
      "OpenCV is built with OpenMP support. This usually results in poor performance. For details, see https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py\n"
     ]
    }
   ],
   "source": [
    "sgp.DLCProject.import_labeled_frames(\n",
    "    project_key.copy(),\n",
    "    import_project_path=\"/nimbus/deeplabcut/projects/tutorial_model-LorenLab-2022-07-15/\",\n",
    "    video_filenames=[\"20201103_peanut_04_r2\", \"20210529_J16_02_r1\"],\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e3eab-60c7-4a3c-bc8f-fd4e8dcf52a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>This step and beyond should be run on a GPU-enabled machine.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48ecf0",
   "metadata": {},
   "source": [
    "#### [DLCModelTraining](#ToC)<a id='DLCModelTraining1'></a> \n",
    "Please make sure you're running this notebook on a GPU-enabled machine.<br>\n",
    "Now that we've imported existing frames, we can get ready to train our model.<br>\n",
    "First, we'll need to define a set of parameters for `DLCModelTrainingParams`, which will get used by DeepLabCut during training<br>\n",
    "Let's start with `gputouse`<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<code>gputouse</code> determines which GPU core to use for pose estimation. Run the cell below to determine which core has space and set the <code>gputouse</code> variable accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8fc5bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 80383, 1: 35, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 7: 35, 8: 35, 9: 35}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgp.dlc_utils.get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca035a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Set GPU core here</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff0e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "gputouse = 1  ## 1-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b047686",
   "metadata": {},
   "source": [
    "Now let's define the rest of our parameters and insert the entry.<br>\n",
    "(If you want to see all possible parameters that you can pass, checkout the line below):\n",
    ">```python\n",
    "sgp.DLCModelTrainingParams.get_accepted_params()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399581ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New param set not added\n",
      "A param set with name: tutorial already exists\n"
     ]
    }
   ],
   "source": [
    "training_params_name = \"tutorial\"\n",
    "sgp.DLCModelTrainingParams.insert_new_params(\n",
    "    paramset_name=training_params_name,\n",
    "    params={\n",
    "        \"trainingsetindex\": 0,\n",
    "        \"shuffle\": 1,\n",
    "        \"gputouse\": gputouse,\n",
    "        \"net_type\": \"resnet_50\",\n",
    "        \"augmenter_type\": \"imgaug\",\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cc709",
   "metadata": {},
   "source": [
    "Next we'll modify the `project_key` from above to include the necessary entries for `DLCModelTraining`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7acd150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_key['project_path'] = os.path.dirname(project_key['config_path'])\n",
    "if \"config_path\" in project_key:\n",
    "    del project_key[\"config_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7ddaa",
   "metadata": {},
   "source": [
    "And here we can insert an entry into `DLCModelTrainingSelection` and populate `DLCModelTraining` with that entry, which will run training for us.<br>\n",
    "**Note**: You can stop training at any point using `I + I` or interrupt the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d56d3c39-7b85-4f6a-b9fb-816a1d1912da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Specification for a DLC model training instance\n",
       "project_name         : varchar(100)                 # name of DLC project\n",
       "dlc_training_params_name : varchar(50)                  # descriptive name of parameter set\n",
       "training_id          : int                          # unique integer,\n",
       "---\n",
       "model_prefix=\"\"      : varchar(32)                  # "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgp.DLCModelTrainingSelection.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "139d2f30",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20-Apr-23 15:00:16] in /home/dgramling/Src/spyglass/src/spyglass/position/v1/position_dlc_training.py, line 179: creating training dataset\n",
      "INFO:DLC_project_{project_name}_training:creating training dataset\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nimbus/deeplabcut/projects/tutorial_scratch_DG-LorenLab-2022-11-03/labeled-data/20201103_peanut_04_r2/img22800.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m sgp\u001b[38;5;241m.\u001b[39mDLCModelTrainingSelection()\u001b[38;5;241m.\u001b[39minsert1({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mproject_key,\n\u001b[1;32m      2\u001b[0m                                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdlc_training_params_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_params_name,\n\u001b[1;32m      3\u001b[0m                                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,})\n\u001b[1;32m      5\u001b[0m model_training_key \u001b[38;5;241m=\u001b[39m (sgp\u001b[38;5;241m.\u001b[39mDLCModelTrainingSelection \u001b[38;5;241m&\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mproject_key,\n\u001b[1;32m      6\u001b[0m                                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdlc_training_params_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:training_params_name,})\u001b[38;5;241m.\u001b[39mfetch1(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43msgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDLCModelTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_training_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/datajoint/autopopulate.py:230\u001b[0m, in \u001b[0;36mAutoPopulate.populate\u001b[0;34m(self, suppress_errors, return_exception_objects, reserve_jobs, order, limit, max_calls, display_progress, processes, make_kwargs, *restrictions)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m         tqdm(keys, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m display_progress \u001b[38;5;28;01melse\u001b[39;00m keys\n\u001b[1;32m    229\u001b[0m     ):\n\u001b[0;32m--> 230\u001b[0m         error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopulate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m             error_list\u001b[38;5;241m.\u001b[39mappend(error)\n",
      "File \u001b[0;32m~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/datajoint/autopopulate.py:281\u001b[0m, in \u001b[0;36mAutoPopulate._populate1\u001b[0;34m(self, key, jobs, suppress_errors, return_exception_objects, make_kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_insert \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmake_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Src/spyglass/src/spyglass/position/v1/position_dlc_training.py:180\u001b[0m, in \u001b[0;36mDLCModelTraining.make\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    176\u001b[0m training_dataset_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    177\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dlc_config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m training_dataset_input_args\n\u001b[1;32m    178\u001b[0m }\n\u001b[1;32m    179\u001b[0m logger\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating training dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m \u001b[43mcreate_training_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdlc_cfg_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_dataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# ---- Trigger DLC model training job ----\u001b[39;00m\n\u001b[1;32m    182\u001b[0m train_network_input_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(train_network)\u001b[38;5;241m.\u001b[39mparameters)\n",
      "File \u001b[0;32m~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py:992\u001b[0m, in \u001b[0;36mcreate_training_dataset\u001b[0;34m(config, num_shuffles, Shuffles, windows2linux, userfeedback, trainIndices, testIndices, net_type, augmenter_type, posecfg_template)\u001b[0m\n\u001b[1;32m    982\u001b[0m (\n\u001b[1;32m    983\u001b[0m     datafilename,\n\u001b[1;32m    984\u001b[0m     metadatafilename,\n\u001b[1;32m    985\u001b[0m ) \u001b[38;5;241m=\u001b[39m auxiliaryfunctions\u001b[38;5;241m.\u001b[39mget_data_and_metadata_filenames(\n\u001b[1;32m    986\u001b[0m     trainingsetfolder, trainFraction, shuffle, cfg\n\u001b[1;32m    987\u001b[0m )\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Saving data file (convert to training file for deeper cut (*.mat))\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m data, MatlabData \u001b[38;5;241m=\u001b[39m \u001b[43mformat_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainIndices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbodyparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_path\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m sio\u001b[38;5;241m.\u001b[39msavemat(\n\u001b[1;32m    996\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_path, datafilename), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: MatlabData}\n\u001b[1;32m    997\u001b[0m )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# Saving metadata (Pickle file)\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py:685\u001b[0m, in \u001b[0;36mformat_training_data\u001b[0;34m(df, train_inds, nbodyparts, project_path)\u001b[0m\n\u001b[1;32m    683\u001b[0m filename \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex[i]\n\u001b[1;32m    684\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 685\u001b[0m img_shape \u001b[38;5;241m=\u001b[39m \u001b[43mread_image_shape_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m img_shape\n\u001b[1;32m    687\u001b[0m temp \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py:667\u001b[0m, in \u001b[0;36mread_image_shape_fast\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_image_shape_fast\u001b[39m(path):\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# Blazing fast and does not load the image into memory\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[1;32m    668\u001b[0m         width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mgetbands()), height, width\n",
      "File \u001b[0;32m~/anaconda3/envs/spyglass-position/lib/python3.9/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nimbus/deeplabcut/projects/tutorial_scratch_DG-LorenLab-2022-11-03/labeled-data/20201103_peanut_04_r2/img22800.png'"
     ]
    }
   ],
   "source": [
    "sgp.DLCModelTrainingSelection().insert1(\n",
    "    {\n",
    "        **project_key,\n",
    "        \"dlc_training_params_name\": training_params_name,\n",
    "        \"training_id\": 0,\n",
    "        \"model_prefix\": \"\",\n",
    "    }\n",
    ")\n",
    "model_training_key = (\n",
    "    sgp.DLCModelTrainingSelection\n",
    "    & {\n",
    "        **project_key,\n",
    "        \"dlc_training_params_name\": training_params_name,\n",
    "    }\n",
    ").fetch1(\"KEY\")\n",
    "sgp.DLCModelTraining.populate(model_training_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da004b3e",
   "metadata": {},
   "source": [
    "Here we'll make sure that the entry made it into the table properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5306fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgp.DLCModelTraining() & model_training_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b7687",
   "metadata": {},
   "source": [
    "Populating `DLCModelTraining` automatically inserts the entry into `DLCModelSource`.  `DLCModelSource` is a table that is used to switch between the models we train using Spyglass and pre-existing projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCModelSource() & model_training_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb8969",
   "metadata": {},
   "source": [
    "Notice the `source` field in the table above. It will only accept _\"FromImport\"_ or _\"FromUpstream\"_ as entries. Let's checkout the `FromUpstream` part table attached to `DLCModelSource` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCModelSource.FromUpstream() & model_training_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9b2c6",
   "metadata": {},
   "source": [
    "#### [DLCModel](#TableOfContents) <a id='DLCModel1'></a>\n",
    "Next we'll get ready to populate the `DLCModel` table, which holds all the relevant information for both pre-trained models and models trained within Spyglass.<br>First we'll need to determine a set of parameters for our model to select the correct model file.<br>We can visualize a default set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb663861",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCModelParams.get_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45a6ed",
   "metadata": {},
   "source": [
    "> Here is the syntax to add your own parameter set:\n",
    ">```python\n",
    "dlc_model_params_name = \"make_this_yours\"\n",
    "params = {\n",
    "            \"params\": {},\n",
    "            \"shuffle\": 1,\n",
    "            \"trainingsetindex\": 0,\n",
    "            \"model_prefix\": \"\",\n",
    "        }\n",
    "sgp.DLCModelParams.insert1({\"dlc_model_params_name\": dlc_model_params_name, \"params\": params}, skip_duplicates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce9696",
   "metadata": {},
   "source": [
    "Now that we've defined a set of parameters and inserted into `DLCModelParams`, we can insert an entry into `DLCModelSelection` and populate `DLCModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa23fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model_key = (sgp.DLCModelSource & model_training_key).fetch1(\"KEY\")\n",
    "sgp.DLCModelSelection().insert1({\n",
    "    **temp_model_key,\n",
    "    \"dlc_model_params_name\": \"default\"},\n",
    "    skip_duplicates=True)\n",
    "model_key = (sgp.DLCModelSelection & ).fetch1(\"KEY\")\n",
    "sgp.DLCModel.populate(model_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1b839",
   "metadata": {},
   "source": [
    "Again, let's make sure that everything looks correct in `DLCModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCModel() & model_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce4ee4",
   "metadata": {},
   "source": [
    "#### [DLCPoseEstimation](#TableOfContents) <a id='DLCPoseEstimation1'></a>\n",
    "\n",
    "Alright, now that we've trained model and populated the `DLCModel` table, we're ready to set-up Pose Estimation on a behavioral video of your choice.<br>For this tutorial, you can choose to use an epoch of your choice, we can also use the one specified below. If you'd like to use your own video, just specify the `nwb_file_name` and `epoch` number and make sure it's in the `VideoFile` table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a8dab-7caf-4389-8494-9158d2ec5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwb_file_name = \"J1620210604_.nwb\"\n",
    "epoch = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecfe757-cee8-435a-b902-77b1e6dc1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgc.VideoFile() & {\"nwb_file_name\": nwb_file_name, \"epoch\": epoch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26a081-859d-4dff-bb58-84cec2ff4b3f",
   "metadata": {},
   "source": [
    "To set up pose estimation, we need to make sure a few things are in order. Using `insert_estimation_task` will take care of these steps for us!<br>Briefly, it will convert out video to be in .mp4 format (DLC struggles with .h264) and determine the directory in which we'll store the pose estimation results.<br>\n",
    ">**`task_mode`** determines whether or not populating `DLCPoseEstimation` runs a new pose estimation, or loads an existing. Use _'trigger'_ unless you've already run this specific pose estimation.<br>**`video_file_num`** will be 0 in almost all cases.<br>__`gputouse`__ has already been set above during the training step. It may be a good idea to make sure that core is still free before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5644f-febc-49d7-a60d-6991798c20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_estimation_key = sgp.DLCPoseEstimationSelection.insert_estimation_task(\n",
    "    {\n",
    "        \"nwb_file_name\": nwb_file_name,\n",
    "        \"epoch\": epoch,\n",
    "        \"video_file_num\": 0,\n",
    "        **model_key,\n",
    "    },\n",
    "    task_mode=\"trigger\",\n",
    "    params={\"gputouse\": gputouse, \"videotype\": \"mp4\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb2a26-fae1-41ca-828f-cc6c73ebd24e",
   "metadata": {},
   "source": [
    "And now we populate `DLCPoseEstimation`! This might take a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f28ecc-d3a4-40f9-a1fb-afb4bdd04497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCPoseEstimation().populate(pose_estimation_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88757488-cfa4-4e7c-b965-7dacac43810a",
   "metadata": {},
   "source": [
    "Let's visualize the output from Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd4f3b-7bf4-41b7-be5f-820fe3ee9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sgp.DLCPoseEstimation() & pose_estimation_key).fetch_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f45ab3-9344-4975-b5ff-f80a5727cdac",
   "metadata": {},
   "source": [
    "#### [DLCSmoothInterp](#TableOfContents) <a id='DLCSmoothInterp1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd5dbe-097a-4138-a234-da78a5902684",
   "metadata": {},
   "source": [
    "Now that we've completed pose estimation, it's time to interpolate over low likelihood periods and smooth the resulting positions.<br>First we need to define some parameters for smoothing and interpolation. We can see the default parameter set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e44a34-8d6d-4206-b02a-9ca38a68f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sgp.DLCSmoothInterpParams.get_default())\n",
    "si_params_name = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245c9e5-e8f6-4c6f-b9e1-d71ab3e06d59",
   "metadata": {},
   "source": [
    "> If you'd like to change any of these parameters, here is the syntax to do that\n",
    ">```python\n",
    "si_params_name = 'your_unique_param_name'\n",
    "params = {\n",
    "    \"smoothing_params\": {\n",
    "        \"smoothing_duration\": 0.##,\n",
    "        \"smooth_method\": \"moving_avg\",\n",
    "    },\n",
    "    \"interp_params\": {\n",
    "        \"likelihood_thresh\": 0.##,\n",
    "    },\n",
    "    \"max_plausible_speed\": ###,\n",
    "    \"speed_smoothing_std_dev\": 0.###,\n",
    "}\n",
    "sgp.DLCSmoothInterpParams().insert1(\n",
    "    {\n",
    "        'dlc_si_params_name': si_params_name,\n",
    "        \"params\": params,\n",
    "    },\n",
    "    skip_duplicates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139036e-ce7e-41ec-be78-aa15a4b0b795",
   "metadata": {},
   "source": [
    "Here we'll create a dictionary with the correct set of keys for the `DLCSmoothInterpSelection` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec730b91-a974-4f54-9d55-35f52e08487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_key = pose_estimation_key.copy()\n",
    "fields = list(sgp.DLCSmoothInterpSelection.fetch().dtype.fields.keys())\n",
    "si_key = {key: val for key, val in si_key.items() if key in fields}\n",
    "si_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47a6de-51ff-4980-b105-42a75ef7f7a3",
   "metadata": {},
   "source": [
    "And now we can insert all of the bodyparts we want to process into `DLCSmoothInterpSelection`<br>\n",
    "First lets visualize the bodyparts we have available to us.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fcad0-e211-4bd7-82b1-d69bec0eb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((sgp.DLCPoseEstimation.BodyPart & pose_estimation_key).fetch(\"bodypart\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e3ad2-1960-43cd-a223-784c08211013",
   "metadata": {},
   "source": [
    "We can use `insert1` to insert a single bodypart, but would suggest using `insert` to insert a list of keys with different bodyparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70621afe-a144-47f2-9d70-212fbcd85aba",
   "metadata": {},
   "source": [
    ">_Syntax to insert a single bodypart_\n",
    ">```python\n",
    "sgp.DLCSmoothInterpSelection.insert1(\n",
    "    {\n",
    "        **si_key,\n",
    "        'bodypart': 'greenLED',\n",
    "        'dlc_si_params_name': si_params_name,\n",
    "    },\n",
    "    skip_duplicates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f73cd-2534-40a2-86e6-948ccd902812",
   "metadata": {},
   "source": [
    "Lets set a list of bodyparts we want to insert and then insert them into `DLCSmoothInterpSelection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e826d-38ef-4219-8d52-5353c6b4b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = [\"greenLED\", \"redLED_C\"]\n",
    "sgp.DLCSmoothInterpSelection.insert(\n",
    "    [\n",
    "        {\n",
    "            **si_key,\n",
    "            \"bodypart\": bodypart,\n",
    "            \"dlc_si_params_name\": si_params_name,\n",
    "        }\n",
    "        for bodypart in bodyparts\n",
    "    ],\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca5640-3e9a-42b7-bc61-7f3e1a219619",
   "metadata": {},
   "source": [
    "And to make sure that all of the bodyparts we want made it into the the selection table, we can visualize the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b347b29-1583-4fbc-9b35-8e062b611d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCSmoothInterpSelection() & si_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f0d26-3879-4f50-a076-e60685028083",
   "metadata": {},
   "source": [
    "Now we can populate `DLCSmoothInterp`, which will perform smoothing and interpolation on all of the bodyparts we specified.<br>We can limit the populate using `si_key` since it is bodypart agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf16c32-0f5e-4cd2-b814-56745e836599",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCSmoothInterp().populate(si_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3af0a2-16cc-43dc-af9c-0ec606cfe1e1",
   "metadata": {},
   "source": [
    "And let's visualize the resulting position data using a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced96b05-e6dc-4771-bfb8-bcbddfb8e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sgp.DLCSmoothInterp() & {**si_key, \"bodypart\": bodyparts[0]}\n",
    ").fetch1_dataframe().plot.scatter(x=\"x\", y=\"y\", s=1, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838e4c4-8ff9-4b73-aee5-00eb91ea899f",
   "metadata": {},
   "source": [
    "#### [DLCSmoothInterpCohort](#TableOfContents) <a id='DLCSmoothInterpCohort1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3d882-2c24-46ca-bfcc-72f21712e47b",
   "metadata": {},
   "source": [
    "Now that we've smoothed and interpolated our position data for each bodypart, we need to form a set of bodyparts from which we want to derive a centroid and orientation (or potentially a second set for orientation). This is the goal of the `DLCSmoothInterpCohort` table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017fd46-2bb9-4349-981b-f9789ffec338",
   "metadata": {},
   "source": [
    "First, let's make a key that represents the 'cohort' we want to form.\n",
    "> We'll set the `dlc_si_cohort_selection_name` to a concise name<br>We'll also form a dictionary with the bodypart name as the key and the smoothing/interpolation parameter name used for that bodypart as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb1af9-20cf-46d9-a518-a7f551334bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_key = si_key.copy()\n",
    "if \"bodypart\" in cohort_key:\n",
    "    del cohort_key[\"bodypart\"]\n",
    "if \"dlc_si_params_name\" in cohort_key:\n",
    "    del cohort_key[\"dlc_si_params_name\"]\n",
    "cohort_key[\"dlc_si_cohort_selection_name\"] = \"green_red_led\"\n",
    "cohort_key[\"bodyparts_params_dict\"] = {\n",
    "    \"greenLED\": si_params_name,\n",
    "    \"redLED_C\": si_params_name,\n",
    "}\n",
    "print(cohort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6a327-d4b0-4de1-a2c6-10a0443a3f96",
   "metadata": {},
   "source": [
    "Here we'll insert the cohort into the `DLCSmoothInterpCohortSelection` table<br>..and populate `DLCSmoothInterpCohort`, which collates the separately smoothed and interpolated bodyparts into a single entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f55c1-3c7b-4cf9-bdd7-98743810c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCSmoothInterpCohortSelection().insert1(cohort_key, skip_duplicates=True)\n",
    "sgp.DLCSmoothInterpCohort.populate(cohort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7d361-47c5-4748-ac59-f51b897f7fe6",
   "metadata": {},
   "source": [
    "And of course, let's make sure that the table populated correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7672b63-6dfc-46db-b8df-95c1e6730b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCSmoothInterpCohort.BodyPart() & cohort_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871bdca-2278-43ec-a70c-52257ad26170",
   "metadata": {},
   "source": [
    "#### [DLCCentroid](#TableOfContents) <a id='DLCCentroid1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc37edb-fdd3-4a05-8cd5-91f3c5f7cbbb",
   "metadata": {},
   "source": [
    "We now have a cohort of smoothed and interpolated bodyparts from which to determine a centroid!<br>To start, we'll need a set of parameters to use for determining the centroid. For this tutorial, we can use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31c8db-0396-475a-af71-ae38433d2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the default set\n",
    "print(sgp.DLCCentroidParams.get_default())\n",
    "centroid_params_name = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852948f7-e743-4319-be6b-265dadfca713",
   "metadata": {},
   "source": [
    ">Here is the syntax to add your own parameters:\n",
    ">```python\n",
    "centroid_params = {\n",
    "    'centroid_method': 'two_pt_centroid',\n",
    "    'points' : {\n",
    "        'greenLED': 'greenLED',\n",
    "        'redLED_C': 'redLED_C',},\n",
    "    'speed_smoothing_std_dev': 0.100,\n",
    "}\n",
    "centroid_params_name = 'your_unique_param_name'\n",
    "sgp.DLCCentroidParams.insert1({'dlc_centroid_params_name': centroid_params_name,\n",
    "                                'params': centroid_params},\n",
    "                                skip_duplicates=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad4e53-43dd-4e05-84c4-7d4504766746",
   "metadata": {},
   "source": [
    "And now let's make a key to insert into `DLCCentroidSelection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac17cb-4bb3-47b2-b1b9-1c4b37797591",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_key = cohort_key.copy()\n",
    "fields = list(sgp.DLCCentroidSelection.fetch().dtype.fields.keys())\n",
    "centroid_key = {key: val for key, val in centroid_key.items() if key in fields}\n",
    "centroid_key[\"dlc_centroid_params_name\"] = centroid_params_name\n",
    "print(centroid_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674c0d3-d3fd-4cd9-a843-260c442c2d23",
   "metadata": {},
   "source": [
    "Let's insert it into `DLCCentroidSelection` and then populate `DLCCentroid` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fccef4-2fef-4f74-b7a4-8564328b14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCCentroidSelection.insert1(centroid_key, skip_duplicates=True)\n",
    "sgp.DLCCentroid.populate(centroid_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49c5ad-909f-4f1a-a156-f8f8a84fb78a",
   "metadata": {},
   "source": [
    "Here we can visualize the resulting centroid position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7e447-fa6f-4f06-9ec9-4b9838b7255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sgp.DLCCentroid() & centroid_key).fetch1_dataframe().plot.scatter(\n",
    "    x=\"position_x\",\n",
    "    y=\"position_y\",\n",
    "    c=\"speed\",\n",
    "    colormap=\"viridis\",\n",
    "    alpha=0.5,\n",
    "    s=0.5,\n",
    "    figsize=(10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb513a9d-5250-404c-8887-639f785516c7",
   "metadata": {},
   "source": [
    "#### [DLCOrientation](#TableOfContents) <a id='DLCOrientation1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509076f0-f0b8-4fd0-8884-32c48ca4a125",
   "metadata": {},
   "source": [
    "We'll now go through a similar process to identify the orientation!<br>To start, we'll need a set of parameters to use for determining the orientation. For this tutorial, we can use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf244b3-7295-48ed-90ea-cf878e85e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sgp.DLCOrientationParams.get_default())\n",
    "dlc_orientation_params_name = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec170be-7a7a-4a20-986c-d055aee1a08b",
   "metadata": {},
   "source": [
    "Here we'll prune the `cohort_key` we used above and add our `dlc_orientation_params_name` to make it suitable for `DLCOrientationSelection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4a6cf-472e-43e3-90aa-f7ff7fb9dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = list(sgp.DLCOrientationSelection.fetch().dtype.fields.keys())\n",
    "orient_key = {key: val for key, val in cohort_key.items() if key in fields}\n",
    "orient_key[\"dlc_orientation_params_name\"] = dlc_orientation_params_name\n",
    "print(orient_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406d2de-9b71-4591-82f6-ed53f2d4f220",
   "metadata": {},
   "source": [
    "And now let's insert into `DLCOrientationSelection` and populate `DLCOrientation` to determine the orientation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d23302-02e3-427a-ac35-2f648e3ae674",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCOrientationSelection().insert1(orient_key, skip_duplicates=True)\n",
    "sgp.DLCOrientation().populate(orient_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62da0-0cc5-4ffb-b2df-7b68c3f6e268",
   "metadata": {},
   "source": [
    "We can fetch the output of `DLCOrientation` as a dataframe to make sure everything looks appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eba7f4-0b32-486a-894a-c97404c74d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sgp.DLCOrientation() & orient_key).fetch1_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75aeaf-018a-46ed-83a8-6603ae100791",
   "metadata": {},
   "source": [
    "#### [DLCPos](#TableOfContents) <a id='DLCPos1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3f9ba-dc89-4c32-a125-1fa85cd4132d",
   "metadata": {},
   "source": [
    "Ok, we're now done with processing the position data! We just have to do some table manipulations to make sure everything ends up in the same format and same location.<br>\n",
    "To summarize, we brought in a pretrained DLC project, used that model to run pose estimation on a new behavioral video, smoothed and interpolated the result, formed a cohort of bodyparts, and determined the centroid and orientation of this cohort. **_Whew!_**<br>\n",
    "Now let's populate `DLCPos` with our centroid and orientation entries from above.<br>----<br>\n",
    "To begin, we'll make a key that combines the cohort names we used for the orientation and centroid as well as the params names for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a166dd6-3863-4349-97ac-19d7d6a841b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = list(sgp.DLCPos.fetch().dtype.fields.keys())\n",
    "dlc_key = {key: val for key, val in centroid_key.items() if key in fields}\n",
    "dlc_key[\"dlc_si_cohort_centroid\"] = centroid_key[\"dlc_si_cohort_selection_name\"]\n",
    "dlc_key[\"dlc_si_cohort_orientation\"] = orient_key[\n",
    "    \"dlc_si_cohort_selection_name\"\n",
    "]\n",
    "dlc_key[\"dlc_orientation_params_name\"] = orient_key[\n",
    "    \"dlc_orientation_params_name\"\n",
    "]\n",
    "print(dlc_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e4c5e-7c32-46b0-a138-80064a212fbe",
   "metadata": {},
   "source": [
    "Now we can insert into `DLCPosSelection` and populate `DLCPos` with our `dlc_key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7badff-0ad7-48cf-aef6-a4f55df8ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCPosSelection().insert1(dlc_key, skip_duplicates=True)\n",
    "sgp.DLCPos().populate(dlc_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f1cff-2ead-4489-8a10-9fa7a5d33292",
   "metadata": {},
   "source": [
    "We can also make sure that all of our data made it through by fetching the dataframe attached to this entry.<br>We should expect 8 columns:\n",
    ">time<br>video_frame_ind<br>position_x<br>position_y<br>orientation<br>velocity_x<br>velocity_y<br>speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853db96b-1cd4-4ff6-91ea-aca7f7d3851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sgp.DLCPos() & dlc_key).fetch1_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8623a8-1725-4e02-b1a2-d2f993988102",
   "metadata": {},
   "source": [
    "And even more, we can fetch the `pose_eval_result` that is calculated during this step. This field contains the percentage of frames that each bodypart was below the likelihood threshold of 0.95 as a means of assessing the quality of the pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f06244-9d59-44d4-bcbb-062809b3ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sgp.DLCPos() & dlc_key).fetch1(\"pose_eval_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2303147-3657-479c-8f72-b3fc6905a596",
   "metadata": {},
   "source": [
    "#### [DLCPosVideo](#TableOfContents) <a id='DLCPosVideo1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b081d-f619-4c38-ba48-6ae1c0c5ff2b",
   "metadata": {},
   "source": [
    "Here we can create a video with the centroid and orientation overlaid on the animal's behavioral video. This will also plot the likelihood of each bodypart used in the cohort. This is completely optional, but a good idea to make sure everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a725c08-a616-43a0-8925-4a82bf872ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCPosVideoParams.insert_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2f782-ba45-487a-8e8f-e80dd33d9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"percent_frames\": 0.05,\n",
    "    \"incl_likelihood\": True,\n",
    "}\n",
    "sgp.DLCPosVideoParams.insert1(\n",
    "    {\"dlc_pos_video_params_name\": \"five_percent\", \"params\": params},\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758e2fc-13e6-46cb-9a93-ae1b4c1f4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCPosVideoSelection.insert1(\n",
    "    {**dlc_key, \"dlc_pos_video_params_name\": \"five_percent\"},\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887c0a5-77c8-421e-935e-0692f3f1fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.DLCPosVideo().populate(dlc_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68bba8-9871-40ac-84c9-51ac0e76d44e",
   "metadata": {},
   "source": [
    "#### [PositionOutput](#TableOfContents) <a id='PositionOutput1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25325173-bbaf-4b85-aef6-201384d9933b",
   "metadata": {},
   "source": [
    "`PositionOutput` is the final table of the position pipeline and is automatically populated when we populate `DLCPosV1`! Let's make sure that our entry made it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec40c9-78d8-4edd-8158-be91fb15af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.PositionOutput() & dlc_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414d9e0-e495-42ef-a8b0-1c7d53aed02e",
   "metadata": {},
   "source": [
    "`PositionOutput` also has a part table, similar to the `DLCModelSource` table above. Let's check that out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50760123-7f09-4a94-a1f7-41a037914fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PositionOutput.DLCPosV1() & dlc_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96daaa9-5e70-4a2c-b0a4-c2849e3a1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "(PositionOutput.DLCPosV1() & dlc_key).fetch1_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c7a4e-0bbc-4101-baf2-e84f1f5739d5",
   "metadata": {},
   "source": [
    "#### [PositionVideo](#TableOfContents)<a id='PositionVideo1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e6602-8e80-47fa-be78-4ae120d52e41",
   "metadata": {},
   "source": [
    "Bonus points if you made it this far... We can use the `PositionVideo` table to create a video that overlays just the centroid and orientation (regardless of upstream source) on the behavioral video. This table uses the parameter `plot` to determine whether to plot the entry deriving from the DLC arm or from the Trodes arm of the position pipeline. This parameter also accepts 'all', which will plot both (if they exist) in order to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a782ce-0a14-4725-887f-ae6f341635f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.PositionVideoSelection().insert1(\n",
    "    {\n",
    "        \"nwb_file_name\": \"J1620210604_.nwb\",\n",
    "        \"interval_list_name\": \"pos 13 valid times\",\n",
    "        \"trodes_position_id\": 0,\n",
    "        \"dlc_position_id\": 1,\n",
    "        \"plot\": \"DLC\",\n",
    "        \"output_dir\": \"/home/dgramling/Src/\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32993e7-5b32-46f9-a2f9-9634aef785f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgp.PositionVideo.populate({\"plot\": \"DLC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be097052-3789-4d55-aca1-e44d426c39b4",
   "metadata": {},
   "source": [
    "### _CONGRATULATIONS!!_\n",
    "Please treat yourself to a nice tea break :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c90a2",
   "metadata": {},
   "source": [
    "### [Return To Table of Contents](#TableOfContents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c1e1c-0a5f-40fe-8e02-749bb7f0d457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spyglass-position] *",
   "language": "python",
   "name": "conda-env-spyglass-position-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
